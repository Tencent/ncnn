// Copyright 2022 Tencent
// SPDX-License-Identifier: BSD-3-Clause

#version 450

#extension GL_GOOGLE_include_directive : enable
#include "vulkan_activation.comp"

#define LOCAL_MEMORY_UNROLL_INCH 8
#define LOCAL_MEMORY_PAD         1

layout(constant_id = 0) const int kernel_w = 1;
layout(constant_id = 1) const int kernel_h = 1;
layout(constant_id = 2) const int dilation_w = 1;
layout(constant_id = 3) const int dilation_h = 1;
layout(constant_id = 4) const int stride_w = 1;
layout(constant_id = 5) const int stride_h = 1;
layout(constant_id = 6) const int bias_term = 0;
layout(constant_id = 7) const int activation_type = 0;
layout(constant_id = 8) const float activation_param_0 = 0;
layout(constant_id = 9) const float activation_param_1 = 0;

#define shape_constant_id_offset 10
layout(constant_id = shape_constant_id_offset + 0) const int w = 0;
layout(constant_id = shape_constant_id_offset + 1) const int h = 0;
layout(constant_id = shape_constant_id_offset + 2) const int c = 0;
layout(constant_id = shape_constant_id_offset + 3) const int cstep = 0;

layout(constant_id = shape_constant_id_offset + 4) const int outw = 0;
layout(constant_id = shape_constant_id_offset + 5) const int outh = 0;
layout(constant_id = shape_constant_id_offset + 6) const int outc = 0;
layout(constant_id = shape_constant_id_offset + 7) const int outcstep = 0;

layout(binding = 0) readonly buffer bottom_blob { sfp bottom_blob_data[]; };
layout(binding = 1) writeonly buffer top_blob { sfp top_blob_data[]; };
layout(binding = 2) readonly buffer weight_blob { sfp weight_data[]; };
layout(binding = 3) readonly buffer bias_blob { sfp bias_data[]; };

layout(push_constant) uniform parameter
{
    int w;
    int h;
    int c;
    int cstep;

    int outw;
    int outh;
    int outc;
    int outcstep;
} p;

#if NCNN_shader_local_memory
shared lfp tmp_v[8][LOCAL_MEMORY_UNROLL_INCH][4 + LOCAL_MEMORY_PAD];
shared lfp tmp_k[8][LOCAL_MEMORY_UNROLL_INCH + LOCAL_MEMORY_PAD];
#endif

void main()
{
    int gx = int(gl_GlobalInvocationID.x) * 4;
    int gy = int(gl_GlobalInvocationID.y);

    const int outsize = psc(outw) * psc(outh);

#if !NCNN_shader_local_memory
    if (gx >= outsize || gy >= psc(outc))
        return;
#endif

    afp sum0;
    afp sum1;
    afp sum2;
    afp sum3;

    if (bias_term == 1)
    {
        sum0 = buffer_ld1(bias_data, gy);
        sum1 = sum0;
        sum2 = sum0;
        sum3 = sum0;
    }
    else
    {
        sum0 = afp(0.f);
        sum1 = afp(0.f);
        sum2 = afp(0.f);
        sum3 = afp(0.f);
    }

    const int maxk = kernel_w * kernel_h;
    const int N = psc(c) * maxk;

    const ivec4 gx4 = gx + ivec4(0, 1, 2, 3);

    const ivec4 sy4 = gx4 / psc(outw);
    const ivec4 sx4 = gx4 % psc(outw);

    const ivec4 sxs4 = sx4 * stride_w;
    const ivec4 sys4 = sy4 * stride_h;

    int w_offset = gy * N;

#if NCNN_shader_local_memory
    const int lx = int(gl_LocalInvocationID.x);
    const int ly = int(gl_LocalInvocationID.y);

    int z = 0;
    for (; z + (LOCAL_MEMORY_UNROLL_INCH - 1) < N; z += LOCAL_MEMORY_UNROLL_INCH)
    {
        if (ly < 4)
        {
            for (int z4 = 0; z4 < LOCAL_MEMORY_UNROLL_INCH; z4++)
            {
                const int sz = (z + z4) / maxk;
                const int k = (z + z4) % maxk;

                const int ky = k / kernel_w;
                const int kx = k % kernel_w;

                const int v_offset = sz * psc(cstep) + (sys4[ly] + ky * dilation_h) * psc(w) + sxs4[ly] + kx * dilation_w;

                tmp_v[lx][z4][ly] = buffer_sm1(bottom_blob_data, v_offset);
            }
        }

        if (lx == 0)
        {
            for (int z4 = 0; z4 < LOCAL_MEMORY_UNROLL_INCH; z4++)
            {
                tmp_k[ly][z4] = buffer_sm1(weight_data, w_offset + z4);
            }
        }

        barrier();

        // K unroll 4x
        int z4 = 0;
        for (; z4 + 3 < LOCAL_MEMORY_UNROLL_INCH; z4 += 4)
        {
            afp k0 = lfp2afp(tmp_k[ly][z4 + 0]);
            afp k1 = lfp2afp(tmp_k[ly][z4 + 1]);
            afp k2 = lfp2afp(tmp_k[ly][z4 + 2]);
            afp k3 = lfp2afp(tmp_k[ly][z4 + 3]);
            afpvec4 k4 = afpvec4(k0, k1, k2, k3);

            // column 0
            afpvec4 v0 = afpvec4(
                lfp2afp(tmp_v[lx][z4 + 0][0]),
                lfp2afp(tmp_v[lx][z4 + 1][0]),
                lfp2afp(tmp_v[lx][z4 + 2][0]),
                lfp2afp(tmp_v[lx][z4 + 3][0]));
            // column 1
            afpvec4 v1 = afpvec4(
                lfp2afp(tmp_v[lx][z4 + 0][1]),
                lfp2afp(tmp_v[lx][z4 + 1][1]),
                lfp2afp(tmp_v[lx][z4 + 2][1]),
                lfp2afp(tmp_v[lx][z4 + 3][1]));
            // column 2
            afpvec4 v2 = afpvec4(
                lfp2afp(tmp_v[lx][z4 + 0][2]),
                lfp2afp(tmp_v[lx][z4 + 1][2]),
                lfp2afp(tmp_v[lx][z4 + 2][2]),
                lfp2afp(tmp_v[lx][z4 + 3][2]));
            // column 3
            afpvec4 v3 = afpvec4(
                lfp2afp(tmp_v[lx][z4 + 0][3]),
                lfp2afp(tmp_v[lx][z4 + 1][3]),
                lfp2afp(tmp_v[lx][z4 + 2][3]),
                lfp2afp(tmp_v[lx][z4 + 3][3]));

            sum0 += dot(v0, k4);
            sum1 += dot(v1, k4);
            sum2 += dot(v2, k4);
            sum3 += dot(v3, k4);
        }

        // tail inside block (normally none when UNROLL=8)
        for (; z4 < LOCAL_MEMORY_UNROLL_INCH; z4++)
        {
            afp k = lfp2afp(tmp_k[ly][z4]);

            afp v0s = lfp2afp(tmp_v[lx][z4][0]);
            afp v1s = lfp2afp(tmp_v[lx][z4][1]);
            afp v2s = lfp2afp(tmp_v[lx][z4][2]);
            afp v3s = lfp2afp(tmp_v[lx][z4][3]);

            sum0 += v0s * k;
            sum1 += v1s * k;
            sum2 += v2s * k;
            sum3 += v3s * k;
        }

        w_offset += LOCAL_MEMORY_UNROLL_INCH;

        barrier();
    }

    if (z < N)
    {
        const int remain = N - z;

        if (ly < 4)
        {
            for (int z4 = 0; z4 < remain; z4++)
            {
                const int sz = (z + z4) / maxk;
                const int k = (z + z4) % maxk;

                const int ky = k / kernel_w;
                const int kx = k % kernel_w;

                const int v_offset = sz * psc(cstep) + (sys4[ly] + ky * dilation_h) * psc(w) + sxs4[ly] + kx * dilation_w;

                tmp_v[lx][z4][ly] = buffer_sm1(bottom_blob_data, v_offset);
            }
        }

        if (lx == 0)
        {
            for (int z4 = 0; z4 < remain; z4++)
            {
                tmp_k[ly][z4] = buffer_sm1(weight_data, w_offset + z4);
            }
        }

        barrier();

        int z4 = 0;
        for (; z4 + 3 < remain; z4 += 4)
        {
            afp k0 = lfp2afp(tmp_k[ly][z4 + 0]);
            afp k1 = lfp2afp(tmp_k[ly][z4 + 1]);
            afp k2 = lfp2afp(tmp_k[ly][z4 + 2]);
            afp k3 = lfp2afp(tmp_k[ly][z4 + 3]);
            afpvec4 k4 = afpvec4(k0, k1, k2, k3);

            afpvec4 v0 = afpvec4(
                lfp2afp(tmp_v[lx][z4 + 0][0]),
                lfp2afp(tmp_v[lx][z4 + 1][0]),
                lfp2afp(tmp_v[lx][z4 + 2][0]),
                lfp2afp(tmp_v[lx][z4 + 3][0]));
            afpvec4 v1 = afpvec4(
                lfp2afp(tmp_v[lx][z4 + 0][1]),
                lfp2afp(tmp_v[lx][z4 + 1][1]),
                lfp2afp(tmp_v[lx][z4 + 2][1]),
                lfp2afp(tmp_v[lx][z4 + 3][1]));
            afpvec4 v2 = afpvec4(
                lfp2afp(tmp_v[lx][z4 + 0][2]),
                lfp2afp(tmp_v[lx][z4 + 1][2]),
                lfp2afp(tmp_v[lx][z4 + 2][2]),
                lfp2afp(tmp_v[lx][z4 + 3][2]));
            afpvec4 v3 = afpvec4(
                lfp2afp(tmp_v[lx][z4 + 0][3]),
                lfp2afp(tmp_v[lx][z4 + 1][3]),
                lfp2afp(tmp_v[lx][z4 + 2][3]),
                lfp2afp(tmp_v[lx][z4 + 3][3]));

            sum0 += dot(v0, k4);
            sum1 += dot(v1, k4);
            sum2 += dot(v2, k4);
            sum3 += dot(v3, k4);
        }

        for (; z4 < remain; z4++)
        {
            afp k = lfp2afp(tmp_k[ly][z4]);

            afp v0s = lfp2afp(tmp_v[lx][z4][0]);
            afp v1s = lfp2afp(tmp_v[lx][z4][1]);
            afp v2s = lfp2afp(tmp_v[lx][z4][2]);
            afp v3s = lfp2afp(tmp_v[lx][z4][3]);

            sum0 += v0s * k;
            sum1 += v1s * k;
            sum2 += v2s * k;
            sum3 += v3s * k;
        }
    }
#else
    // no shared memory: unroll K by 4, vectorize weights into afpvec4 + dot()
    int z = 0;
    for (; z + 3 < N; z += 4)
    {
        // z+0
        int z0 = z + 0;
        int sz0 = z0 / maxk;
        int kk0 = z0 % maxk;
        int ky0 = kk0 / kernel_w;
        int kx0 = kk0 % kernel_w;

        // z+1
        int z1 = z + 1;
        int sz1 = z1 / maxk;
        int kk1 = z1 % maxk;
        int ky1 = kk1 / kernel_w;
        int kx1 = kk1 % kernel_w;

        // z+2
        int z2 = z + 2;
        int sz2 = z2 / maxk;
        int kk2 = z2 % maxk;
        int ky2 = kk2 / kernel_w;
        int kx2 = kk2 % kernel_w;

        // z+3
        int z3 = z + 3;
        int sz3 = z3 / maxk;
        int kk3 = z3 % maxk;
        int ky3 = kk3 / kernel_w;
        int kx3 = kk3 % kernel_w;

        afp k0 = buffer_ld1(weight_data, w_offset + 0);
        afp k1 = buffer_ld1(weight_data, w_offset + 1);
        afp k2 = buffer_ld1(weight_data, w_offset + 2);
        afp k3 = buffer_ld1(weight_data, w_offset + 3);
        afpvec4 k4 = afpvec4(k0, k1, k2, k3);

        // gather A for each output column, pack into vec4
        ivec4 v0o = sz0 * psc(cstep) + (sys4 + ky0 * dilation_h) * psc(w) + sxs4 + kx0 * dilation_w;
        ivec4 v1o = sz1 * psc(cstep) + (sys4 + ky1 * dilation_h) * psc(w) + sxs4 + kx1 * dilation_w;
        ivec4 v2o = sz2 * psc(cstep) + (sys4 + ky2 * dilation_h) * psc(w) + sxs4 + kx2 * dilation_w;
        ivec4 v3o = sz3 * psc(cstep) + (sys4 + ky3 * dilation_h) * psc(w) + sxs4 + kx3 * dilation_w;

        // col 0
        afpvec4 v0 = afpvec4(
            buffer_ld1(bottom_blob_data, v0o.r),
            buffer_ld1(bottom_blob_data, v1o.r),
            buffer_ld1(bottom_blob_data, v2o.r),
            buffer_ld1(bottom_blob_data, v3o.r));
        // col 1
        afpvec4 v1 = afpvec4(
            buffer_ld1(bottom_blob_data, v0o.g),
            buffer_ld1(bottom_blob_data, v1o.g),
            buffer_ld1(bottom_blob_data, v2o.g),
            buffer_ld1(bottom_blob_data, v3o.g));
        // col 2
        afpvec4 v2 = afpvec4(
            buffer_ld1(bottom_blob_data, v0o.b),
            buffer_ld1(bottom_blob_data, v1o.b),
            buffer_ld1(bottom_blob_data, v2o.b),
            buffer_ld1(bottom_blob_data, v3o.b));
        // col 3
        afpvec4 v3 = afpvec4(
            buffer_ld1(bottom_blob_data, v0o.a),
            buffer_ld1(bottom_blob_data, v1o.a),
            buffer_ld1(bottom_blob_data, v2o.a),
            buffer_ld1(bottom_blob_data, v3o.a));

        sum0 += dot(v0, k4);
        sum1 += dot(v1, k4);
        sum2 += dot(v2, k4);
        sum3 += dot(v3, k4);

        w_offset += 4;
    }

    for (; z < N; z++)
    {
        const int sz = z / maxk;
        const int kk = z % maxk;

        const int ky = kk / kernel_w;
        const int kx = kk % kernel_w;

        const ivec4 v_offset = sz * psc(cstep) + (sys4 + ky * dilation_h) * psc(w) + sxs4 + kx * dilation_w;

        afp v0 = buffer_ld1(bottom_blob_data, v_offset.r);
        afp v1 = buffer_ld1(bottom_blob_data, v_offset.g);
        afp v2 = buffer_ld1(bottom_blob_data, v_offset.b);
        afp v3 = buffer_ld1(bottom_blob_data, v_offset.a);

        afp k = buffer_ld1(weight_data, w_offset);

        sum0 += v0 * k;
        sum1 += v1 * k;
        sum2 += v2 * k;
        sum3 += v3 * k;

        w_offset += 1;
    }
#endif

#if NCNN_shader_local_memory
    if (gx >= outsize || gy >= psc(outc))
        return;
#endif

    sum0 = activation_afp(sum0, activation_type, activation_param_0, activation_param_1);
    sum1 = activation_afp(sum1, activation_type, activation_param_0, activation_param_1);
    sum2 = activation_afp(sum2, activation_type, activation_param_0, activation_param_1);
    sum3 = activation_afp(sum3, activation_type, activation_param_0, activation_param_1);

    const int gi = gy * psc(outcstep) + gx;

    buffer_st1(top_blob_data, gi, sum0);
    if (gx + 1 < outsize) buffer_st1(top_blob_data, gi + 1, sum1);
    if (gx + 2 < outsize) buffer_st1(top_blob_data, gi + 2, sum2);
    if (gx + 3 < outsize) buffer_st1(top_blob_data, gi + 3, sum3);
}
