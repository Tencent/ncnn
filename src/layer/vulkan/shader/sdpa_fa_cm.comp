// Copyright 2026 Tencent
// SPDX-License-Identifier: BSD-3-Clause

#version 450

#extension GL_EXT_control_flow_attributes : require

#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_arithmetic : require

#extension GL_KHR_shader_subgroup_shuffle : require

#extension GL_KHR_memory_scope_semantics : require
#extension GL_EXT_shader_explicit_arithmetic_types : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#if ncnn_VK_KHR_cooperative_matrix
#extension GL_KHR_cooperative_matrix : require
#elif ncnn_VK_NV_cooperative_matrix
#extension GL_NV_cooperative_matrix : require
#endif

layout(constant_id = 0) const int attn_mask = 0;

layout(constant_id = 1 + 0) const uint M = 1;
layout(constant_id = 1 + 1) const uint N = 1;
layout(constant_id = 1 + 2) const uint K = 1;
layout(constant_id = 1 + 3) const uint subgroup_size = 32;
layout(constant_id = 1 + 4) const uint UNROLL_WG_M = 2;

layout(binding = 0) readonly buffer Q_blob { uvec4 Q_blob_data[]; };
layout(binding = 1) readonly buffer K_blob { uvec4 K_blob_data[]; };
layout(binding = 2) readonly buffer V_blob { uvec4 V_blob_data[]; };
// layout(binding = 3) writeonly buffer top_blob { sfp top_blob_data[]; };
layout(binding = 3) writeonly buffer top_blob { uvec4 top_blob_data[]; };
layout(binding = 4) readonly buffer mask_blob { sfp mask_blob_data[]; };

layout(push_constant) uniform parameter
{
    float scale;
    int src_seqlen;
    int dst_seqlen;
    int embed_dim;
    int out_embed_dim;
    int num_heads;
    int attn_mask_dims;
    int num_heads_per_group;
    int Q_cstep;
    int K_cstep;
    int V_cstep;
    int out_cstep;
    int mask_cstep;
} p;

// Shared Memory 布局常量
// 最大支持的 out_embed_dim / 16 块数
#define MAX_OUT_CHUNKS 8

// Shared Memory - 直接使用 float，简洁明了
shared vec4 tmp_s[UNROLL_WG_M * M * N / 4]; // Score matrix (16x16 FP32)
shared vec4 tmp_o[UNROLL_WG_M * M * N / 4]; // Output tile for rescaling (16x16 FP32)

// Shared memory for row statistics (用于跨线程同步)
shared float smem_row_max[UNROLL_WG_M * M];
shared float smem_row_sum[UNROLL_WG_M * M];
shared float smem_correction[UNROLL_WG_M * M];


// ============================================
// 辅助函数：N 路归约
// ============================================
float reduceN_max(float val, uint n)
{
    [[unroll]] for (uint offset = 1; offset < n; offset *= 2)
    {
        val = max(val, subgroupShuffleXor(val, offset));
    }
    return val;
}

vec4 reduceN_max_vec4(vec4 val, uint n)
{
    [[unroll]] for (uint offset = 1; offset < n; offset *= 2)
    {
        val = max(val, subgroupShuffleXor(val, offset));
    }
    return val;
}

float reduceN_add(float val, uint n)
{
    [[unroll]] for (uint offset = 1; offset < n; offset *= 2)
    {
        val = val + subgroupShuffleXor(val, offset);
    }
    return val;
}

vec4 reduceN_add_vec4(vec4 val, uint n)
{
    [[unroll]] for (uint offset = 1; offset < n; offset *= 2)
    {
        val = val + subgroupShuffleXor(val, offset);
    }
    return val;
}

void main()
{
    const int gz = int(gl_GlobalInvocationID.z);

    if (gz >= p.num_heads)
        return;

    // neither gl_SubgroupSize nor gl_WorkGroupSize.x is a constant
    const uint local_size = subgroup_size * UNROLL_WG_M;

    const uint wgi = gl_WorkGroupID.x;
    const uint sgi = gl_SubgroupID;

//     const uint ni = sgi * UNROLL_WG_M;
    const uint mi = wgi * UNROLL_WG_M + sgi;

    // 当前 Q 块的起始行
    const uint q_row_start = mi * M;
    if (q_row_start >= p.src_seqlen)
        return;

    const uint li = gl_LocalInvocationID.x;
    const uint si = gl_SubgroupInvocationID;

    // assume cooperative matrix is 16x16x16

    //     const uint sum_N = out_embed_dim / 16;

    //     const uint dst_seqlen_d16 = dst_seqlen / 16;
    //     const uint embed_dim_d16 = embed_dim / 16;

    // 计算各种维度
    const uint dst_seqlen_d16 = (p.dst_seqlen + 15) / 16;
    const uint embed_dim_d16 = (p.embed_dim + 15) / 16;
    const uint out_embed_dim_d16 = (p.out_embed_dim + 15) / 16;

    // KV head index (for GQA support)
    const int kv_head_idx = gz / p.num_heads_per_group;

    // 每行元素数和每线程处理元素数
    //     const uint elements_per_row = N;  // 16
    const uint elements_per_thread = M * N / subgroup_size;

    // --- 1. 初始化 Accumulators (Output O) ---
    // 为了支持较大的 out_embed_dim，我们需要一组 accumulator
    // 假设最大支持 out_embed_dim = 64 (4 blocks) 或 128 (8 blocks)
    // 根据 spec 这里只能用常量大小数组
    // declare O for final output (w=out_embed_dim, h=16)
    coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> om[MAX_OUT_CHUNKS];

    {
        [[unroll]] for (uint zn = 0; zn < MAX_OUT_CHUNKS; zn++)
        {
            om[zn] = coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(0.f);
        }
    }

    // ========================================
    // 2. 初始化 Row Statistics (在 SMEM 中)
    // ========================================
    // 只让部分线程初始化，避免竞争
    // 使用循环处理，兼容不同 subgroup_size
//     for (uint i = si; i < M; i += subgroup_size)
    for (uint i = li; i < UNROLL_WG_M * M; i += local_size)
    {
        smem_row_max[i] = -3.402823e+38f;
        smem_row_sum[i] = 0.f;
        smem_correction[i] = 1.f;
    }
    barrier();

    // loop on dst_seqlen / 16

    for (uint j = 0; j < dst_seqlen_d16; j++)
    {
        coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> qkm;
        qkm = coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(0.f);

        // QK
        {
#if NCNN_bf16_storage || NCNN_bf16_packed
            coopmat<bfloat16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> qm;
            coopmat<bfloat16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> km;
#else
            coopmat<float16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> qm;
            coopmat<float16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> km;
#endif

            for (uint k = 0; k < embed_dim_d16; k++)
            {
                // load Q (w=embed_dim, h=16)
                // load K (w=embed_dim, h=16)

                const uint qi = (gz * p.Q_cstep + mi * M * p.embed_dim + k * K) / 8;
                coopMatLoad(qm, Q_blob_data, qi, p.embed_dim / 8, gl_CooperativeMatrixLayoutRowMajor);

                const uint ki = (kv_head_idx * p.K_cstep + j * N * p.embed_dim + k * K) / 8;
                coopMatLoad(km, K_blob_data, ki, p.embed_dim / 8, gl_CooperativeMatrixLayoutColumnMajor);

                // calculate QK (w=16, h=16)
                qkm = coopMatMulAdd(qm, km, qkm);
            }
        }

        // QK *= scale
        {
            qkm = qkm * p.scale;
        }

        // QK += mask
        if (attn_mask != 0)
        {
            // load mask
#if NCNN_bf16_storage || NCNN_bf16_packed
            // load Q (w=embed_dim, h=16)
            coopmat<bfloat16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> mask;
#else
            coopmat<float16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> mask;
#endif

            uint mask_head = (p.attn_mask_dims == 3) ? gz : 0;
            const uint mmi = mask_head * p.mask_cstep + mi * M * p.dst_seqlen + j * N;
            coopMatLoad(mask, mask_blob_data, mmi, p.dst_seqlen, gl_CooperativeMatrixLayoutRowMajor);

            qkm = qkm + coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(mask);
        }

//         coopMatStore(qkm, tmp_s, 0, N, gl_CooperativeMatrixLayoutRowMajor);
        coopMatStore(qkm, tmp_s, sgi * M * N / 4, N / 4, gl_CooperativeMatrixLayoutRowMajor);

        barrier();

        // ----------------------------------------
        // 3.5 Online Softmax - 计算 row max
        // ----------------------------------------
        for (uint idx = si; idx < M * N / 4; idx += subgroup_size)
        {
            const uint row = idx / (N / 4);
            const uint col = idx % (N / 4);

//             float val = tmp_s[idx];
            vec4 val = tmp_s[sgi * M * N / 4 + idx];

            // 行内归约取 max
//             float row_max_val = reduceN_max(val, N);
            vec4 row_max_val_4 = reduceN_max_vec4(val, N / 4);

            // 第一个线程更新 SMEM
            if (col == 0)
            {
                vec2 row_max_val_2 = max(row_max_val_4.rg, row_max_val_4.ba);
                float row_max_val = max(row_max_val_2.x, row_max_val_2.y);

                float old_max = smem_row_max[sgi * M + row];
                float new_max = max(old_max, row_max_val);
                smem_correction[sgi * M + row] = exp(old_max - new_max);
                smem_row_max[sgi * M + row] = new_max;
            }
        }

        barrier();

        // ----------------------------------------
        // 3.6 Online Softmax - 计算 exp 和 sum
        // ----------------------------------------
        for (uint idx = si; idx < M * N / 4; idx += subgroup_size)
        {
            const uint row = idx / (N / 4);
            const uint col = idx % (N / 4);

            float new_max = smem_row_max[sgi * M + row];
//             float val = tmp_s[idx];
            vec4 val = tmp_s[sgi * M * N / 4 + idx];

            // exp(x - max)
            val = exp(val - new_max);

            // 行内归约求 sum
//             float row_sum_val = reduceN_add(val, N);
//             vec4 row_sum_val_4 = reduceN_add_vec4(val, N / 4);
            vec4 row_sum_val_4 = reduceN_add_vec4(val, N / 4);

            // 更新全局 sum
            if (col == 0)
            {
                vec2 row_sum_val_2 = row_sum_val_4.rg + row_sum_val_4.ba;
                float row_sum_val = row_sum_val_2.x + row_sum_val_2.y;

                float correction = smem_correction[sgi * M + row];
                smem_row_sum[sgi * M + row] = smem_row_sum[sgi * M + row] * correction + row_sum_val;
            }

            // 写回 P
            tmp_s[sgi * M * N / 4 + idx] = val;
        }

        barrier();

        // O = O * correction (matrix-vector)
        // ----------------------------------------
        // 3.6 Rescale O: O = O * correction
        // ----------------------------------------
//         {
//             coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> cc;
//             coopMatLoad(cc, smem_correction, 0, 0, gl_CooperativeMatrixLayoutColumnMajor);
//
//             [[unroll]] for (uint c = 0; c < out_embed_dim_d16 && c < MAX_OUT_CHUNKS; c++)
//             {
//                 om[c] = om[c] * cc;
//             }
//         }
#if 1
        // 由于 correction 是按行的，我们需要 unload O，逐元素乘，reload
        [[unroll]] for (uint c = 0; c < out_embed_dim_d16 && c < MAX_OUT_CHUNKS; c++)
        {
            // Store O to SMEM
            coopMatStore(om[c], tmp_o, sgi * M * N / 4, N / 4, gl_CooperativeMatrixLayoutRowMajor);

            barrier();

            // 并行 rescale
            for (uint idx = si; idx < M * N / 4; idx += subgroup_size)
            {
                const uint row = idx / (N / 4);
                tmp_o[sgi * M * N / 4 + idx] *= smem_correction[sgi * M + row];
            }

            barrier();

            // Reload O
            coopMatLoad(om[c], tmp_o, sgi * M * N / 4, N / 4, gl_CooperativeMatrixLayoutRowMajor);
        }
#endif

        // ----------------------------------------
        // 3.7 Load P from SMEM
        // ----------------------------------------
        // 需要将 FP32 的 tmp_s 转换为 FP16 给 CoopMat
        // 方案1: 使用 FP32 CoopMat (如果硬件支持)
        // 方案2: 转换为 FP16 packed buffer

        // 这里使用方案2: 将 tmp_s 转换为 packed FP16 format
        // 复用 tmp_o 的低半部分作为 FP16 buffer (32 uvec4 = 128 floats 空间够用)

        // 将 FP32 P 转换为 packed FP16
        {
            coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> a;
            coopMatLoad(a, tmp_s, sgi * M * N / 4, N / 4, gl_CooperativeMatrixLayoutRowMajor);

#if NCNN_bf16_storage || NCNN_bf16_packed
            coopmat<bfloat16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> b = coopmat<bfloat16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(a);
#else
            coopmat<float16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> b = coopmat<float16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(a);
#endif
            coopMatStore(b, tmp_o, sgi * M * N / 4, N / 8, gl_CooperativeMatrixLayoutRowMajor);
        }
// #if 0
// //         for (uint idx = si; idx < M * N / 2; idx += subgroup_size)
//         for (uint idx = si; idx < M * N / 4; idx += subgroup_size)
//         {
// //             uint src_idx = idx * 2;
// //             float v0 = tmp_s[src_idx];
// //             float v1 = tmp_s[src_idx + 1];
//
//             vec4 v = tmp_s[idx];
//
//             // 使用 tmp_o 的前 32 个 float 位置存储 packed FP16
//             // 每个 float 位置存储 2 个 FP16
// #if NCNN_bf16_storage
// //             tmp_o[idx] = uintBitsToFloat(packBFloat2x16(vec2(v0, v1)));
//             tmp_o[idx * 2] = uintBitsToFloat(packBFloat2x16(v.rg));
//             tmp_o[idx * 2 + 1] = uintBitsToFloat(packBFloat2x16(v.ba));
// #else
// //             tmp_o[idx] = uintBitsToFloat(packHalf2x16(vec2(v0, v1)));
//             tmp_o[idx * 2] = uintBitsToFloat(packHalf2x16(v.rg));
//             tmp_o[idx * 2 + 1] = uintBitsToFloat(packHalf2x16(v.ba));
// #endif
//         }
// #endif

        barrier();

        // 加载 FP16 P
#if NCNN_bf16_storage || NCNN_bf16_packed
        coopmat<bfloat16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> pm;
#else
        coopmat<float16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> pm;
#endif
//         coopMatLoad(pm, tmp_o, 0, N / 2, gl_CooperativeMatrixLayoutRowMajor);
        coopMatLoad(pm, tmp_o, sgi * M * N / 4, N / 8, gl_CooperativeMatrixLayoutRowMajor);

        // qkv cross
        for (uint c = 0; c < MAX_OUT_CHUNKS; c++)
        {
#if NCNN_bf16_storage || NCNN_bf16_packed
            // load V (w=16, h=16)
            coopmat<bfloat16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> vm;
#else
            coopmat<float16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> vm;
#endif

            const uint vi = (kv_head_idx * p.V_cstep + j * N * p.out_embed_dim + c * N) / 8;
            coopMatLoad(vm, V_blob_data, vi, p.out_embed_dim / 8, gl_CooperativeMatrixLayoutRowMajor);

            // calculate O += PV (w=16, h=16)
            om[c] = coopMatMulAdd(pm, vm, om[c]);
        }
    }

    // O = O / row_sum
    // store top_blob
    for (uint c = 0; c < MAX_OUT_CHUNKS; c++)
    {
//         coopMatStore(om[c], tmp_o, 0, N, gl_CooperativeMatrixLayoutRowMajor);
        coopMatStore(om[c], tmp_o, sgi * M * N / 4, N / 4, gl_CooperativeMatrixLayoutRowMajor);

        barrier();

        // 并行归一化和存储
        // 归一化并转换为 FP16 输出
        for (uint idx = si; idx < M * N / 8; idx += subgroup_size)
        {
            // 每个线程处理 8 个 float，输出 1 个 uvec4
            uint base = idx * 8;
            uint row = base / N;
            uint col = base % N;

            // 边界检查
            if (mi * M + int(row) >= p.src_seqlen)
                continue;

            float inv_sum = 1.0f / smem_row_sum[sgi * M + row];

            // 读取 8 个 float
            vec4 v0 = tmp_o[sgi * M * N / 4 + base / 4 + 0] * inv_sum;
            vec4 v1 = tmp_o[sgi * M * N / 4 + base / 4 + 1] * inv_sum;

            // 打包为 uvec4 (8 个 FP16)
            uvec4 out_data;
#if NCNN_bf16_storage
            out_data.x = packBFloat2x16(v0.rg);
            out_data.y = packBFloat2x16(v0.ba);
            out_data.z = packBFloat2x16(v1.rg);
            out_data.w = packBFloat2x16(v1.ba);
#else
            out_data.x = packHalf2x16(v0.rg);
            out_data.y = packHalf2x16(v0.ba);
            out_data.z = packHalf2x16(v1.rg);
            out_data.w = packHalf2x16(v1.ba);
#endif

            // 写入 Global Memory
            const uint out_row = mi * M + row;
            const uint out_col = c * N + col;

            if (out_col + 8 <= p.out_embed_dim)
            {
                const uint out_idx = (gz * p.out_cstep + out_row * p.out_embed_dim + out_col) / 8;
                top_blob_data[out_idx] = out_data;
            }
        }
    }
}
