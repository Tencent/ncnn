// Copyright 2026 Tencent
// SPDX-License-Identifier: BSD-3-Clause

#version 450

#extension GL_EXT_control_flow_attributes : require

#extension GL_KHR_shader_subgroup_basic : require
#extension GL_KHR_shader_subgroup_arithmetic : require

#extension GL_KHR_shader_subgroup_shuffle : require

#extension GL_KHR_memory_scope_semantics : require
#extension GL_EXT_shader_explicit_arithmetic_types : require
#extension GL_EXT_shader_explicit_arithmetic_types_float16 : require
#if ncnn_VK_KHR_cooperative_matrix
#extension GL_KHR_cooperative_matrix : require
#elif ncnn_VK_NV_cooperative_matrix
#extension GL_NV_cooperative_matrix : require
#endif

layout(constant_id = 0) const int attn_mask = 0;

layout(constant_id = 1 + 0) const uint M = 1;
layout(constant_id = 1 + 1) const uint N = 1;
layout(constant_id = 1 + 2) const uint K = 1;
layout(constant_id = 1 + 3) const uint subgroup_size = 32;
layout(constant_id = 1 + 4) const uint UNROLL_SG_M = 2;
layout(constant_id = 1 + 5) const uint UNROLL_WG_M = 2;
layout(constant_id = 1 + 6) const uint MAX_OUT_CHUNKS = 8;
layout(constant_id = 1 + 7) const uint UNROLL_P_N = 4;

layout(binding = 0) readonly buffer Q_blob { uvec4 Q_blob_data[]; };
layout(binding = 1) readonly buffer K_blob { uvec4 K_blob_data[]; };
layout(binding = 2) readonly buffer V_blob { uvec4 V_blob_data[]; };
layout(binding = 3) writeonly buffer top_blob { uvec4 top_blob_data[]; };
layout(binding = 4) readonly buffer mask_blob { sfp mask_blob_data[]; };

layout(push_constant) uniform parameter
{
    float scale;
    int src_seqlen;
    int dst_seqlen;
    int embed_dim;
    int out_embed_dim;
    int num_heads;
    int attn_mask_dims;
    int num_heads_per_group;
    int Q_cstep;
    int K_cstep;
    int V_cstep;
    int out_cstep;
    int mask_cstep;
} p;

// Shared Memory 布局常量
// 最大支持的 out_embed_dim / 16 块数
// #define MAX_OUT_CHUNKS 8

// assert N == K

const uint Nd4 = N / 4;

const uint Nd8 = N / 8;
const uint Kd8 = K / 8;

// avoid bank conflict
#define PAD 1

const uint Np = N + PAD;

const uint Nd4p = Nd4 + PAD;

const uint Nd8p = Nd8 + PAD;
const uint Kd8p = Kd8 + PAD;

shared uvec4 tmp_q[UNROLL_WG_M * UNROLL_SG_M * M * Kd8p];
shared uvec4 tmp_k[UNROLL_P_N * N * Kd8p];
// shared uvec4 tmp_v[UNROLL_P_N * N * Kd8p];
#define tmp_v tmp_k

shared float tmp_s[UNROLL_WG_M * UNROLL_SG_M * UNROLL_P_N * M * Np]; // Score matrix (16x16 FP32)
shared vec4 tmp_o[UNROLL_WG_M * UNROLL_SG_M * M * Nd4p];              // Output tile for rescaling (16x16 FP32)

// Shared memory for row statistics (用于跨线程同步)
shared float smem_row_max[UNROLL_WG_M * UNROLL_SG_M * M];
shared float smem_row_sum[UNROLL_WG_M * UNROLL_SG_M * M];
shared float smem_correction[UNROLL_WG_M * UNROLL_SG_M * M];

// ============================================
// 辅助函数：N 路归约
// ============================================
float reduceN_max(float val, uint n)
{
    [[unroll]] for (uint offset = 1; offset < n; offset *= 2)
    {
        val = max(val, subgroupShuffleXor(val, offset));
    }
    return val;
}

float reduceN_add(float val, uint n)
{
    [[unroll]] for (uint offset = 1; offset < n; offset *= 2)
    {
        val = val + subgroupShuffleXor(val, offset);
    }
    return val;
}

void main()
{
    const int gz = int(gl_GlobalInvocationID.z);

    if (gz >= p.num_heads)
        return;

    // neither gl_SubgroupSize nor gl_WorkGroupSize.x is a constant
    const uint local_size = subgroup_size * UNROLL_WG_M;

    const uint wgi = gl_WorkGroupID.x;
    const uint sgi = gl_SubgroupID;

    const uint wgmm = (p.src_seqlen + M * UNROLL_SG_M * UNROLL_WG_M - 1) / (M * UNROLL_SG_M * UNROLL_WG_M);

    if (wgi >= wgmm)
        return;

    //     const uint ni = sgi * UNROLL_WG_M;
    const uint mi = (wgi * UNROLL_WG_M + sgi) * UNROLL_SG_M;

    const uint li = gl_LocalInvocationID.x;
    const uint si = gl_SubgroupInvocationID;

    // 计算各种维度
    const uint dst_seqlen_d16 = (p.dst_seqlen + 15) / 16;
    const uint embed_dim_d16 = (p.embed_dim + 15) / 16;
    const uint out_embed_dim_d16 = (p.out_embed_dim + 15) / 16;

    // KV head index (for GQA support)
    const int kv_head_idx = gz / p.num_heads_per_group;

    // 每行元素数和每线程处理元素数
    //     const uint elements_per_row = N;  // 16
    const uint elements_per_thread = M * N / subgroup_size;

    // --- 1. 初始化 Accumulators (Output O) ---
    // 为了支持较大的 out_embed_dim，我们需要一组 accumulator
    // 假设最大支持 out_embed_dim = 64 (4 blocks) 或 128 (8 blocks)
    // 根据 spec 这里只能用常量大小数组
    // declare O for final output (w=out_embed_dim, h=16)
    coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> om[UNROLL_SG_M][MAX_OUT_CHUNKS];

    [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
    {
        [[unroll]] for (uint zn = 0; zn < MAX_OUT_CHUNKS; zn++)
        {
            om[zm][zn] = coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(0.f);
        }
    }

    // ========================================
    // 2. 初始化 Row Statistics (在 SMEM 中)
    // ========================================
    [[unroll]] for (uint i = li; i < UNROLL_WG_M * UNROLL_SG_M * M; i += local_size)
    {
        smem_row_max[i] = -3.402823e+38f;
        smem_row_sum[i] = 0.f;
        smem_correction[i] = 1.f;
    }
    barrier();

    // loop on dst_seqlen / 16

    uint j = 0;
#if 1
    for (; j + (UNROLL_P_N - 1) < dst_seqlen_d16; j += UNROLL_P_N)
    {
        coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> qkm[UNROLL_SG_M][UNROLL_P_N];
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            [[unroll]] for (uint zp = 0; zp < UNROLL_P_N; zp++)
            {
                qkm[zm][zp] = coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(0.f);
            }
        }

        // QK
        {
            for (uint k = 0; k < embed_dim_d16; k++)
            {
                // load Q (w=embed_dim, h=16)
                // load K (w=embed_dim, h=16)

                for (uint idx = si; idx < UNROLL_SG_M * M * Kd8; idx += subgroup_size)
                {
                    const uint zm = idx / (M * Kd8);
                    const uint rowcol = idx % (M * Kd8);
                    const uint row = rowcol / Kd8;
                    const uint col = rowcol % Kd8;

                    const uint gm = (mi + zm) * M + row;
                    const uint gk = k * Kd8 + col;

                    uvec4 v = uvec4(0);
                    if (gm < p.src_seqlen && gk * 8 < p.embed_dim)
                    {
                        const uint qi = gz * (p.Q_cstep / 8) + gm * (p.embed_dim / 8) + gk;
                        v = Q_blob_data[qi];
                    }

                    tmp_q[((sgi * UNROLL_SG_M + zm) * M + row) * Kd8p + col] = v;
                }

                for (uint idx = li; idx < UNROLL_P_N * N * Kd8; idx += local_size)
                {
                    const uint zp = idx / (N * Kd8);
                    const uint rowcol = idx % (N * Kd8);
                    const uint row = rowcol / Kd8;
                    const uint col = rowcol % Kd8;

                    const uint gn = (j + zp) * N + row;
                    const uint gk = k * Kd8 + col;

                    uvec4 v = uvec4(0);
                    if (gn < p.dst_seqlen && gk * 8 < p.embed_dim)
                    {
                        const uint ki = kv_head_idx * (p.K_cstep / 8) + gn * (p.embed_dim / 8) + gk;
                        v = K_blob_data[ki];
                    }

                    tmp_k[(zp * N + row) * Kd8p + col] = v;
                }

                barrier();

#if NCNN_bf16_storage || NCNN_bf16_packed
                coopmat<bfloat16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> qm[UNROLL_SG_M];
                coopmat<bfloat16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> km;
#else
                coopmat<float16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> qm[UNROLL_SG_M];
                coopmat<float16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> km;
#endif

                [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
                {
                    coopMatLoad(qm[zm], tmp_q, (sgi * UNROLL_SG_M + zm) * M * Kd8p, Kd8p, gl_CooperativeMatrixLayoutRowMajor);
                }

                [[unroll]] for (uint zp = 0; zp < UNROLL_P_N; zp++)
                {
                    coopMatLoad(km, tmp_k, zp * N * Kd8p, Kd8p, gl_CooperativeMatrixLayoutColumnMajor);

                    [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
                    {
                        // calculate QK (w=16, h=16)
                        qkm[zm][zp] = coopMatMulAdd(qm[zm], km, qkm[zm][zp]);
                    }
                }
            }
        }

        // QK *= scale
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            [[unroll]] for (uint zp = 0; zp < UNROLL_P_N; zp++)
            {
                qkm[zm][zp] = qkm[zm][zp] * p.scale;
            }
        }

        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            [[unroll]] for (uint zp = 0; zp < UNROLL_P_N; zp++)
            {
                coopMatStore(qkm[zm][zp], tmp_s, ((sgi * UNROLL_SG_M + zm) * UNROLL_P_N + zp) * M * Np, Np, gl_CooperativeMatrixLayoutRowMajor);
            }
        }

        barrier();

        // QK += mask
        if (attn_mask != 0)
        {
            [[unroll]] for (uint idx = si; idx < UNROLL_SG_M * UNROLL_P_N * M * N; idx += subgroup_size)
            {
                const uint zm = idx / (UNROLL_P_N * M * N);
                const uint zprowcol = idx % (UNROLL_P_N * M * N);
                const uint zp = zprowcol / (M * N);
                const uint rowcol = zprowcol % (M * N);
                const uint row = rowcol / (N);
                const uint col = rowcol % (N);

                const uint gm = (mi + zm) * M + row;
                const uint gn = (j + zp) * N + col;

                float mask = 0.f;
                if (gm < p.src_seqlen && gn < p.dst_seqlen)
                {
                    const uint mask_head = (p.attn_mask_dims == 3) ? gz : 0;
                    const uint mmi = mask_head * p.mask_cstep + gm * p.dst_seqlen + gn;

                    mask = buffer_ld1(mask_blob_data, mmi);
                }

                tmp_s[(((sgi * UNROLL_SG_M + zm) * UNROLL_P_N + zp) * M + row) * Np + col] += mask;
            }

            barrier();
        }

        // ----------------------------------------
        // 3.5 Online Softmax - 计算 row max
        // ----------------------------------------
        [[unroll]] for (uint idx = si; idx < UNROLL_SG_M * M * UNROLL_P_N * N; idx += subgroup_size)
        {
            const uint zm = idx / (M * UNROLL_P_N * N);
            const uint rowcol = idx % (M * UNROLL_P_N * N);
            const uint row = rowcol / (UNROLL_P_N * N);
            const uint zpcol = rowcol % (UNROLL_P_N * N);
            const uint zp = zpcol / (N);
            const uint col = zpcol % (N);

            const uint gm = (mi + zm) * M + row;
            const uint gn = (j + zp) * N + col;

            float val = -3.402823e+38f;
            if (gm < p.src_seqlen && gn < p.dst_seqlen)
            {
                val = tmp_s[(((sgi * UNROLL_SG_M + zm) * UNROLL_P_N + zp) * M + row) * Np + col];
            }

            // 行内归约取 max
            if (UNROLL_P_N * N == subgroup_size)
            {
                float row_max_val = subgroupMax(val);

                // 第一个线程更新 SMEM
                if (subgroupElect())
                {
                    float old_max = smem_row_max[(sgi * UNROLL_SG_M + zm) * M + row];
                    float new_max = max(old_max, row_max_val);
                    smem_correction[(sgi * UNROLL_SG_M + zm) * M + row] = exp(old_max - new_max);
                    smem_row_max[(sgi * UNROLL_SG_M + zm) * M + row] = new_max;
                }
            }
            else
            {
                float row_max_val = reduceN_max(val, UNROLL_P_N * N);

                // 第一个线程更新 SMEM
                if (zpcol == 0)
                {
                    float old_max = smem_row_max[(sgi * UNROLL_SG_M + zm) * M + row];
                    float new_max = max(old_max, row_max_val);
                    smem_correction[(sgi * UNROLL_SG_M + zm) * M + row] = exp(old_max - new_max);
                    smem_row_max[(sgi * UNROLL_SG_M + zm) * M + row] = new_max;
                }
            }
        }

        barrier();

        // ----------------------------------------
        // 3.6 Online Softmax - 计算 exp 和 sum
        // ----------------------------------------
        [[unroll]] for (uint idx = si; idx < UNROLL_SG_M * M * UNROLL_P_N * N; idx += subgroup_size)
        {
            const uint zm = idx / (M * UNROLL_P_N * N);
            const uint rowcol = idx % (M * UNROLL_P_N * N);
            const uint row = rowcol / (UNROLL_P_N * N);
            const uint zpcol = rowcol % (UNROLL_P_N * N);
            const uint zp = zpcol / (N);
            const uint col = zpcol % (N);

            const uint gm = (mi + zm) * M + row;
            const uint gn = (j + zp) * N + col;

            float val = 0.f;
            if (gm < p.src_seqlen && gn < p.dst_seqlen)
            {
                float new_max = smem_row_max[(sgi * UNROLL_SG_M + zm) * M + row];

                val = tmp_s[(((sgi * UNROLL_SG_M + zm) * UNROLL_P_N + zp) * M + row) * Np + col];

                val = exp(val - new_max);
            }

            // 行内归约求 sum
            if (UNROLL_P_N * N == subgroup_size)
            {
                float row_sum_val = subgroupAdd(val);

                // 更新全局 sum
                if (subgroupElect())
                {
                    float correction = smem_correction[(sgi * UNROLL_SG_M + zm) * M + row];
                    smem_row_sum[(sgi * UNROLL_SG_M + zm) * M + row] = smem_row_sum[(sgi * UNROLL_SG_M + zm) * M + row] * correction + row_sum_val;
                }
            }
            else
            {
                float row_sum_val = reduceN_add(val, UNROLL_P_N * N);

                // 更新全局 sum
                if (zpcol == 0)
                {
                    float correction = smem_correction[(sgi * UNROLL_SG_M + zm) * M + row];
                    smem_row_sum[(sgi * UNROLL_SG_M + zm) * M + row] = smem_row_sum[(sgi * UNROLL_SG_M + zm) * M + row] * correction + row_sum_val;
                }
            }

            // 写回 P
            tmp_s[(((sgi * UNROLL_SG_M + zm) * UNROLL_P_N + zp) * M + row) * Np + col] = val;
        }

        barrier();

        // O = O * correction (matrix-vector)
        // ----------------------------------------
        // 3.6 Rescale O: O = O * correction
        // ----------------------------------------
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> cc;
            coopMatLoad(cc, smem_correction, (sgi * UNROLL_SG_M + zm) * M, 0, gl_CooperativeMatrixLayoutColumnMajor);

            [[unroll]] for (uint c = 0; c < MAX_OUT_CHUNKS; c++)
            {
                om[zm][c] = om[zm][c] * cc;
            }
        }

        // ----------------------------------------
        // 3.7 Load P from SMEM
        // ----------------------------------------
        // 需要将 FP32 的 tmp_s 转换为 FP16 给 CoopMat
        // 方案1: 使用 FP32 CoopMat (如果硬件支持)
        // 方案2: 转换为 FP16 packed buffer

        // 这里使用方案2: 将 tmp_s 转换为 packed FP16 format
        // 复用 tmp_o 的低半部分作为 FP16 buffer (32 uvec4 = 128 floats 空间够用)

        // 将 FP32 P 转换为 packed FP16
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            [[unroll]] for (uint zp = 0; zp < UNROLL_P_N; zp++)
            {
                coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> a;
                coopMatLoad(a, tmp_s, ((sgi * UNROLL_SG_M + zm) * UNROLL_P_N + zp) * M * Np, Np, gl_CooperativeMatrixLayoutRowMajor);

#if NCNN_bf16_storage || NCNN_bf16_packed
                coopmat<bfloat16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> b = coopmat<bfloat16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(a);
#else
                coopmat<float16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> b = coopmat<float16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(a);
#endif
                coopMatStore(b, tmp_s, ((sgi * UNROLL_SG_M + zm) * UNROLL_P_N + zp) * M * Np, Np, gl_CooperativeMatrixLayoutRowMajor);
            }
        }

        barrier();

        // 加载 FP16 P
#if NCNN_bf16_storage || NCNN_bf16_packed
        coopmat<bfloat16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> pm[UNROLL_SG_M][UNROLL_P_N];
#else
        coopmat<float16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> pm[UNROLL_SG_M][UNROLL_P_N];
#endif
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            [[unroll]] for (uint zp = 0; zp < UNROLL_P_N; zp++)
            {
                coopMatLoad(pm[zm][zp], tmp_s, ((sgi * UNROLL_SG_M + zm) * UNROLL_P_N + zp) * M * Np, Np, gl_CooperativeMatrixLayoutRowMajor);
            }
        }

        // qkv cross
        for (uint c = 0; c < MAX_OUT_CHUNKS; c++)
        {
            for (uint idx = li; idx < UNROLL_P_N * K * Nd8; idx += local_size)
            {
                const uint zp = idx / (K * Nd8);
                const uint rowcol = idx % (K * Nd8);
                const uint row = rowcol / Nd8;
                const uint col = rowcol % Nd8;

                const uint gn = (j + zp) * N + row;
                const uint gk = c * Nd8 + col;

                uvec4 v = uvec4(0);
                if (gn < p.dst_seqlen && gk * 8 < p.out_embed_dim)
                {
                    const uint vi = kv_head_idx * (p.V_cstep / 8) + gn * (p.out_embed_dim / 8) + gk;
                    v = V_blob_data[vi];
                }

                tmp_v[(zp * N + row) * Nd8p + col] = v;
            }

            barrier();

#if NCNN_bf16_storage || NCNN_bf16_packed
            // load V (w=16, h=16)
            coopmat<bfloat16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> vm;
#else
            coopmat<float16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> vm;
#endif

            [[unroll]] for (uint zp = 0; zp < UNROLL_P_N; zp++)
            {
                coopMatLoad(vm, tmp_v, zp * K * Nd8p, Nd8p, gl_CooperativeMatrixLayoutRowMajor);

                [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
                {
                    // calculate O += PV (w=16, h=16)
                    om[zm][c] = coopMatMulAdd(pm[zm][zp], vm, om[zm][c]);
                }
            }
        }
    }
#endif
    for (; j < dst_seqlen_d16; j++)
    {
        coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> qkm[UNROLL_SG_M];
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            qkm[zm] = coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(0.f);
        }

        // QK
        {
            for (uint k = 0; k < embed_dim_d16; k++)
            {
                // load Q (w=embed_dim, h=16)
                // load K (w=embed_dim, h=16)

                [[unroll]] for (uint idx = si; idx < UNROLL_SG_M * M * Kd8; idx += subgroup_size)
                {
                    const uint zm = idx / (M * Kd8);
                    const uint rowcol = idx % (M * Kd8);
                    const uint row = rowcol / Kd8;
                    const uint col = rowcol % Kd8;

                    const uint gm = (mi + zm) * M + row;
                    const uint gk = k * Kd8 + col;

                    uvec4 v = uvec4(0);
                    if (gm < p.src_seqlen && gk * 8 < p.embed_dim)
                    {
                        const uint qi = gz * (p.Q_cstep / 8) + gm * (p.embed_dim / 8) + gk;
                        v = Q_blob_data[qi];
                    }

                    tmp_q[((sgi * UNROLL_SG_M + zm) * M + row) * Kd8p + col] = v;
                }

                [[unroll]] for (uint idx = li; idx < N * Kd8; idx += local_size)
                {
                    const uint row = idx / Kd8;
                    const uint col = idx % Kd8;

                    const uint gn = j * N + row;
                    const uint gk = k * Kd8 + col;

                    uvec4 v = uvec4(0);
                    if (gn < p.dst_seqlen && gk * 8 < p.embed_dim)
                    {
                        const uint ki = kv_head_idx * (p.K_cstep / 8) + gn * (p.embed_dim / 8) + gk;
                        v = K_blob_data[ki];
                    }

                    tmp_k[row * Kd8p + col] = v;
                }

                barrier();

#if NCNN_bf16_storage || NCNN_bf16_packed
                coopmat<bfloat16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> qm;
                coopmat<bfloat16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> km;
#else
                coopmat<float16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> qm;
                coopmat<float16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> km;
#endif

                coopMatLoad(km, tmp_k, 0, Kd8p, gl_CooperativeMatrixLayoutColumnMajor);

                [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
                {
                    coopMatLoad(qm, tmp_q, (sgi * UNROLL_SG_M + zm) * M * Kd8p, Kd8p, gl_CooperativeMatrixLayoutRowMajor);

                    // calculate QK (w=16, h=16)
                    qkm[zm] = coopMatMulAdd(qm, km, qkm[zm]);
                }
            }
        }

        // QK *= scale
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            qkm[zm] = qkm[zm] * p.scale;
        }

        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            coopMatStore(qkm[zm], tmp_s, (sgi * UNROLL_SG_M + zm) * M * Np, Np, gl_CooperativeMatrixLayoutRowMajor);
        }

        barrier();

        // QK += mask
        if (attn_mask != 0)
        {
            [[unroll]] for (uint idx = si; idx < UNROLL_SG_M * M * N; idx += subgroup_size)
            {
                const uint zm = idx / (M * N);
                const uint rowcol = idx % (M * N);
                const uint row = rowcol / (N);
                const uint col = rowcol % (N);

                const uint gm = (mi + zm) * M + row;
                const uint gn = j * N + col;

                float mask = 0.f;
                if (gm < p.src_seqlen && gn < p.dst_seqlen)
                {
                    const uint mask_head = (p.attn_mask_dims == 3) ? gz : 0;
                    const uint mmi = mask_head * p.mask_cstep + gm * p.dst_seqlen + gn;

                    mask = buffer_ld1(mask_blob_data, mmi);
                }

                tmp_s[((sgi * UNROLL_SG_M + zm) * M + row) * Np + col] += mask;
            }

            barrier();
        }

        // ----------------------------------------
        // 3.5 Online Softmax - 计算 row max
        // ----------------------------------------
        [[unroll]] for (uint idx = si; idx < UNROLL_SG_M * M * N; idx += subgroup_size)
        {
            const uint zm = idx / (M * N);
            const uint rowcol = idx % (M * N);
            const uint row = rowcol / (N);
            const uint col = rowcol % (N);

            const uint gm = (mi + zm) * M + row;
            const uint gn = j * N + col;

            float val = -3.402823e+38f;
            if (gm < p.src_seqlen && gn < p.dst_seqlen)
            {
                val = tmp_s[((sgi * UNROLL_SG_M + zm) * M + row) * Np + col];
            }

            // 行内归约取 max
            float row_max_val = reduceN_max(val, N);

            // 第一个线程更新 SMEM
            if (col == 0)
            {
                float old_max = smem_row_max[(sgi * UNROLL_SG_M + zm) * M + row];
                float new_max = max(old_max, row_max_val);
                smem_correction[(sgi * UNROLL_SG_M + zm) * M + row] = exp(old_max - new_max);
                smem_row_max[(sgi * UNROLL_SG_M + zm) * M + row] = new_max;
            }
        }

        barrier();

        // ----------------------------------------
        // 3.6 Online Softmax - 计算 exp 和 sum
        // ----------------------------------------
        [[unroll]] for (uint idx = si; idx < UNROLL_SG_M * M * N; idx += subgroup_size)
        {
            const uint zm = idx / (M * N);
            const uint rowcol = idx % (M * N);
            const uint row = rowcol / (N);
            const uint col = rowcol % (N);

            const uint gm = (mi + zm) * M + row;
            const uint gn = j * N + col;

            float val = 0.f;
            if (gm < p.src_seqlen && gn < p.dst_seqlen)
            {
                float new_max = smem_row_max[(sgi * UNROLL_SG_M + zm) * M + row];

                val = tmp_s[((sgi * UNROLL_SG_M + zm) * M + row) * Np + col];

                val = exp(val - new_max);
            }

            // 行内归约求 sum
            float row_sum_val = reduceN_add(val, N);

            // 更新全局 sum
            if (col == 0)
            {
                float correction = smem_correction[(sgi * UNROLL_SG_M + zm) * M + row];
                smem_row_sum[(sgi * UNROLL_SG_M + zm) * M + row] = smem_row_sum[(sgi * UNROLL_SG_M + zm) * M + row] * correction + row_sum_val;
            }

            // 写回 P
            tmp_s[((sgi * UNROLL_SG_M + zm) * M + row) * Np + col] = val;
        }

        barrier();

        // O = O * correction (matrix-vector)
        // ----------------------------------------
        // 3.6 Rescale O: O = O * correction
        // ----------------------------------------
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> cc;
            coopMatLoad(cc, smem_correction, (sgi * UNROLL_SG_M + zm) * M, 0, gl_CooperativeMatrixLayoutColumnMajor);

            [[unroll]] for (uint c = 0; c < MAX_OUT_CHUNKS; c++)
            {
                om[zm][c] = om[zm][c] * cc;
            }
        }

        // ----------------------------------------
        // 3.7 Load P from SMEM
        // ----------------------------------------
        // 需要将 FP32 的 tmp_s 转换为 FP16 给 CoopMat
        // 方案1: 使用 FP32 CoopMat (如果硬件支持)
        // 方案2: 转换为 FP16 packed buffer

        // 这里使用方案2: 将 tmp_s 转换为 packed FP16 format
        // 复用 tmp_o 的低半部分作为 FP16 buffer (32 uvec4 = 128 floats 空间够用)

        // 将 FP32 P 转换为 packed FP16
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            coopmat<afp, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> a;
            coopMatLoad(a, tmp_s, (sgi * UNROLL_SG_M + zm) * M * Np, Np, gl_CooperativeMatrixLayoutRowMajor);

#if NCNN_bf16_storage || NCNN_bf16_packed
            coopmat<bfloat16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> b = coopmat<bfloat16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(a);
#else
            coopmat<float16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator> b = coopmat<float16_t, gl_ScopeSubgroup, M, N, gl_MatrixUseAccumulator>(a);
#endif
            coopMatStore(b, tmp_s, (sgi * UNROLL_SG_M + zm) * M * Np, Np, gl_CooperativeMatrixLayoutRowMajor);
        }

        barrier();

        // 加载 FP16 P
#if NCNN_bf16_storage || NCNN_bf16_packed
        coopmat<bfloat16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> pm[UNROLL_SG_M];
#else
        coopmat<float16_t, gl_ScopeSubgroup, M, K, gl_MatrixUseA> pm[UNROLL_SG_M];
#endif
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            coopMatLoad(pm[zm], tmp_s, (sgi * UNROLL_SG_M + zm) * M * Np, Np, gl_CooperativeMatrixLayoutRowMajor);
        }

        // qkv cross
        for (uint c = 0; c < MAX_OUT_CHUNKS; c++)
        {
            [[unroll]] for (uint idx = li; idx < K * Nd8; idx += local_size)
            {
                const uint row = idx / Nd8;
                const uint col = idx % Nd8;

                const uint gn = j * N + row;
                const uint gk = c * Nd8 + col;

                uvec4 v = uvec4(0);
                if (gn < p.dst_seqlen && gk * 8 < p.out_embed_dim)
                {
                    const uint vi = kv_head_idx * (p.V_cstep / 8) + gn * (p.out_embed_dim / 8) + gk;
                    v = V_blob_data[vi];
                }

                tmp_v[row * Nd8p + col] = v;
            }

            barrier();

#if NCNN_bf16_storage || NCNN_bf16_packed
            // load V (w=16, h=16)
            coopmat<bfloat16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> vm;
#else
            coopmat<float16_t, gl_ScopeSubgroup, K, N, gl_MatrixUseB> vm;
#endif

            coopMatLoad(vm, tmp_v, 0, Nd8p, gl_CooperativeMatrixLayoutRowMajor);

            [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
            {
                // calculate O += PV (w=16, h=16)
                om[zm][c] = coopMatMulAdd(pm[zm], vm, om[zm][c]);
            }
        }
    }

    // O = O / row_sum
    // store top_blob
    for (uint c = 0; c < MAX_OUT_CHUNKS; c++)
    {
        [[unroll]] for (uint zm = 0; zm < UNROLL_SG_M; zm++)
        {
            coopMatStore(om[zm][c], tmp_o, (sgi * UNROLL_SG_M + zm) * M * Nd4p, Nd4p, gl_CooperativeMatrixLayoutRowMajor);
        }

        barrier();

        // 并行归一化和存储
        // 归一化并转换为 FP16 输出
        [[unroll]] for (uint idx = si; idx < UNROLL_SG_M * M * (Nd4 / 2); idx += subgroup_size)
        {
            // 每个线程处理 8 个 float，输出 1 个 uvec4
            const uint zm = idx / (M * (Nd4 / 2));
            const uint rowcol = idx % (M * (Nd4 / 2));
            const uint row = rowcol / (Nd4 / 2);
            const uint col = rowcol % (Nd4 / 2);

            const uint out_row = (mi + zm) * M + row;
            const uint out_col = c * Nd8 + col;

            if (out_row < p.src_seqlen && out_col * 8 < p.out_embed_dim)
            {
                float inv_sum = 1.0f / smem_row_sum[(sgi * UNROLL_SG_M + zm) * M + row];

                // 读取 8 个 float
                vec4 v0 = tmp_o[(sgi * UNROLL_SG_M + zm) * M * Nd4p + row * Nd4p + col * 2 + 0] * inv_sum;
                vec4 v1 = tmp_o[(sgi * UNROLL_SG_M + zm) * M * Nd4p + row * Nd4p + col * 2 + 1] * inv_sum;

                // 打包为 uvec4 (8 个 FP16)
                uvec4 out_data;
#if NCNN_bf16_storage
                out_data.x = packBFloat2x16(v0.rg);
                out_data.y = packBFloat2x16(v0.ba);
                out_data.z = packBFloat2x16(v1.rg);
                out_data.w = packBFloat2x16(v1.ba);
#else
                out_data.x = packHalf2x16(v0.rg);
                out_data.y = packHalf2x16(v0.ba);
                out_data.z = packHalf2x16(v1.rg);
                out_data.w = packHalf2x16(v1.ba);
#endif

                const uint out_idx = gz * (p.out_cstep / 8) + out_row * (p.out_embed_dim / 8) + out_col;
                top_blob_data[out_idx] = out_data;
            }
        }
    }
}
