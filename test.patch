diff --git a/.github/workflows/linux-x64-cpu-clang.yml b/.github/workflows/linux-x64-cpu-clang.yml
index 318004fb..e8e02edd 100644
--- a/.github/workflows/linux-x64-cpu-clang.yml
+++ b/.github/workflows/linux-x64-cpu-clang.yml
@@ -67,6 +67,16 @@ jobs:
         cmake --build . -j 2
     - name: test-avx2
       run: cd build-avx2 && ctest --output-on-failure -j 2
+     - name: build-avx
+      env:
+        CC: clang
+        CXX: clang++
+      run: |
+        mkdir build-avx && cd build-avx
+        cmake -DNCNN_AVX2=OFF -DNCNN_AVX=ON -DNCNN_BUILD_TESTS=ON ..
+        cmake --build . -j 2
+    - name: test-avx
+      run: cd build-avx && ctest --output-on-failure -j 2
     - name: build-noint8
       env:
         CC: clang
diff --git a/.github/workflows/linux-x64-cpu-gcc.yml b/.github/workflows/linux-x64-cpu-gcc.yml
index 6849cb7d..4a72076e 100644
--- a/.github/workflows/linux-x64-cpu-gcc.yml
+++ b/.github/workflows/linux-x64-cpu-gcc.yml
@@ -58,6 +58,13 @@ jobs:
         cmake --build . -j 2
     - name: test-avx2
       run: cd build-avx2 && ctest --output-on-failure -j 2
+    - name: build-avx
+      run: |
+        mkdir build-avx && cd build-avx
+        cmake -DNCNN_AVX2=OFF -DNCNN_AVX=ON -DNCNN_BUILD_TESTS=ON ..
+        cmake --build . -j 2
+    - name: test-avx
+      run: cd build-avx && ctest --output-on-failure -j 2
     - name: build-noint8
       run: |
         mkdir build-noint8 && cd build-noint8
diff --git a/.github/workflows/release.yml b/.github/workflows/release.yml
index e3e9b679..dc7ac5bd 100644
--- a/.github/workflows/release.yml
+++ b/.github/workflows/release.yml
@@ -1291,7 +1291,7 @@ jobs:
         source emsdk/emsdk_env.sh
         mkdir build && cd build
         cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=install -DNCNN_VERSION_STRING="${{ needs.setup.outputs.VERSION }}" \
-            -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF \
+            -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF  -DNCNN_AV2=OFF \
             -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF -DNCNN_BUILD_BENCHMARK=OFF ..
         cmake --build . -j 2
         cmake --build . --target install
@@ -1300,7 +1300,7 @@ jobs:
         source emsdk/emsdk_env.sh
         mkdir build-simd && cd build-simd
         cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=install -DNCNN_VERSION_STRING="${{ needs.setup.outputs.VERSION }}" \
-            -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF \
+            -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF -DNCNN_AV2=OFF \
             -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF -DNCNN_BUILD_BENCHMARK=OFF ..
         cmake --build . -j 2
         cmake --build . --target install
@@ -1309,7 +1309,7 @@ jobs:
         source emsdk/emsdk_env.sh
         mkdir build-threads && cd build-threads
         cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=install -DNCNN_VERSION_STRING="${{ needs.setup.outputs.VERSION }}" \
-            -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF \
+            -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF -DNCNN_AV2=OFF \
             -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF -DNCNN_BUILD_BENCHMARK=OFF ..
         cmake --build . -j 2
         cmake --build . --target install
@@ -1318,7 +1318,7 @@ jobs:
         source emsdk/emsdk_env.sh
         mkdir build-simd-threads && cd build-simd-threads
         cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=install -DNCNN_VERSION_STRING="${{ needs.setup.outputs.VERSION }}" \
-            -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF \
+            -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF -DNCNN_AV2=OFF \
             -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF -DNCNN_BUILD_BENCHMARK=OFF ..
         cmake --build . -j 2
         cmake --build . --target install
diff --git a/.github/workflows/test-coverage.yml b/.github/workflows/test-coverage.yml
index 3163c548..b2c8e989 100644
--- a/.github/workflows/test-coverage.yml
+++ b/.github/workflows/test-coverage.yml
@@ -146,6 +146,33 @@ jobs:
       with:
         token: ${{ secrets.CODECOV_TOKEN }}
         file: build/lcov.info
+  linux-gcc-avx-omp:
+    runs-on: ubuntu-latest
+    steps:
+    - name: cancel-previous-runs
+      uses: styfle/cancel-workflow-action@0.9.0
+      with:
+        access_token: ${{ secrets.GITHUB_TOKEN }}
+    - uses: actions/checkout@v2
+    - name: lcov
+      run: sudo apt-get install lcov
+    - name: configure
+      run: mkdir build && cd build && cmake -DCMAKE_BUILD_TYPE=debug -DNCNN_COVERAGE=ON -DNCNN_RUNTIME_CPU=ON -DNCNN_AVX2=OFF -DNCNN_AVX=ON -DNCNN_OPENMP=ON -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF -DNCNN_BUILD_TESTS=ON ..
+    - name: build
+      run: cmake --build build -j 2
+    - name: test
+      run: cd build && ctest --output-on-failure -j 2
+    - name: lcov-collect
+      run: |
+        cd build
+        lcov -d ./src -c -o lcov.info
+        lcov -r lcov.info '/usr/*' -o lcov.info
+        lcov --list lcov.info
+    - name: codecov
+      uses: codecov/codecov-action@v1.5.2
+      with:
+        token: ${{ secrets.CODECOV_TOKEN }}
+        file: build/lcov.info
 
   linux-gcc-arm:
     runs-on: ubuntu-20.04
diff --git a/.github/workflows/web-assembly.yml b/.github/workflows/web-assembly.yml
index c8732d19..5919c6d6 100644
--- a/.github/workflows/web-assembly.yml
+++ b/.github/workflows/web-assembly.yml
@@ -39,7 +39,7 @@ jobs:
       run: |
         source emsdk/emsdk_env.sh
         mkdir build-basic && cd build-basic
-        cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF -DNCNN_BUILD_TESTS=ON ..
+        cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF -DNCNN_AVX=OFF -DNCNN_BUILD_TESTS=ON ..
         cmake --build . -j 2
     - name: test-basic
       run: |
@@ -49,7 +49,7 @@ jobs:
       run: |
         source emsdk/emsdk_env.sh
         mkdir build-simd && cd build-simd
-        cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF -DNCNN_BUILD_TESTS=ON ..
+        cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF -DNCNN_AVX=OFF -DNCNN_BUILD_TESTS=ON ..
         cmake --build . -j 2
     - name: test-simd
       run: |
@@ -59,7 +59,7 @@ jobs:
       run: |
         source emsdk/emsdk_env.sh
         mkdir build-simd-omp && cd build-simd-omp
-        cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF -DNCNN_BUILD_TESTS=ON ..
+        cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_SIMPLEOCV=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF -DNCNN_AVX=OFF -DNCNN_BUILD_TESTS=ON ..
         cmake --build . -j 2
     - name: test-simd-omp
       run: |
diff --git a/.github/workflows/windows-x64-cpu-vs2015.yml b/.github/workflows/windows-x64-cpu-vs2015.yml
index b7238028..b1694588 100644
--- a/.github/workflows/windows-x64-cpu-vs2015.yml
+++ b/.github/workflows/windows-x64-cpu-vs2015.yml
@@ -51,7 +51,7 @@ jobs:
     - name: build-sse2
       run: |
         mkdir build-sse2; cd build-sse2
-        cmake -T v140,host=x64 -A x64 -DProtobuf_INCLUDE_DIR="$env:GITHUB_WORKSPACE\protobuf-install\include" -DProtobuf_LIBRARIES="$env:GITHUB_WORKSPACE\protobuf-install\lib\libprotobuf.lib" -DProtobuf_PROTOC_EXECUTABLE="$env:GITHUB_WORKSPACE\protobuf-install\bin\protoc.exe" -DNCNN_RUNTIME_CPU=OFF -DNCNN_AVX2=OFF -DNCNN_BUILD_TESTS=ON ..
+        cmake -T v140,host=x64 -A x64 -DProtobuf_INCLUDE_DIR="$env:GITHUB_WORKSPACE\protobuf-install\include" -DProtobuf_LIBRARIES="$env:GITHUB_WORKSPACE\protobuf-install\lib\libprotobuf.lib" -DProtobuf_PROTOC_EXECUTABLE="$env:GITHUB_WORKSPACE\protobuf-install\bin\protoc.exe" -DNCNN_RUNTIME_CPU=OFF -DNCNN_AVX2=OFF -DNCNN_AVX=OFF -DNCNN_BUILD_TESTS=ON ..
         cmake --build . --config Release -j 2
     - name: test-sse2
       run: cd build-sse2; ctest -C Release --output-on-failure -j 2
@@ -67,3 +67,10 @@ jobs:
         cmake --build . --config Release -j 2
     - name: test-avx2
       run: cd build-avx2; ctest -C Release --output-on-failure -j 2
+    - name: build-avx
+      run: |
+        mkdir build-avx; cd build-avx
+        cmake -T v140,host=x64 -A x64 -DProtobuf_INCLUDE_DIR="$env:GITHUB_WORKSPACE\protobuf-install\include" -DProtobuf_LIBRARIES="$env:GITHUB_WORKSPACE\protobuf-install\lib\libprotobuf.lib" -DProtobuf_PROTOC_EXECUTABLE="$env:GITHUB_WORKSPACE\protobuf-install\bin\protoc.exe" -DNCNN_RUNTIME_CPU=OFF -DNCNN_AVX2=OFF -DNCNN_AVX=ON -DNCNN_BUILD_TESTS=ON ..
+        cmake --build . --config Release -j 2
+    - name: test-avx
+      run: cd build-avx; ctest -C Release --output-on-failure -j 2
diff --git a/.github/workflows/windows-x64-cpu-vs2017.yml b/.github/workflows/windows-x64-cpu-vs2017.yml
index abc7e23b..7c343588 100644
--- a/.github/workflows/windows-x64-cpu-vs2017.yml
+++ b/.github/workflows/windows-x64-cpu-vs2017.yml
@@ -51,7 +51,7 @@ jobs:
     - name: build-sse2
       run: |
         mkdir build-sse2; cd build-sse2
-        cmake -T v141,host=x64 -A x64 -DProtobuf_INCLUDE_DIR="$env:GITHUB_WORKSPACE\protobuf-install\include" -DProtobuf_LIBRARIES="$env:GITHUB_WORKSPACE\protobuf-install\lib\libprotobuf.lib" -DProtobuf_PROTOC_EXECUTABLE="$env:GITHUB_WORKSPACE\protobuf-install\bin\protoc.exe" -DNCNN_RUNTIME_CPU=OFF -DNCNN_AVX2=OFF -DNCNN_BUILD_TESTS=ON ..
+        cmake -T v141,host=x64 -A x64 -DProtobuf_INCLUDE_DIR="$env:GITHUB_WORKSPACE\protobuf-install\include" -DProtobuf_LIBRARIES="$env:GITHUB_WORKSPACE\protobuf-install\lib\libprotobuf.lib" -DProtobuf_PROTOC_EXECUTABLE="$env:GITHUB_WORKSPACE\protobuf-install\bin\protoc.exe" -DNCNN_RUNTIME_CPU=OFF -DNCNN_AVX2=OFF -DNCNN_AVX=OFF -DNCNN_BUILD_TESTS=ON ..
         cmake --build . --config Release -j 2
     - name: test-sse2
       run: cd build-sse2; ctest -C Release --output-on-failure -j 2
@@ -67,3 +67,11 @@ jobs:
         cmake --build . --config Release -j 2
     - name: test-avx2
       run: cd build-avx2; ctest -C Release --output-on-failure -j 2
+    - name: build-avx
+      run: |
+        mkdir build-avx; cd build-avx
+        cmake -T v141,host=x64 -A x64 -DProtobuf_INCLUDE_DIR="$env:GITHUB_WORKSPACE\protobuf-install\include" -DProtobuf_LIBRARIES="$env:GITHUB_WORKSPACE\protobuf-install\lib\libprotobuf.lib" -DProtobuf_PROTOC_EXECUTABLE="$env:GITHUB_WORKSPACE\protobuf-install\bin\protoc.exe" -DNCNN_RUNTIME_CPU=OFF -DNCNN_AVX2=OFF -DNCNN_AVX=ON -DNCNN_BUILD_TESTS=ON ..
+        cmake --build . --config Release -j 2
+    - name: test-avx
+      run: cd build-avx; ctest -C Release --output-on-failure -j 2
+    
diff --git a/.github/workflows/windows-x64-cpu-vs2019.yml b/.github/workflows/windows-x64-cpu-vs2019.yml
index a3263f6a..7e87eae6 100644
--- a/.github/workflows/windows-x64-cpu-vs2019.yml
+++ b/.github/workflows/windows-x64-cpu-vs2019.yml
@@ -51,7 +51,7 @@ jobs:
     - name: build-sse2
       run: |
         mkdir build-sse2; cd build-sse2
-        cmake -DProtobuf_INCLUDE_DIR="$env:GITHUB_WORKSPACE\protobuf-install\include" -DProtobuf_LIBRARIES="$env:GITHUB_WORKSPACE\protobuf-install\lib\libprotobuf.lib" -DProtobuf_PROTOC_EXECUTABLE="$env:GITHUB_WORKSPACE\protobuf-install\bin\protoc.exe" -DNCNN_RUNTIME_CPU=OFF -DNCNN_AVX2=OFF -DNCNN_BUILD_TESTS=ON ..
+        cmake -DProtobuf_INCLUDE_DIR="$env:GITHUB_WORKSPACE\protobuf-install\include" -DProtobuf_LIBRARIES="$env:GITHUB_WORKSPACE\protobuf-install\lib\libprotobuf.lib" -DProtobuf_PROTOC_EXECUTABLE="$env:GITHUB_WORKSPACE\protobuf-install\bin\protoc.exe" -DNCNN_RUNTIME_CPU=OFF -DNCNN_AVX2=OFF -DNCNN_AVX=OFF -DNCNN_BUILD_TESTS=ON ..
         cmake --build . --config Release -j 2
     - name: test-sse2
       run: cd build-sse2; ctest -C Release --output-on-failure -j 2
@@ -67,3 +67,10 @@ jobs:
         cmake --build . --config Release -j 2
     - name: test-avx2
       run: cd build-avx2; ctest -C Release --output-on-failure -j 2
+    - name: build-avx
+      run: |
+        mkdir build-avx; cd build-avx
+        cmake -DProtobuf_INCLUDE_DIR="$env:GITHUB_WORKSPACE\protobuf-install\include" -DProtobuf_LIBRARIES="$env:GITHUB_WORKSPACE\protobuf-install\lib\libprotobuf.lib" -DProtobuf_PROTOC_EXECUTABLE="$env:GITHUB_WORKSPACE\protobuf-install\bin\protoc.exe" -DNCNN_RUNTIME_CPU=OFF -DNCNN_AVX=ON -DNCNN_AVX2=OFF -DNCNN_BUILD_TESTS=ON ..
+        cmake --build . --config Release -j 2
+    - name: test-avx
+      run: cd build-avx; ctest -C Release --output-on-failure -j 2
diff --git a/CMakeLists.txt b/CMakeLists.txt
index 6d2c3de4..5309c5b4 100644
--- a/CMakeLists.txt
+++ b/CMakeLists.txt
@@ -183,6 +183,7 @@ else()
     option(NCNN_SSE2 "optimize x86 platform with sse2" ON)
     if(NOT CMAKE_SYSTEM_NAME STREQUAL "Emscripten" AND NOT (CMAKE_CXX_COMPILER_ID MATCHES "GNU" AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 4.7))
         option(NCNN_AVX2 "optimize x86 platform with avx2" ON)
+        option(NCNN_AVX "optimize x86 platform with avx" ON)
     endif()
 endif()
 
diff --git a/cmake/ncnn_add_layer.cmake b/cmake/ncnn_add_layer.cmake
index 9affe324..e8538696 100644
--- a/cmake/ncnn_add_layer.cmake
+++ b/cmake/ncnn_add_layer.cmake
@@ -152,8 +152,17 @@ macro(ncnn_add_layer class)
     if(NCNN_RUNTIME_CPU AND NCNN_AVX2 AND NCNN_TARGET_ARCH STREQUAL "x86")
         if(CMAKE_CXX_COMPILER_ID MATCHES "MSVC" OR (CMAKE_CXX_COMPILER_ID MATCHES "Clang" AND CMAKE_CXX_SIMULATE_ID MATCHES "MSVC" AND CMAKE_CXX_COMPILER_FRONTEND_VARIANT MATCHES "MSVC"))
             ncnn_add_arch_opt_layer(${class} avx2 "/arch:AVX2 /DAVX2 /fp:strict")
+            ncnn_add_arch_opt_layer(${class} avx "/arch:AVX /DAVX /fp:strict")            
         else()
             ncnn_add_arch_opt_layer(${class} avx2 "-mfma -mf16c -mavx2")
+            ncnn_add_arch_opt_layer(${class} avx "-mavx")
+        endif()
+    endif()
+    if(NCNN_RUNTIME_CPU AND NOT NCNN_AVX2 AND  NCNN_AVX AND NCNN_TARGET_ARCH STREQUAL "x86")
+        if(CMAKE_CXX_COMPILER_ID MATCHES "MSVC" OR (CMAKE_CXX_COMPILER_ID MATCHES "Clang" AND CMAKE_CXX_SIMULATE_ID MATCHES "MSVC" AND CMAKE_CXX_COMPILER_FRONTEND_VARIANT MATCHES "MSVC"))
+            ncnn_add_arch_opt_layer(${class} avx "/arch:AVX /DAVX /fp:strict")            
+        else()
+            ncnn_add_arch_opt_layer(${class} avx "-mavx")
         endif()
     endif()
 
diff --git a/cmake/ncnn_generate_avx_source.cmake b/cmake/ncnn_generate_avx_source.cmake
new file mode 100644
index 00000000..9a359857
--- /dev/null
+++ b/cmake/ncnn_generate_avx_source.cmake
@@ -0,0 +1,14 @@
+
+# must define SRC DST CLASS
+
+file(READ ${SRC} source_data)
+
+# replace
+string(TOUPPER ${CLASS} CLASS_UPPER)
+string(TOLOWER ${CLASS} CLASS_LOWER)
+
+string(REGEX REPLACE "LAYER_${CLASS_UPPER}_X86_H" "LAYER_${CLASS_UPPER}_X86_AVX_H" source_data "${source_data}")
+string(REGEX REPLACE "${CLASS}_x86" "${CLASS}_x86_avx" source_data "${source_data}")
+string(REGEX REPLACE "#include \"${CLASS_LOWER}_x86.h\"" "#include \"${CLASS_LOWER}_x86_avx.h\"" source_data "${source_data}")
+
+file(WRITE ${DST} "${source_data}")
diff --git a/docs/how-to-build/how-to-build.md b/docs/how-to-build/how-to-build.md
index 9cd4b2b3..e79430af 100644
--- a/docs/how-to-build/how-to-build.md
+++ b/docs/how-to-build/how-to-build.md
@@ -515,7 +515,7 @@ Build without any extension for general compatibility:
 mkdir -p build
 cd build
 cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake \
-    -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF \
+    -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF -DNCNN_AVX=OFF \
     -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF -DNCNN_BUILD_BENCHMARK=OFF ..
 cmake --build . -j 4
 cmake --build . --target install
@@ -526,7 +526,7 @@ Build with WASM SIMD extension:
 mkdir -p build-simd
 cd build-simd
 cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake \
-    -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF \
+    -DNCNN_THREADS=OFF -DNCNN_OPENMP=OFF -DNCNN_SIMPLEOMP=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF -DNCNN_AVX=OFF \
     -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF -DNCNN_BUILD_BENCHMARK=OFF ..
 cmake --build . -j 4
 cmake --build . --target install
@@ -537,7 +537,7 @@ Build with WASM Thread extension:
 mkdir -p build-threads
 cd build-threads
 cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake \
-    -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF \
+    -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=OFF -DNCNN_AVX2=OFF -DNCNN_AVX=OFF \
     -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF -DNCNN_BUILD_BENCHMARK=OFF ..
 cmake --build . -j 4
 cmake --build . --target install
@@ -548,7 +548,7 @@ Build with WASM SIMD and Thread extension:
 mkdir -p build-simd-threads
 cd build-simd-threads
 cmake -DCMAKE_TOOLCHAIN_FILE=../emsdk/upstream/emscripten/cmake/Modules/Platform/Emscripten.cmake \
-    -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF \
+    -DNCNN_THREADS=ON -DNCNN_OPENMP=ON -DNCNN_SIMPLEOMP=ON -DNCNN_RUNTIME_CPU=OFF -DNCNN_SSE2=ON -DNCNN_AVX2=OFF -DNCNN_AVX=OFF \
     -DNCNN_BUILD_TOOLS=OFF -DNCNN_BUILD_EXAMPLES=OFF -DNCNN_BUILD_BENCHMARK=OFF ..
 cmake --build . -j 4
 cmake --build . --target install
diff --git a/python/src/main.cpp b/python/src/main.cpp
index cb39cc28..f18135f2 100644
--- a/python/src/main.cpp
+++ b/python/src/main.cpp
@@ -903,6 +903,7 @@ PYBIND11_MODULE(ncnn, m)
     m.def("cpu_support_arm_vfpv4", &cpu_support_arm_vfpv4);
     m.def("cpu_support_arm_asimdhp", &cpu_support_arm_asimdhp);
     m.def("cpu_support_x86_avx2", &cpu_support_x86_avx2);
+    m.def("cpu_support_x86_avx", &cpu_support_x86_avx);
     m.def("get_cpu_count", &get_cpu_count);
     m.def("get_little_cpu_count", &get_little_cpu_count);
     m.def("get_big_cpu_count", &get_big_cpu_count);
diff --git a/src/CMakeLists.txt b/src/CMakeLists.txt
index 28c827db..5dac4831 100644
--- a/src/CMakeLists.txt
+++ b/src/CMakeLists.txt
@@ -342,6 +342,13 @@ if(NCNN_TARGET_ARCH STREQUAL "x86")
             target_compile_options(ncnn PRIVATE -mfma -mf16c -mavx2)
         endif()
     endif()
+    if(NOT NCNN_RUNTIME_CPU AND NOT NCNN_AVX2 AND NCNN_AVX)
+        if(CMAKE_CXX_COMPILER_ID MATCHES "MSVC" OR (CMAKE_CXX_COMPILER_ID MATCHES "Clang" AND CMAKE_CXX_SIMULATE_ID MATCHES "MSVC" AND CMAKE_CXX_COMPILER_FRONTEND_VARIANT MATCHES "MSVC"))
+            target_compile_options(ncnn PRIVATE /arch:AVX)
+        else()
+            target_compile_options(ncnn PRIVATE -mavx)
+        endif()
+    endif()
 endif()
 
 if(((IOS AND CMAKE_OSX_ARCHITECTURES MATCHES "arm64") OR (APPLE AND CMAKE_OSX_ARCHITECTURES MATCHES "arm64") OR (CMAKE_SYSTEM_PROCESSOR MATCHES "^(arm64|aarch64)")))
diff --git a/src/cpu.cpp b/src/cpu.cpp
index 732380a5..9ec57ae3 100644
--- a/src/cpu.cpp
+++ b/src/cpu.cpp
@@ -365,6 +365,7 @@ int cpu_support_arm_asimddp()
 
 int cpu_support_x86_avx2()
 {
+    return 0;
 #if (_M_AMD64 || __x86_64__) || (_M_IX86 || __i386__)
 #if defined(_MSC_VER)
     // TODO move to init function
@@ -404,6 +405,46 @@ int cpu_support_x86_avx2()
 #endif
 }
 
+int cpu_support_x86_avx()
+{
+    return 0;
+#if (_M_AMD64 || __x86_64__) || (_M_IX86 || __i386__)
+#if defined(_MSC_VER)
+    // TODO move to init function
+    int cpu_info[4];
+    __cpuid(cpu_info, 0);
+
+    int nIds = cpu_info[0];
+    if (nIds < 7)
+        return 0;
+
+    __cpuid(cpu_info, 1);
+    // check AVX XSAVE OSXSAVE
+    if (!(cpu_info[2] & 0x10000000) || !(cpu_info[2] & 0x04000000) || !(cpu_info[2] & 0x08000000))
+        return 0;
+
+    // check XSAVE enabled by kernel
+    if ((_xgetbv(0) & 6) != 6)
+        return 0;
+    return 1;
+#elif defined(__clang__)
+#if __clang_major__ >= 6
+    __builtin_cpu_init();
+#endif
+    return __builtin_cpu_supports("avx");
+#elif __GNUC__ > 4 || (__GNUC__ == 4 && __GNUC_MINOR__ >= 8)
+    __builtin_cpu_init();
+    return __builtin_cpu_supports("avx");
+#else
+    // TODO: other x86 compilers checking avx here
+    NCNN_LOGE("AVX detection method is unknown for current compiler");
+    return 0;
+#endif
+#else
+    return 0;
+#endif
+}
+
 int cpu_support_mips_msa()
 {
 #if defined __ANDROID__ || defined __linux__
@@ -821,7 +862,7 @@ int set_cpu_thread_affinity(const CpuSet& thread_affinity_mask)
     // set affinity for each thread
     set_omp_num_threads(num_threads);
     std::vector<int> ssarets(num_threads, 0);
-    #pragma omp parallel for num_threads(num_threads)
+#pragma omp parallel for num_threads(num_threads)
     for (int i = 0; i < num_threads; i++)
     {
         ssarets[i] = set_sched_affinity(thread_affinity_mask);
@@ -846,7 +887,7 @@ int set_cpu_thread_affinity(const CpuSet& thread_affinity_mask)
     // set affinity for each thread
     set_omp_num_threads(num_threads);
     std::vector<int> ssarets(num_threads, 0);
-    #pragma omp parallel for num_threads(num_threads)
+#pragma omp parallel for num_threads(num_threads)
     for (int i = 0; i < num_threads; i++)
     {
         // assign one core for each thread
diff --git a/src/cpu.h b/src/cpu.h
index 723a2fc5..90f3a938 100644
--- a/src/cpu.h
+++ b/src/cpu.h
@@ -57,6 +57,9 @@ NCNN_EXPORT int cpu_support_arm_asimddp();
 // avx2 = x86_64 avx2 + fma + f16c
 NCNN_EXPORT int cpu_support_x86_avx2();
 
+// avx = x86_64 avx
+NCNN_EXPORT int cpu_support_x86_avx();
+
 // msa = mips mas
 NCNN_EXPORT int cpu_support_mips_msa();
 // mmi = loongson mmi
diff --git a/src/layer.cpp b/src/layer.cpp
index 367abebb..9c452248 100644
--- a/src/layer.cpp
+++ b/src/layer.cpp
@@ -236,9 +236,15 @@ Layer* create_layer(int index)
     if (ncnn::cpu_support_x86_avx2())
     {
         layer_creator = layer_registry_avx2[index].creator;
+    } else
+#endif// NCNN_RUNTIME_CPU && NCNN_AVX2
+#if NCNN_RUNTIME_CPU && (NCNN_AVX2 || NCNN_AVX)
+    if (ncnn::cpu_support_x86_avx())
+    {
+        layer_creator = layer_registry_avx[index].creator;
     }
     else
-#endif // NCNN_RUNTIME_CPU && NCNN_AVX2
+#endif // NCNN_RUNTIME_CPU && (NCNN_AVX2 || NCNN_AVX)
 #if NCNN_RUNTIME_CPU && NCNN_ARM82DOT
     if (ncnn::cpu_support_arm_asimdhp() && ncnn::cpu_support_arm_asimddp())
     {
diff --git a/src/layer/x86/avx_mathfun.h b/src/layer/x86/avx_mathfun.h
index c978d917..9f1a68e4 100644
--- a/src/layer/x86/avx_mathfun.h
+++ b/src/layer/x86/avx_mathfun.h
@@ -91,6 +91,45 @@ _PS256_CONST(cephes_log_p8, +3.3333331174E-1f);
 _PS256_CONST(cephes_log_q1, -2.12194440e-4f);
 _PS256_CONST(cephes_log_q2, 0.693359375f);
 
+#ifndef __AVX2__
+#define AVX2_BITOP_USING_SSE2(fn)                            \
+    static inline __m256i _mm256_comp_##fn(__m256i x, int a) \
+    {                                                        \
+        /* use SSE2 instruction to perform the bitop AVX2 */ \
+        __m128i x1, x2;                                      \
+        __m256i ret;                                         \
+        COPY_IMM_TO_XMM(x, x1, x2);                          \
+        x1 = _mm_##fn(x1, a);                                \
+        x2 = _mm_##fn(x2, a);                                \
+        COPY_XMM_TO_IMM(x1, x2, ret);                        \
+        return (ret);                                        \
+    }
+#define AVX2_INTOP_USING_SSE2(fn)                                         \
+    static inline __m256i _mm256_comp_##fn(__m256i x, __m256i y)          \
+    {                                                                     \
+        /* use SSE2 instructions to perform the AVX2 integer operation */ \
+        __m128i x1, x2;                                                   \
+        __m128i y1, y2;                                                   \
+        __m256i ret;                                                      \
+        COPY_IMM_TO_XMM(x, x1, x2);                                       \
+        COPY_IMM_TO_XMM(y, y1, y2);                                       \
+        x1 = _mm_##fn(x1, y1);                                            \
+        x2 = _mm_##fn(x2, y2);                                            \
+        COPY_XMM_TO_IMM(x1, x2, ret);                                     \
+        return (ret);                                                     \
+    }
+#else
+#define AVX2_BITOP_USING_SSE2(fn)                            \
+    static inline __m256i _mm256_comp_##fn(__m256i x, int a) \
+    {                                                        \
+        return _mm256_##fn(x, a);                            \
+    }
+#define AVX2_INTOP_USING_SSE2(fn)                                \
+    static inline __m256i _mm256_comp_##fn(__m256i x, __m256i y) \
+    {                                                            \
+        return _mm256_##fn(x, y);                                \
+    }
+#endif
 #ifndef __AVX2__
 
 typedef union imm_xmm_union
@@ -115,54 +154,28 @@ typedef union imm_xmm_union
         imm_ = u.imm;                            \
     }
 
-#define AVX2_BITOP_USING_SSE2(fn)                            \
-    static inline __m256i _mm256_##fn(__m256i x, int a)      \
-    {                                                        \
-        /* use SSE2 instruction to perform the bitop AVX2 */ \
-        __m128i x1, x2;                                      \
-        __m256i ret;                                         \
-        COPY_IMM_TO_XMM(x, x1, x2);                          \
-        x1 = _mm_##fn(x1, a);                                \
-        x2 = _mm_##fn(x2, a);                                \
-        COPY_XMM_TO_IMM(x1, x2, ret);                        \
-        return (ret);                                        \
-    }
-
 #if _MSC_VER
 #pragma WARNING(Using SSE2 to perform AVX2 bitshift ops)
 #else
 #warning "Using SSE2 to perform AVX2 bitshift ops"
 #endif
-AVX2_BITOP_USING_SSE2(slli_epi32)
-AVX2_BITOP_USING_SSE2(srli_epi32)
-
-#define AVX2_INTOP_USING_SSE2(fn)                                         \
-    static inline __m256i _mm256_##fn(__m256i x, __m256i y)               \
-    {                                                                     \
-        /* use SSE2 instructions to perform the AVX2 integer operation */ \
-        __m128i x1, x2;                                                   \
-        __m128i y1, y2;                                                   \
-        __m256i ret;                                                      \
-        COPY_IMM_TO_XMM(x, x1, x2);                                       \
-        COPY_IMM_TO_XMM(y, y1, y2);                                       \
-        x1 = _mm_##fn(x1, y1);                                            \
-        x2 = _mm_##fn(x2, y2);                                            \
-        COPY_XMM_TO_IMM(x1, x2, ret);                                     \
-        return (ret);                                                     \
-    }
 
 #if _MSC_VER
 #pragma WARNING(Using SSE2 to perform AVX2 bitshift ops)
 #else
 #warning "Using SSE2 to perform AVX2 integer ops"
 #endif
-AVX2_INTOP_USING_SSE2(and_si128)
-AVX2_INTOP_USING_SSE2(andnot_si128)
+#endif /* __AVX2__ */
+AVX2_BITOP_USING_SSE2(slli_epi32)
+AVX2_BITOP_USING_SSE2(srli_epi32)
 AVX2_INTOP_USING_SSE2(cmpeq_epi32)
 AVX2_INTOP_USING_SSE2(sub_epi32)
 AVX2_INTOP_USING_SSE2(add_epi32)
 
-#endif /* __AVX2__ */
+#ifndef __AVX2__
+AVX2_INTOP_USING_SSE2(and_si128)
+AVX2_INTOP_USING_SSE2(andnot_si128)
+#endif
 
 /* natural logarithm computed for 8 simultaneous float
    return NaN for x <= 0
@@ -178,14 +191,14 @@ static inline __m256 log256_ps(__m256 x)
     x = _mm256_max_ps(x, *(__m256*)_ps256_min_norm_pos); /* cut off denormalized stuff */
 
     // can be done with AVX2
-    imm0 = _mm256_srli_epi32(_mm256_castps_si256(x), 23);
+    imm0 = _mm256_comp_srli_epi32(_mm256_castps_si256(x), 23);
 
     /* keep only the fractional part */
     x = _mm256_and_ps(x, *(__m256*)_ps256_inv_mant_mask);
     x = _mm256_or_ps(x, *(__m256*)_ps256_0p5);
 
     // this is again another AVX2 instruction
-    imm0 = _mm256_sub_epi32(imm0, *(__m256i*)_pi32_256_0x7f);
+    imm0 = _mm256_comp_sub_epi32(imm0, *(__m256i*)_pi32_256_0x7f);
     __m256 e = _mm256_cvtepi32_ps(imm0);
 
     e = _mm256_add_ps(e, one);
@@ -303,8 +316,8 @@ static inline __m256 exp256_ps(__m256 x)
     /* build 2^n */
     imm0 = _mm256_cvttps_epi32(fx);
     // another two AVX2 instructions
-    imm0 = _mm256_add_epi32(imm0, *(__m256i*)_pi32_256_0x7f);
-    imm0 = _mm256_slli_epi32(imm0, 23);
+    imm0 = _mm256_comp_add_epi32(imm0, *(__m256i*)_pi32_256_0x7f);
+    imm0 = _mm256_comp_slli_epi32(imm0, 23);
     __m256 pow2n = _mm256_castsi256_ps(imm0);
     y = _mm256_mul_ps(y, pow2n);
     return y;
@@ -363,13 +376,13 @@ static inline __m256 sin256_ps(__m256 x)
     imm2 = _mm256_cvttps_epi32(y);
     /* j=(j+1) & (~1) (see the cephes sources) */
     // another two AVX2 instruction
-    imm2 = _mm256_add_epi32(imm2, *(__m256i*)_pi32_256_1);
+    imm2 = _mm256_comp_add_epi32(imm2, *(__m256i*)_pi32_256_1);
     imm2 = _mm256_and_si256(imm2, *(__m256i*)_pi32_256_inv1);
     y = _mm256_cvtepi32_ps(imm2);
 
     /* get the swap sign flag */
     imm0 = _mm256_and_si256(imm2, *(__m256i*)_pi32_256_4);
-    imm0 = _mm256_slli_epi32(imm0, 29);
+    imm0 = _mm256_comp_slli_epi32(imm0, 29);
     /* get the polynom selection mask
        there is one polynom for 0 <= x <= Pi/4
        and another one for Pi/4<x<=Pi/2
@@ -481,14 +494,14 @@ static inline __m256 cos256_ps(__m256 x)
     /* store the integer part of y in mm0 */
     imm2 = _mm256_cvttps_epi32(y);
     /* j=(j+1) & (~1) (see the cephes sources) */
-    imm2 = _mm256_add_epi32(imm2, *(__m256i*)_pi32_256_1);
+    imm2 = _mm256_comp_add_epi32(imm2, *(__m256i*)_pi32_256_1);
     imm2 = _mm256_and_si256(imm2, *(__m256i*)_pi32_256_inv1);
     y = _mm256_cvtepi32_ps(imm2);
-    imm2 = _mm256_sub_epi32(imm2, *(__m256i*)_pi32_256_2);
+    imm2 = _mm256_comp_sub_epi32(imm2, *(__m256i*)_pi32_256_2);
 
     /* get the swap sign flag */
     imm0 = _mm256_andnot_si256(imm2, *(__m256i*)_pi32_256_4);
-    imm0 = _mm256_slli_epi32(imm0, 29);
+    imm0 = _mm256_comp_slli_epi32(imm0, 29);
     /* get the polynom selection mask */
     imm2 = _mm256_and_si256(imm2, *(__m256i*)_pi32_256_2);
     imm2 = _mm256_cmpeq_epi32(imm2, *(__m256i*)_pi32_256_0);
@@ -604,7 +617,7 @@ static inline void sincos256_ps(__m256 x, __m256* s, __m256* c)
     imm2 = _mm256_cvttps_epi32(y);
 
     /* j=(j+1) & (~1) (see the cephes sources) */
-    imm2 = _mm256_add_epi32(imm2, *(__m256i*)_pi32_256_1);
+    imm2 = _mm256_comp_add_epi32(imm2, *(__m256i*)_pi32_256_1);
     imm2 = _mm256_and_si256(imm2, *(__m256i*)_pi32_256_inv1);
 
     y = _mm256_cvtepi32_ps(imm2);
@@ -612,7 +625,7 @@ static inline void sincos256_ps(__m256 x, __m256* s, __m256* c)
 
     /* get the swap sign flag for the sine */
     imm0 = _mm256_and_si256(imm2, *(__m256i*)_pi32_256_4);
-    imm0 = _mm256_slli_epi32(imm0, 29);
+    imm0 = _mm256_comp_slli_epi32(imm0, 29);
     //__m256 swap_sign_bit_sin = _mm256_castsi256_ps(imm0);
 
     /* get the polynom selection mask for the sine*/
@@ -667,9 +680,9 @@ static inline void sincos256_ps(__m256 x, __m256* s, __m256* c)
     x = _mm256_add_ps(x, xmm3);
 
 #ifdef __AVX2__
-    imm4 = _mm256_sub_epi32(imm4, *(__m256i*)_pi32_256_2);
+    imm4 = _mm256_comp_sub_epi32(imm4, *(__m256i*)_pi32_256_2);
     imm4 = _mm256_andnot_si256(imm4, *(__m256i*)_pi32_256_4);
-    imm4 = _mm256_slli_epi32(imm4, 29);
+    imm4 = _mm256_comp_slli_epi32(imm4, 29);
 #else
     imm4_1 = _mm_sub_epi32(imm4_1, *(__m128i*)_pi32avx_2);
     imm4_2 = _mm_sub_epi32(imm4_2, *(__m128i*)_pi32avx_2);
diff --git a/src/layer/x86/batchnorm_x86.cpp b/src/layer/x86/batchnorm_x86.cpp
index 441686e5..e02c446b 100644
--- a/src/layer/x86/batchnorm_x86.cpp
+++ b/src/layer/x86/batchnorm_x86.cpp
@@ -20,6 +20,7 @@
 #include <immintrin.h>
 #endif // __AVX__
 #endif // __SSE2__
+#include "x86_usability.h"
 
 namespace ncnn {
 
@@ -44,7 +45,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
         {
             int w = bottom_top_blob.w;
 
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < w; i++)
             {
                 float* ptr = (float*)bottom_top_blob + i * 8;
@@ -53,7 +54,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
                 __m256 _b = _mm256_loadu_ps((const float*)b_data + i * 8);
 
                 __m256 _p = _mm256_loadu_ps(ptr);
-                _p = _mm256_fmadd_ps(_p, _b, _a);
+                _p = _mm256_comp_fmadd_ps(_p, _b, _a);
                 _mm256_storeu_ps(ptr, _p);
             }
         }
@@ -63,7 +64,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
             int w = bottom_top_blob.w;
             int h = bottom_top_blob.h;
 
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < h; i++)
             {
                 __m256 _a = _mm256_loadu_ps((const float*)a_data + i * 8);
@@ -74,7 +75,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
                 for (int j = 0; j < w; j++)
                 {
                     __m256 _p = _mm256_loadu_ps(ptr);
-                    _p = _mm256_fmadd_ps(_p, _b, _a);
+                    _p = _mm256_comp_fmadd_ps(_p, _b, _a);
                     _mm256_storeu_ps(ptr, _p);
 
                     ptr += 8;
@@ -89,7 +90,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
             int c = bottom_top_blob.c;
             int size = w * h;
 
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < c; q++)
             {
                 __m256 _a = _mm256_loadu_ps((const float*)a_data + q * 8);
@@ -100,7 +101,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
                 for (int i = 0; i < size; i++)
                 {
                     __m256 _p = _mm256_loadu_ps(ptr);
-                    _p = _mm256_fmadd_ps(_p, _b, _a);
+                    _p = _mm256_comp_fmadd_ps(_p, _b, _a);
                     _mm256_storeu_ps(ptr, _p);
 
                     ptr += 8;
@@ -118,7 +119,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
         {
             int w = bottom_top_blob.w;
 
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < w; i++)
             {
                 float* ptr = (float*)bottom_top_blob + i * 4;
@@ -138,7 +139,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
             int w = bottom_top_blob.w;
             int h = bottom_top_blob.h;
 
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < h; i++)
             {
                 __m128 _a = _mm_load_ps((const float*)a_data + i * 4);
@@ -165,7 +166,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
             int c = bottom_top_blob.c;
             int size = w * h;
 
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < c; q++)
             {
                 __m128 _a = _mm_load_ps((const float*)a_data + q * 4);
@@ -197,7 +198,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
     // int c = bottom_top_blob.c;
     int size = w * h;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int q = 0; q < channels; q++)
     {
         float* ptr = bottom_top_blob.channel(q);
@@ -213,7 +214,7 @@ int BatchNorm_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
         for (; i + 7 < size; i += 8)
         {
             __m256 _p = _mm256_loadu_ps(ptr);
-            _p = _mm256_fmadd_ps(_p, _b256, _a256);
+            _p = _mm256_comp_fmadd_ps(_p, _b256, _a256);
             _mm256_storeu_ps(ptr, _p);
             ptr += 8;
         }
diff --git a/src/layer/x86/cast_x86.cpp b/src/layer/x86/cast_x86.cpp
index 141b154c..5148206f 100644
--- a/src/layer/x86/cast_x86.cpp
+++ b/src/layer/x86/cast_x86.cpp
@@ -44,6 +44,8 @@ static inline __m256 bfloat2float_avx(__m128i v0)
     ab = _mm256_insertf128_si256(ab, b, 1); // insert in high 128-bit lane
     return _mm256_castsi256_ps(ab);
 }
+#if __AVX2__
+
 static inline __m256i float2bfloat_avx(__m256 v0, __m256 v1)
 {
     __m256i a = _mm256_castps_si256(v0);
@@ -60,6 +62,7 @@ static inline __m128i float2bfloat_avx(__m256 v0)
     __m256i aaaa = _mm256_packus_epi32(a, a);
     return _mm256_castsi256_si128(_mm256_permutevar8x32_epi32(aaaa, _mm256_setr_epi32(0, 1, 4, 5, 2, 3, 6, 7)));
 }
+#endif
 #endif // __AVX__
 
 namespace ncnn {
@@ -71,7 +74,7 @@ Cast_x86::Cast_x86()
 
 int Cast_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& opt) const
 {
-#if __AVX__
+#if __AVX2__
     if (type_from == type_to)
     {
         top_blob = bottom_blob;
@@ -128,7 +131,6 @@ int Cast_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& opt)
         return -100;
 
     int size = w * h * elempack;
-
     if (type_from == 1 && type_to == 2)
     {
         int nn = size >> 3;
diff --git a/src/layer/x86/convolution_1x1_pack4.h b/src/layer/x86/convolution_1x1_pack4.h
index 78f136d4..cc36e70e 100644
--- a/src/layer/x86/convolution_1x1_pack4.h
+++ b/src/layer/x86/convolution_1x1_pack4.h
@@ -83,7 +83,7 @@ static void conv1x1s1_sgemm_pack4_sse(const Mat& bottom_blob, Mat& top_blob, con
         remain_size_start = 0;
         nn_size = (size - remain_size_start) >> 2;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = remain_size_start + ii * 4;
@@ -112,7 +112,7 @@ static void conv1x1s1_sgemm_pack4_sse(const Mat& bottom_blob, Mat& top_blob, con
         remain_size_start += nn_size << 2;
         nn_size = (size - remain_size_start) >> 1;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = remain_size_start + ii * 2;
@@ -136,7 +136,7 @@ static void conv1x1s1_sgemm_pack4_sse(const Mat& bottom_blob, Mat& top_blob, con
 
         remain_size_start += nn_size << 1;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int i = remain_size_start; i < size; i++)
         {
             const float* img0 = bottom_blob.channel(0);
@@ -155,7 +155,7 @@ static void conv1x1s1_sgemm_pack4_sse(const Mat& bottom_blob, Mat& top_blob, con
         }
     }
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < outch; p++)
     {
         float* outptr0 = top_blob.channel(p);
@@ -199,22 +199,22 @@ static void conv1x1s1_sgemm_pack4_sse(const Mat& bottom_blob, Mat& top_blob, con
                 __m128 _w3 = _mm_load_ps(kptr0 + 12);
 
 #if __AVX__
-                _sum0 = _mm_fmadd_ps(_w0, _val00, _sum0);
-                _sum0 = _mm_fmadd_ps(_w1, _val01, _sum0);
-                _sum0 = _mm_fmadd_ps(_w2, _val02, _sum0);
-                _sum0 = _mm_fmadd_ps(_w3, _val03, _sum0);
-                _sum1 = _mm_fmadd_ps(_w0, _val10, _sum1);
-                _sum1 = _mm_fmadd_ps(_w1, _val11, _sum1);
-                _sum1 = _mm_fmadd_ps(_w2, _val12, _sum1);
-                _sum1 = _mm_fmadd_ps(_w3, _val13, _sum1);
-                _sum2 = _mm_fmadd_ps(_w0, _val20, _sum2);
-                _sum2 = _mm_fmadd_ps(_w1, _val21, _sum2);
-                _sum2 = _mm_fmadd_ps(_w2, _val22, _sum2);
-                _sum2 = _mm_fmadd_ps(_w3, _val23, _sum2);
-                _sum3 = _mm_fmadd_ps(_w0, _val30, _sum3);
-                _sum3 = _mm_fmadd_ps(_w1, _val31, _sum3);
-                _sum3 = _mm_fmadd_ps(_w2, _val32, _sum3);
-                _sum3 = _mm_fmadd_ps(_w3, _val33, _sum3);
+                _sum0 = _mm_comp_fmadd_ps(_w0, _val00, _sum0);
+                _sum0 = _mm_comp_fmadd_ps(_w1, _val01, _sum0);
+                _sum0 = _mm_comp_fmadd_ps(_w2, _val02, _sum0);
+                _sum0 = _mm_comp_fmadd_ps(_w3, _val03, _sum0);
+                _sum1 = _mm_comp_fmadd_ps(_w0, _val10, _sum1);
+                _sum1 = _mm_comp_fmadd_ps(_w1, _val11, _sum1);
+                _sum1 = _mm_comp_fmadd_ps(_w2, _val12, _sum1);
+                _sum1 = _mm_comp_fmadd_ps(_w3, _val13, _sum1);
+                _sum2 = _mm_comp_fmadd_ps(_w0, _val20, _sum2);
+                _sum2 = _mm_comp_fmadd_ps(_w1, _val21, _sum2);
+                _sum2 = _mm_comp_fmadd_ps(_w2, _val22, _sum2);
+                _sum2 = _mm_comp_fmadd_ps(_w3, _val23, _sum2);
+                _sum3 = _mm_comp_fmadd_ps(_w0, _val30, _sum3);
+                _sum3 = _mm_comp_fmadd_ps(_w1, _val31, _sum3);
+                _sum3 = _mm_comp_fmadd_ps(_w2, _val32, _sum3);
+                _sum3 = _mm_comp_fmadd_ps(_w3, _val33, _sum3);
 #else
                 _sum0 = _mm_add_ps(_mm_mul_ps(_w0, _val00), _sum0);
                 _sum0 = _mm_add_ps(_mm_mul_ps(_w1, _val01), _sum0);
@@ -269,14 +269,14 @@ static void conv1x1s1_sgemm_pack4_sse(const Mat& bottom_blob, Mat& top_blob, con
                 __m128 _w3 = _mm_load_ps(kptr0 + 12);
 
 #if __AVX__
-                _sum0 = _mm_fmadd_ps(_w0, _val00, _sum0);
-                _sum0 = _mm_fmadd_ps(_w1, _val01, _sum0);
-                _sum0 = _mm_fmadd_ps(_w2, _val02, _sum0);
-                _sum0 = _mm_fmadd_ps(_w3, _val03, _sum0);
-                _sum1 = _mm_fmadd_ps(_w0, _val10, _sum1);
-                _sum1 = _mm_fmadd_ps(_w1, _val11, _sum1);
-                _sum1 = _mm_fmadd_ps(_w2, _val12, _sum1);
-                _sum1 = _mm_fmadd_ps(_w3, _val13, _sum1);
+                _sum0 = _mm_comp_fmadd_ps(_w0, _val00, _sum0);
+                _sum0 = _mm_comp_fmadd_ps(_w1, _val01, _sum0);
+                _sum0 = _mm_comp_fmadd_ps(_w2, _val02, _sum0);
+                _sum0 = _mm_comp_fmadd_ps(_w3, _val03, _sum0);
+                _sum1 = _mm_comp_fmadd_ps(_w0, _val10, _sum1);
+                _sum1 = _mm_comp_fmadd_ps(_w1, _val11, _sum1);
+                _sum1 = _mm_comp_fmadd_ps(_w2, _val12, _sum1);
+                _sum1 = _mm_comp_fmadd_ps(_w3, _val13, _sum1);
 #else
                 _sum0 = _mm_add_ps(_mm_mul_ps(_w0, _val00), _sum0);
                 _sum0 = _mm_add_ps(_mm_mul_ps(_w1, _val01), _sum0);
@@ -316,10 +316,10 @@ static void conv1x1s1_sgemm_pack4_sse(const Mat& bottom_blob, Mat& top_blob, con
                 __m128 _w3 = _mm_load_ps(kptr0 + 12);
 
 #if __AVX__
-                _sum = _mm_fmadd_ps(_w0, _val0, _sum);
-                _sum = _mm_fmadd_ps(_w1, _val1, _sum);
-                _sum = _mm_fmadd_ps(_w2, _val2, _sum);
-                _sum = _mm_fmadd_ps(_w3, _val3, _sum);
+                _sum = _mm_comp_fmadd_ps(_w0, _val0, _sum);
+                _sum = _mm_comp_fmadd_ps(_w1, _val1, _sum);
+                _sum = _mm_comp_fmadd_ps(_w2, _val2, _sum);
+                _sum = _mm_comp_fmadd_ps(_w3, _val3, _sum);
 #else
                 _sum = _mm_add_ps(_mm_mul_ps(_w0, _val0), _sum);
                 _sum = _mm_add_ps(_mm_mul_ps(_w1, _val1), _sum);
@@ -379,7 +379,7 @@ static void conv1x1s2_pack4_sse(const Mat& bottom_blob, Mat& top_blob, const Mat
     Mat bottom_blob_shrinked;
     bottom_blob_shrinked.create(outw, outh, channels, elemsize, elempack, opt.workspace_allocator);
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < channels; p++)
     {
         const float* r0 = bottom_blob.channel(p);
diff --git a/src/layer/x86/convolution_1x1_pack8.h b/src/layer/x86/convolution_1x1_pack8.h
index da5b0970..e4d8f162 100644
--- a/src/layer/x86/convolution_1x1_pack8.h
+++ b/src/layer/x86/convolution_1x1_pack8.h
@@ -208,7 +208,7 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
     {
         int nn_size = size / 12;
         int remain_size_start = nn_size * 12;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = ii * 12;
@@ -249,7 +249,7 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
             }
         }
         nn_size = (size - remain_size_start) >> 3;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = remain_size_start + ii * 8;
@@ -286,7 +286,7 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
         remain_size_start += nn_size << 3;
         nn_size = (size - remain_size_start) >> 2;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = remain_size_start + ii * 4;
@@ -313,7 +313,7 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
 
         remain_size_start += nn_size << 2;
         nn_size = (size - remain_size_start) >> 1;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = remain_size_start + ii * 2;
@@ -335,7 +335,7 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
         }
 
         remain_size_start += nn_size << 1;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int i = remain_size_start; i < size; i++)
         {
             const float* img0 = bottom_blob.channel(0);
@@ -351,7 +351,7 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
             }
         }
     }
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < outch; p++)
     {
         Mat out = top_blob.channel(p);
@@ -390,7 +390,7 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _val02 = _mm256_broadcast_ss(tmpptr + 2);
                 __m256 _val03 = _mm256_broadcast_ss(tmpptr + 3);
 
-                _mm256_fmadd_ps4(_sum0, _w0, _w1, _w2, _w3, _val00, _val01, _val02, _val03);
+                _mm256_comp_fmadd_ps4(_sum0, _w0, _w1, _w2, _w3, _val00, _val01, _val02, _val03);
 
                 __m256 _w4 = _mm256_loadu_ps(kptr + 32);
                 __m256 _w5 = _mm256_loadu_ps(kptr + 40);
@@ -402,161 +402,161 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _val06 = _mm256_broadcast_ss(tmpptr + 6);
                 __m256 _val07 = _mm256_broadcast_ss(tmpptr + 7);
 
-                _mm256_fmadd_ps4(_sum0, _w4, _w5, _w6, _w7, _val04, _val05, _val06, _val07);
+                _mm256_comp_fmadd_ps4(_sum0, _w4, _w5, _w6, _w7, _val04, _val05, _val06, _val07);
 
                 __m256 _val10 = _mm256_broadcast_ss(tmpptr + 8);
                 __m256 _val11 = _mm256_broadcast_ss(tmpptr + 9);
                 __m256 _val12 = _mm256_broadcast_ss(tmpptr + 10);
                 __m256 _val13 = _mm256_broadcast_ss(tmpptr + 11);
 
-                _mm256_fmadd_ps4(_sum1, _w0, _w1, _w2, _w3, _val10, _val11, _val12, _val13);
+                _mm256_comp_fmadd_ps4(_sum1, _w0, _w1, _w2, _w3, _val10, _val11, _val12, _val13);
 
                 __m256 _val14 = _mm256_broadcast_ss(tmpptr + 12);
                 __m256 _val15 = _mm256_broadcast_ss(tmpptr + 13);
                 __m256 _val16 = _mm256_broadcast_ss(tmpptr + 14);
                 __m256 _val17 = _mm256_broadcast_ss(tmpptr + 15);
 
-                _mm256_fmadd_ps4(_sum1, _w4, _w5, _w6, _w7, _val14, _val15, _val16, _val17);
+                _mm256_comp_fmadd_ps4(_sum1, _w4, _w5, _w6, _w7, _val14, _val15, _val16, _val17);
 
                 __m256 _val20 = _mm256_broadcast_ss(tmpptr + 16);
                 __m256 _val21 = _mm256_broadcast_ss(tmpptr + 17);
                 __m256 _val22 = _mm256_broadcast_ss(tmpptr + 18);
                 __m256 _val23 = _mm256_broadcast_ss(tmpptr + 19);
 
-                _mm256_fmadd_ps4(_sum2, _w0, _w1, _w2, _w3, _val20, _val21, _val22, _val23);
+                _mm256_comp_fmadd_ps4(_sum2, _w0, _w1, _w2, _w3, _val20, _val21, _val22, _val23);
 
                 __m256 _val24 = _mm256_broadcast_ss(tmpptr + 20);
                 __m256 _val25 = _mm256_broadcast_ss(tmpptr + 21);
                 __m256 _val26 = _mm256_broadcast_ss(tmpptr + 22);
                 __m256 _val27 = _mm256_broadcast_ss(tmpptr + 23);
 
-                _mm256_fmadd_ps4(_sum2, _w4, _w5, _w6, _w7, _val24, _val25, _val26, _val27);
+                _mm256_comp_fmadd_ps4(_sum2, _w4, _w5, _w6, _w7, _val24, _val25, _val26, _val27);
 
                 __m256 _val30 = _mm256_broadcast_ss(tmpptr + 24);
                 __m256 _val31 = _mm256_broadcast_ss(tmpptr + 25);
                 __m256 _val32 = _mm256_broadcast_ss(tmpptr + 26);
                 __m256 _val33 = _mm256_broadcast_ss(tmpptr + 27);
 
-                _mm256_fmadd_ps4(_sum3, _w0, _w1, _w2, _w3, _val30, _val31, _val32, _val33);
+                _mm256_comp_fmadd_ps4(_sum3, _w0, _w1, _w2, _w3, _val30, _val31, _val32, _val33);
 
                 __m256 _val34 = _mm256_broadcast_ss(tmpptr + 28);
                 __m256 _val35 = _mm256_broadcast_ss(tmpptr + 29);
                 __m256 _val36 = _mm256_broadcast_ss(tmpptr + 30);
                 __m256 _val37 = _mm256_broadcast_ss(tmpptr + 31);
 
-                _mm256_fmadd_ps4(_sum3, _w4, _w5, _w6, _w7, _val34, _val35, _val36, _val37);
+                _mm256_comp_fmadd_ps4(_sum3, _w4, _w5, _w6, _w7, _val34, _val35, _val36, _val37);
 
                 __m256 _val40 = _mm256_broadcast_ss(tmpptr + 32);
                 __m256 _val41 = _mm256_broadcast_ss(tmpptr + 33);
                 __m256 _val42 = _mm256_broadcast_ss(tmpptr + 34);
                 __m256 _val43 = _mm256_broadcast_ss(tmpptr + 35);
 
-                _mm256_fmadd_ps4(_sum4, _w0, _w1, _w2, _w3, _val40, _val41, _val42, _val43);
+                _mm256_comp_fmadd_ps4(_sum4, _w0, _w1, _w2, _w3, _val40, _val41, _val42, _val43);
 
                 __m256 _val44 = _mm256_broadcast_ss(tmpptr + 36);
                 __m256 _val45 = _mm256_broadcast_ss(tmpptr + 37);
                 __m256 _val46 = _mm256_broadcast_ss(tmpptr + 38);
                 __m256 _val47 = _mm256_broadcast_ss(tmpptr + 39);
 
-                _mm256_fmadd_ps4(_sum4, _w4, _w5, _w6, _w7, _val44, _val45, _val46, _val47);
+                _mm256_comp_fmadd_ps4(_sum4, _w4, _w5, _w6, _w7, _val44, _val45, _val46, _val47);
 
                 __m256 _val50 = _mm256_broadcast_ss(tmpptr + 40);
                 __m256 _val51 = _mm256_broadcast_ss(tmpptr + 41);
                 __m256 _val52 = _mm256_broadcast_ss(tmpptr + 42);
                 __m256 _val53 = _mm256_broadcast_ss(tmpptr + 43);
 
-                _mm256_fmadd_ps4(_sum5, _w0, _w1, _w2, _w3, _val50, _val51, _val52, _val53);
+                _mm256_comp_fmadd_ps4(_sum5, _w0, _w1, _w2, _w3, _val50, _val51, _val52, _val53);
 
                 __m256 _val54 = _mm256_broadcast_ss(tmpptr + 44);
                 __m256 _val55 = _mm256_broadcast_ss(tmpptr + 45);
                 __m256 _val56 = _mm256_broadcast_ss(tmpptr + 46);
                 __m256 _val57 = _mm256_broadcast_ss(tmpptr + 47);
 
-                _mm256_fmadd_ps4(_sum5, _w4, _w5, _w6, _w7, _val54, _val55, _val56, _val57);
+                _mm256_comp_fmadd_ps4(_sum5, _w4, _w5, _w6, _w7, _val54, _val55, _val56, _val57);
 
                 __m256 _val60 = _mm256_broadcast_ss(tmpptr + 48);
                 __m256 _val61 = _mm256_broadcast_ss(tmpptr + 49);
                 __m256 _val62 = _mm256_broadcast_ss(tmpptr + 50);
                 __m256 _val63 = _mm256_broadcast_ss(tmpptr + 51);
 
-                _mm256_fmadd_ps4(_sum6, _w0, _w1, _w2, _w3, _val60, _val61, _val62, _val63);
+                _mm256_comp_fmadd_ps4(_sum6, _w0, _w1, _w2, _w3, _val60, _val61, _val62, _val63);
 
                 __m256 _val64 = _mm256_broadcast_ss(tmpptr + 52);
                 __m256 _val65 = _mm256_broadcast_ss(tmpptr + 53);
                 __m256 _val66 = _mm256_broadcast_ss(tmpptr + 54);
                 __m256 _val67 = _mm256_broadcast_ss(tmpptr + 55);
 
-                _mm256_fmadd_ps4(_sum6, _w4, _w5, _w6, _w7, _val64, _val65, _val66, _val67);
+                _mm256_comp_fmadd_ps4(_sum6, _w4, _w5, _w6, _w7, _val64, _val65, _val66, _val67);
 
                 __m256 _val70 = _mm256_broadcast_ss(tmpptr + 56);
                 __m256 _val71 = _mm256_broadcast_ss(tmpptr + 57);
                 __m256 _val72 = _mm256_broadcast_ss(tmpptr + 58);
                 __m256 _val73 = _mm256_broadcast_ss(tmpptr + 59);
 
-                _mm256_fmadd_ps4(_sum7, _w0, _w1, _w2, _w3, _val70, _val71, _val72, _val73);
+                _mm256_comp_fmadd_ps4(_sum7, _w0, _w1, _w2, _w3, _val70, _val71, _val72, _val73);
 
                 __m256 _val74 = _mm256_broadcast_ss(tmpptr + 60);
                 __m256 _val75 = _mm256_broadcast_ss(tmpptr + 61);
                 __m256 _val76 = _mm256_broadcast_ss(tmpptr + 62);
                 __m256 _val77 = _mm256_broadcast_ss(tmpptr + 63);
 
-                _mm256_fmadd_ps4(_sum7, _w4, _w5, _w6, _w7, _val74, _val75, _val76, _val77);
+                _mm256_comp_fmadd_ps4(_sum7, _w4, _w5, _w6, _w7, _val74, _val75, _val76, _val77);
 
                 __m256 _val80 = _mm256_broadcast_ss(tmpptr + 64);
                 __m256 _val81 = _mm256_broadcast_ss(tmpptr + 65);
                 __m256 _val82 = _mm256_broadcast_ss(tmpptr + 66);
                 __m256 _val83 = _mm256_broadcast_ss(tmpptr + 67);
 
-                _mm256_fmadd_ps4(_sum8, _w0, _w1, _w2, _w3, _val80, _val81, _val82, _val83);
+                _mm256_comp_fmadd_ps4(_sum8, _w0, _w1, _w2, _w3, _val80, _val81, _val82, _val83);
 
                 __m256 _val84 = _mm256_broadcast_ss(tmpptr + 68);
                 __m256 _val85 = _mm256_broadcast_ss(tmpptr + 69);
                 __m256 _val86 = _mm256_broadcast_ss(tmpptr + 70);
                 __m256 _val87 = _mm256_broadcast_ss(tmpptr + 71);
 
-                _mm256_fmadd_ps4(_sum8, _w4, _w5, _w6, _w7, _val84, _val85, _val86, _val87);
+                _mm256_comp_fmadd_ps4(_sum8, _w4, _w5, _w6, _w7, _val84, _val85, _val86, _val87);
 
                 __m256 _val90 = _mm256_broadcast_ss(tmpptr + 72);
                 __m256 _val91 = _mm256_broadcast_ss(tmpptr + 73);
                 __m256 _val92 = _mm256_broadcast_ss(tmpptr + 74);
                 __m256 _val93 = _mm256_broadcast_ss(tmpptr + 75);
 
-                _mm256_fmadd_ps4(_sum9, _w0, _w1, _w2, _w3, _val90, _val91, _val92, _val93);
+                _mm256_comp_fmadd_ps4(_sum9, _w0, _w1, _w2, _w3, _val90, _val91, _val92, _val93);
 
                 __m256 _val94 = _mm256_broadcast_ss(tmpptr + 76);
                 __m256 _val95 = _mm256_broadcast_ss(tmpptr + 77);
                 __m256 _val96 = _mm256_broadcast_ss(tmpptr + 78);
                 __m256 _val97 = _mm256_broadcast_ss(tmpptr + 79);
 
-                _mm256_fmadd_ps4(_sum9, _w4, _w5, _w6, _w7, _val94, _val95, _val96, _val97);
+                _mm256_comp_fmadd_ps4(_sum9, _w4, _w5, _w6, _w7, _val94, _val95, _val96, _val97);
 
                 __m256 _val100 = _mm256_broadcast_ss(tmpptr + 80);
                 __m256 _val101 = _mm256_broadcast_ss(tmpptr + 81);
                 __m256 _val102 = _mm256_broadcast_ss(tmpptr + 82);
                 __m256 _val103 = _mm256_broadcast_ss(tmpptr + 83);
 
-                _mm256_fmadd_ps4(_sum10, _w0, _w1, _w2, _w3, _val100, _val101, _val102, _val103);
+                _mm256_comp_fmadd_ps4(_sum10, _w0, _w1, _w2, _w3, _val100, _val101, _val102, _val103);
 
                 __m256 _val104 = _mm256_broadcast_ss(tmpptr + 84);
                 __m256 _val105 = _mm256_broadcast_ss(tmpptr + 85);
                 __m256 _val106 = _mm256_broadcast_ss(tmpptr + 86);
                 __m256 _val107 = _mm256_broadcast_ss(tmpptr + 87);
 
-                _mm256_fmadd_ps4(_sum10, _w4, _w5, _w6, _w7, _val104, _val105, _val106, _val107);
+                _mm256_comp_fmadd_ps4(_sum10, _w4, _w5, _w6, _w7, _val104, _val105, _val106, _val107);
 
                 __m256 _val110 = _mm256_broadcast_ss(tmpptr + 88);
                 __m256 _val111 = _mm256_broadcast_ss(tmpptr + 89);
                 __m256 _val112 = _mm256_broadcast_ss(tmpptr + 90);
                 __m256 _val113 = _mm256_broadcast_ss(tmpptr + 91);
 
-                _mm256_fmadd_ps4(_sum11, _w0, _w1, _w2, _w3, _val110, _val111, _val112, _val113);
+                _mm256_comp_fmadd_ps4(_sum11, _w0, _w1, _w2, _w3, _val110, _val111, _val112, _val113);
 
                 __m256 _val114 = _mm256_broadcast_ss(tmpptr + 92);
                 __m256 _val115 = _mm256_broadcast_ss(tmpptr + 93);
                 __m256 _val116 = _mm256_broadcast_ss(tmpptr + 94);
                 __m256 _val117 = _mm256_broadcast_ss(tmpptr + 95);
 
-                _mm256_fmadd_ps4(_sum11, _w4, _w5, _w6, _w7, _val114, _val115, _val116, _val117);
+                _mm256_comp_fmadd_ps4(_sum11, _w4, _w5, _w6, _w7, _val114, _val115, _val116, _val117);
 
                 tmpptr += 96;
 
@@ -619,22 +619,22 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _val16 = _mm256_broadcast_ss(tmpptr + 14);
                 __m256 _val17 = _mm256_broadcast_ss(tmpptr + 15);
 
-                _sum0 = _mm256_fmadd_ps(_w0, _val00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w1, _val01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w2, _val02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w3, _val03, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w4, _val04, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w5, _val05, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w6, _val06, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w7, _val07, _sum0);
-                _sum1 = _mm256_fmadd_ps(_w0, _val10, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w1, _val11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w2, _val12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w3, _val13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w4, _val14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w5, _val15, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w6, _val16, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w7, _val17, _sum1);
+                _sum0 = _mm256_comp_fmadd_ps(_w0, _val00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w1, _val01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w2, _val02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w3, _val03, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w4, _val04, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w5, _val05, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w6, _val06, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w7, _val07, _sum0);
+                _sum1 = _mm256_comp_fmadd_ps(_w0, _val10, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w1, _val11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w2, _val12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w3, _val13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w4, _val14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w5, _val15, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w6, _val16, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w7, _val17, _sum1);
 
                 __m256 _val20 = _mm256_broadcast_ss(tmpptr + 16);
                 __m256 _val21 = _mm256_broadcast_ss(tmpptr + 17);
@@ -653,22 +653,22 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _val36 = _mm256_broadcast_ss(tmpptr + 30);
                 __m256 _val37 = _mm256_broadcast_ss(tmpptr + 31);
 
-                _sum2 = _mm256_fmadd_ps(_w0, _val20, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w1, _val21, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w2, _val22, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w3, _val23, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w4, _val24, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w5, _val25, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w6, _val26, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w7, _val27, _sum2);
-                _sum3 = _mm256_fmadd_ps(_w0, _val30, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w1, _val31, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w2, _val32, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w3, _val33, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w4, _val34, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w5, _val35, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w6, _val36, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w7, _val37, _sum3);
+                _sum2 = _mm256_comp_fmadd_ps(_w0, _val20, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w1, _val21, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w2, _val22, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w3, _val23, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w4, _val24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w5, _val25, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w6, _val26, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w7, _val27, _sum2);
+                _sum3 = _mm256_comp_fmadd_ps(_w0, _val30, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w1, _val31, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w2, _val32, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w3, _val33, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w4, _val34, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w5, _val35, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w6, _val36, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w7, _val37, _sum3);
 
                 __m256 _val40 = _mm256_broadcast_ss(tmpptr + 32);
                 __m256 _val41 = _mm256_broadcast_ss(tmpptr + 33);
@@ -687,22 +687,22 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _val56 = _mm256_broadcast_ss(tmpptr + 46);
                 __m256 _val57 = _mm256_broadcast_ss(tmpptr + 47);
 
-                _sum4 = _mm256_fmadd_ps(_w0, _val40, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w1, _val41, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w2, _val42, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w3, _val43, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w4, _val44, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w5, _val45, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w6, _val46, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w7, _val47, _sum4);
-                _sum5 = _mm256_fmadd_ps(_w0, _val50, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w1, _val51, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w2, _val52, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w3, _val53, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w4, _val54, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w5, _val55, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w6, _val56, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w7, _val57, _sum5);
+                _sum4 = _mm256_comp_fmadd_ps(_w0, _val40, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w1, _val41, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w2, _val42, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w3, _val43, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w4, _val44, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w5, _val45, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w6, _val46, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w7, _val47, _sum4);
+                _sum5 = _mm256_comp_fmadd_ps(_w0, _val50, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w1, _val51, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w2, _val52, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w3, _val53, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w4, _val54, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w5, _val55, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w6, _val56, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w7, _val57, _sum5);
 
                 __m256 _val60 = _mm256_broadcast_ss(tmpptr + 48);
                 __m256 _val61 = _mm256_broadcast_ss(tmpptr + 49);
@@ -721,22 +721,22 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _val76 = _mm256_broadcast_ss(tmpptr + 62);
                 __m256 _val77 = _mm256_broadcast_ss(tmpptr + 63);
 
-                _sum6 = _mm256_fmadd_ps(_w0, _val60, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w1, _val61, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w2, _val62, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w3, _val63, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w4, _val64, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w5, _val65, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w6, _val66, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w7, _val67, _sum6);
-                _sum7 = _mm256_fmadd_ps(_w0, _val70, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w1, _val71, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w2, _val72, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w3, _val73, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w4, _val74, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w5, _val75, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w6, _val76, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w7, _val77, _sum7);
+                _sum6 = _mm256_comp_fmadd_ps(_w0, _val60, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w1, _val61, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w2, _val62, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w3, _val63, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w4, _val64, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w5, _val65, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w6, _val66, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w7, _val67, _sum6);
+                _sum7 = _mm256_comp_fmadd_ps(_w0, _val70, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w1, _val71, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w2, _val72, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w3, _val73, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w4, _val74, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w5, _val75, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w6, _val76, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w7, _val77, _sum7);
 
                 tmpptr += 64;
 
@@ -791,22 +791,22 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _val16 = _mm256_broadcast_ss(tmpptr + 14);
                 __m256 _val17 = _mm256_broadcast_ss(tmpptr + 15);
 
-                _sum0 = _mm256_fmadd_ps(_w0, _val00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w1, _val01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w2, _val02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w3, _val03, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w4, _val04, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w5, _val05, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w6, _val06, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w7, _val07, _sum0);
-                _sum1 = _mm256_fmadd_ps(_w0, _val10, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w1, _val11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w2, _val12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w3, _val13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w4, _val14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w5, _val15, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w6, _val16, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w7, _val17, _sum1);
+                _sum0 = _mm256_comp_fmadd_ps(_w0, _val00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w1, _val01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w2, _val02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w3, _val03, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w4, _val04, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w5, _val05, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w6, _val06, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w7, _val07, _sum0);
+                _sum1 = _mm256_comp_fmadd_ps(_w0, _val10, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w1, _val11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w2, _val12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w3, _val13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w4, _val14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w5, _val15, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w6, _val16, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w7, _val17, _sum1);
 
                 __m256 _val20 = _mm256_broadcast_ss(tmpptr + 16);
                 __m256 _val21 = _mm256_broadcast_ss(tmpptr + 17);
@@ -825,22 +825,22 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _val36 = _mm256_broadcast_ss(tmpptr + 30);
                 __m256 _val37 = _mm256_broadcast_ss(tmpptr + 31);
 
-                _sum2 = _mm256_fmadd_ps(_w0, _val20, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w1, _val21, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w2, _val22, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w3, _val23, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w4, _val24, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w5, _val25, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w6, _val26, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w7, _val27, _sum2);
-                _sum3 = _mm256_fmadd_ps(_w0, _val30, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w1, _val31, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w2, _val32, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w3, _val33, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w4, _val34, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w5, _val35, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w6, _val36, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w7, _val37, _sum3);
+                _sum2 = _mm256_comp_fmadd_ps(_w0, _val20, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w1, _val21, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w2, _val22, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w3, _val23, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w4, _val24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w5, _val25, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w6, _val26, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w7, _val27, _sum2);
+                _sum3 = _mm256_comp_fmadd_ps(_w0, _val30, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w1, _val31, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w2, _val32, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w3, _val33, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w4, _val34, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w5, _val35, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w6, _val36, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w7, _val37, _sum3);
 
                 tmpptr += 32;
 
@@ -889,22 +889,22 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _w6 = _mm256_loadu_ps(kptr + 48);
                 __m256 _w7 = _mm256_loadu_ps(kptr + 56);
 
-                _sum0 = _mm256_fmadd_ps(_w0, _val00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w1, _val01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w2, _val02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w3, _val03, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w4, _val04, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w5, _val05, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w6, _val06, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w7, _val07, _sum0);
-                _sum1 = _mm256_fmadd_ps(_w0, _val10, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w1, _val11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w2, _val12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w3, _val13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w4, _val14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w5, _val15, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w6, _val16, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w7, _val17, _sum1);
+                _sum0 = _mm256_comp_fmadd_ps(_w0, _val00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w1, _val01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w2, _val02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w3, _val03, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w4, _val04, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w5, _val05, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w6, _val06, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w7, _val07, _sum0);
+                _sum1 = _mm256_comp_fmadd_ps(_w0, _val10, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w1, _val11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w2, _val12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w3, _val13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w4, _val14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w5, _val15, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w6, _val16, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w7, _val17, _sum1);
 
                 tmpptr += 16;
 
@@ -942,14 +942,14 @@ static void conv1x1s1_sgemm_pack8_avx(const Mat& bottom_blob, Mat& top_blob, con
                 __m256 _w6 = _mm256_loadu_ps(kptr + 48);
                 __m256 _w7 = _mm256_loadu_ps(kptr + 56);
 
-                _sum = _mm256_fmadd_ps(_w0, _val0, _sum);
-                _sum = _mm256_fmadd_ps(_w1, _val1, _sum);
-                _sum = _mm256_fmadd_ps(_w2, _val2, _sum);
-                _sum = _mm256_fmadd_ps(_w3, _val3, _sum);
-                _sum = _mm256_fmadd_ps(_w4, _val4, _sum);
-                _sum = _mm256_fmadd_ps(_w5, _val5, _sum);
-                _sum = _mm256_fmadd_ps(_w6, _val6, _sum);
-                _sum = _mm256_fmadd_ps(_w7, _val7, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w0, _val0, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w1, _val1, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w2, _val2, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w3, _val3, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w4, _val4, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w5, _val5, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w6, _val6, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w7, _val7, _sum);
 
                 tmpptr += 8;
 
@@ -977,7 +977,7 @@ static void conv1x1s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
     Mat bottom_blob_shrinked;
     bottom_blob_shrinked.create(outw, outh, channels, elemsize, elempack, opt.workspace_allocator);
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < channels; p++)
     {
         const float* r0 = bottom_blob.channel(p);
diff --git a/src/layer/x86/convolution_1x1_pack8_fp16.h b/src/layer/x86/convolution_1x1_pack8_fp16.h
index 8202d868..e28d22f8 100644
--- a/src/layer/x86/convolution_1x1_pack8_fp16.h
+++ b/src/layer/x86/convolution_1x1_pack8_fp16.h
@@ -208,7 +208,7 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
     {
         int nn_size = size / 12;
         int remain_size_start = nn_size * 12;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = ii * 12;
@@ -249,7 +249,7 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
             }
         }
         nn_size = (size - remain_size_start) >> 3;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = remain_size_start + ii * 8;
@@ -286,7 +286,7 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
         remain_size_start += nn_size << 3;
         nn_size = (size - remain_size_start) >> 2;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = remain_size_start + ii * 4;
@@ -313,7 +313,7 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
 
         remain_size_start += nn_size << 2;
         nn_size = (size - remain_size_start) >> 1;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = remain_size_start + ii * 2;
@@ -335,7 +335,7 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
         }
 
         remain_size_start += nn_size << 1;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int i = remain_size_start; i < size; i++)
         {
             const float* img0 = bottom_blob.channel(0);
@@ -351,7 +351,7 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
             }
         }
     }
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < outch; p++)
     {
         Mat out = top_blob.channel(p);
@@ -406,22 +406,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val16 = _mm256_broadcast_ss(tmpptr + 14);
                 __m256 _val17 = _mm256_broadcast_ss(tmpptr + 15);
 
-                _sum0 = _mm256_fmadd_ps(_w0, _val00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w1, _val01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w2, _val02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w3, _val03, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w4, _val04, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w5, _val05, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w6, _val06, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w7, _val07, _sum0);
-                _sum1 = _mm256_fmadd_ps(_w0, _val10, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w1, _val11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w2, _val12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w3, _val13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w4, _val14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w5, _val15, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w6, _val16, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w7, _val17, _sum1);
+                _sum0 = _mm256_comp_fmadd_ps(_w0, _val00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w1, _val01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w2, _val02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w3, _val03, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w4, _val04, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w5, _val05, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w6, _val06, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w7, _val07, _sum0);
+                _sum1 = _mm256_comp_fmadd_ps(_w0, _val10, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w1, _val11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w2, _val12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w3, _val13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w4, _val14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w5, _val15, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w6, _val16, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w7, _val17, _sum1);
 
                 __m256 _val20 = _mm256_broadcast_ss(tmpptr + 16);
                 __m256 _val21 = _mm256_broadcast_ss(tmpptr + 17);
@@ -440,22 +440,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val36 = _mm256_broadcast_ss(tmpptr + 30);
                 __m256 _val37 = _mm256_broadcast_ss(tmpptr + 31);
 
-                _sum2 = _mm256_fmadd_ps(_w0, _val20, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w1, _val21, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w2, _val22, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w3, _val23, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w4, _val24, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w5, _val25, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w6, _val26, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w7, _val27, _sum2);
-                _sum3 = _mm256_fmadd_ps(_w0, _val30, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w1, _val31, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w2, _val32, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w3, _val33, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w4, _val34, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w5, _val35, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w6, _val36, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w7, _val37, _sum3);
+                _sum2 = _mm256_comp_fmadd_ps(_w0, _val20, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w1, _val21, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w2, _val22, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w3, _val23, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w4, _val24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w5, _val25, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w6, _val26, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w7, _val27, _sum2);
+                _sum3 = _mm256_comp_fmadd_ps(_w0, _val30, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w1, _val31, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w2, _val32, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w3, _val33, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w4, _val34, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w5, _val35, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w6, _val36, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w7, _val37, _sum3);
 
                 __m256 _val40 = _mm256_broadcast_ss(tmpptr + 32);
                 __m256 _val41 = _mm256_broadcast_ss(tmpptr + 33);
@@ -474,22 +474,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val56 = _mm256_broadcast_ss(tmpptr + 46);
                 __m256 _val57 = _mm256_broadcast_ss(tmpptr + 47);
 
-                _sum4 = _mm256_fmadd_ps(_w0, _val40, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w1, _val41, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w2, _val42, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w3, _val43, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w4, _val44, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w5, _val45, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w6, _val46, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w7, _val47, _sum4);
-                _sum5 = _mm256_fmadd_ps(_w0, _val50, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w1, _val51, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w2, _val52, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w3, _val53, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w4, _val54, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w5, _val55, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w6, _val56, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w7, _val57, _sum5);
+                _sum4 = _mm256_comp_fmadd_ps(_w0, _val40, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w1, _val41, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w2, _val42, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w3, _val43, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w4, _val44, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w5, _val45, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w6, _val46, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w7, _val47, _sum4);
+                _sum5 = _mm256_comp_fmadd_ps(_w0, _val50, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w1, _val51, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w2, _val52, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w3, _val53, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w4, _val54, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w5, _val55, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w6, _val56, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w7, _val57, _sum5);
 
                 __m256 _val60 = _mm256_broadcast_ss(tmpptr + 48);
                 __m256 _val61 = _mm256_broadcast_ss(tmpptr + 49);
@@ -508,22 +508,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val76 = _mm256_broadcast_ss(tmpptr + 62);
                 __m256 _val77 = _mm256_broadcast_ss(tmpptr + 63);
 
-                _sum6 = _mm256_fmadd_ps(_w0, _val60, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w1, _val61, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w2, _val62, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w3, _val63, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w4, _val64, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w5, _val65, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w6, _val66, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w7, _val67, _sum6);
-                _sum7 = _mm256_fmadd_ps(_w0, _val70, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w1, _val71, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w2, _val72, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w3, _val73, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w4, _val74, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w5, _val75, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w6, _val76, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w7, _val77, _sum7);
+                _sum6 = _mm256_comp_fmadd_ps(_w0, _val60, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w1, _val61, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w2, _val62, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w3, _val63, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w4, _val64, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w5, _val65, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w6, _val66, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w7, _val67, _sum6);
+                _sum7 = _mm256_comp_fmadd_ps(_w0, _val70, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w1, _val71, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w2, _val72, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w3, _val73, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w4, _val74, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w5, _val75, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w6, _val76, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w7, _val77, _sum7);
 
                 __m256 _val80 = _mm256_broadcast_ss(tmpptr + 64);
                 __m256 _val81 = _mm256_broadcast_ss(tmpptr + 65);
@@ -542,22 +542,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val96 = _mm256_broadcast_ss(tmpptr + 78);
                 __m256 _val97 = _mm256_broadcast_ss(tmpptr + 79);
 
-                _sum8 = _mm256_fmadd_ps(_w0, _val80, _sum8);
-                _sum8 = _mm256_fmadd_ps(_w1, _val81, _sum8);
-                _sum8 = _mm256_fmadd_ps(_w2, _val82, _sum8);
-                _sum8 = _mm256_fmadd_ps(_w3, _val83, _sum8);
-                _sum8 = _mm256_fmadd_ps(_w4, _val84, _sum8);
-                _sum8 = _mm256_fmadd_ps(_w5, _val85, _sum8);
-                _sum8 = _mm256_fmadd_ps(_w6, _val86, _sum8);
-                _sum8 = _mm256_fmadd_ps(_w7, _val87, _sum8);
-                _sum9 = _mm256_fmadd_ps(_w0, _val90, _sum9);
-                _sum9 = _mm256_fmadd_ps(_w1, _val91, _sum9);
-                _sum9 = _mm256_fmadd_ps(_w2, _val92, _sum9);
-                _sum9 = _mm256_fmadd_ps(_w3, _val93, _sum9);
-                _sum9 = _mm256_fmadd_ps(_w4, _val94, _sum9);
-                _sum9 = _mm256_fmadd_ps(_w5, _val95, _sum9);
-                _sum9 = _mm256_fmadd_ps(_w6, _val96, _sum9);
-                _sum9 = _mm256_fmadd_ps(_w7, _val97, _sum9);
+                _sum8 = _mm256_comp_fmadd_ps(_w0, _val80, _sum8);
+                _sum8 = _mm256_comp_fmadd_ps(_w1, _val81, _sum8);
+                _sum8 = _mm256_comp_fmadd_ps(_w2, _val82, _sum8);
+                _sum8 = _mm256_comp_fmadd_ps(_w3, _val83, _sum8);
+                _sum8 = _mm256_comp_fmadd_ps(_w4, _val84, _sum8);
+                _sum8 = _mm256_comp_fmadd_ps(_w5, _val85, _sum8);
+                _sum8 = _mm256_comp_fmadd_ps(_w6, _val86, _sum8);
+                _sum8 = _mm256_comp_fmadd_ps(_w7, _val87, _sum8);
+                _sum9 = _mm256_comp_fmadd_ps(_w0, _val90, _sum9);
+                _sum9 = _mm256_comp_fmadd_ps(_w1, _val91, _sum9);
+                _sum9 = _mm256_comp_fmadd_ps(_w2, _val92, _sum9);
+                _sum9 = _mm256_comp_fmadd_ps(_w3, _val93, _sum9);
+                _sum9 = _mm256_comp_fmadd_ps(_w4, _val94, _sum9);
+                _sum9 = _mm256_comp_fmadd_ps(_w5, _val95, _sum9);
+                _sum9 = _mm256_comp_fmadd_ps(_w6, _val96, _sum9);
+                _sum9 = _mm256_comp_fmadd_ps(_w7, _val97, _sum9);
 
                 __m256 _val100 = _mm256_broadcast_ss(tmpptr + 80);
                 __m256 _val101 = _mm256_broadcast_ss(tmpptr + 81);
@@ -576,22 +576,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val116 = _mm256_broadcast_ss(tmpptr + 94);
                 __m256 _val117 = _mm256_broadcast_ss(tmpptr + 95);
 
-                _sum10 = _mm256_fmadd_ps(_w0, _val100, _sum10);
-                _sum10 = _mm256_fmadd_ps(_w1, _val101, _sum10);
-                _sum10 = _mm256_fmadd_ps(_w2, _val102, _sum10);
-                _sum10 = _mm256_fmadd_ps(_w3, _val103, _sum10);
-                _sum10 = _mm256_fmadd_ps(_w4, _val104, _sum10);
-                _sum10 = _mm256_fmadd_ps(_w5, _val105, _sum10);
-                _sum10 = _mm256_fmadd_ps(_w6, _val106, _sum10);
-                _sum10 = _mm256_fmadd_ps(_w7, _val107, _sum10);
-                _sum11 = _mm256_fmadd_ps(_w0, _val110, _sum11);
-                _sum11 = _mm256_fmadd_ps(_w1, _val111, _sum11);
-                _sum11 = _mm256_fmadd_ps(_w2, _val112, _sum11);
-                _sum11 = _mm256_fmadd_ps(_w3, _val113, _sum11);
-                _sum11 = _mm256_fmadd_ps(_w4, _val114, _sum11);
-                _sum11 = _mm256_fmadd_ps(_w5, _val115, _sum11);
-                _sum11 = _mm256_fmadd_ps(_w6, _val116, _sum11);
-                _sum11 = _mm256_fmadd_ps(_w7, _val117, _sum11);
+                _sum10 = _mm256_comp_fmadd_ps(_w0, _val100, _sum10);
+                _sum10 = _mm256_comp_fmadd_ps(_w1, _val101, _sum10);
+                _sum10 = _mm256_comp_fmadd_ps(_w2, _val102, _sum10);
+                _sum10 = _mm256_comp_fmadd_ps(_w3, _val103, _sum10);
+                _sum10 = _mm256_comp_fmadd_ps(_w4, _val104, _sum10);
+                _sum10 = _mm256_comp_fmadd_ps(_w5, _val105, _sum10);
+                _sum10 = _mm256_comp_fmadd_ps(_w6, _val106, _sum10);
+                _sum10 = _mm256_comp_fmadd_ps(_w7, _val107, _sum10);
+                _sum11 = _mm256_comp_fmadd_ps(_w0, _val110, _sum11);
+                _sum11 = _mm256_comp_fmadd_ps(_w1, _val111, _sum11);
+                _sum11 = _mm256_comp_fmadd_ps(_w2, _val112, _sum11);
+                _sum11 = _mm256_comp_fmadd_ps(_w3, _val113, _sum11);
+                _sum11 = _mm256_comp_fmadd_ps(_w4, _val114, _sum11);
+                _sum11 = _mm256_comp_fmadd_ps(_w5, _val115, _sum11);
+                _sum11 = _mm256_comp_fmadd_ps(_w6, _val116, _sum11);
+                _sum11 = _mm256_comp_fmadd_ps(_w7, _val117, _sum11);
 
                 tmpptr += 96;
 
@@ -654,22 +654,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val16 = _mm256_broadcast_ss(tmpptr + 14);
                 __m256 _val17 = _mm256_broadcast_ss(tmpptr + 15);
 
-                _sum0 = _mm256_fmadd_ps(_w0, _val00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w1, _val01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w2, _val02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w3, _val03, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w4, _val04, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w5, _val05, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w6, _val06, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w7, _val07, _sum0);
-                _sum1 = _mm256_fmadd_ps(_w0, _val10, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w1, _val11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w2, _val12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w3, _val13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w4, _val14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w5, _val15, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w6, _val16, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w7, _val17, _sum1);
+                _sum0 = _mm256_comp_fmadd_ps(_w0, _val00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w1, _val01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w2, _val02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w3, _val03, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w4, _val04, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w5, _val05, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w6, _val06, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w7, _val07, _sum0);
+                _sum1 = _mm256_comp_fmadd_ps(_w0, _val10, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w1, _val11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w2, _val12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w3, _val13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w4, _val14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w5, _val15, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w6, _val16, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w7, _val17, _sum1);
 
                 __m256 _val20 = _mm256_broadcast_ss(tmpptr + 16);
                 __m256 _val21 = _mm256_broadcast_ss(tmpptr + 17);
@@ -688,22 +688,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val36 = _mm256_broadcast_ss(tmpptr + 30);
                 __m256 _val37 = _mm256_broadcast_ss(tmpptr + 31);
 
-                _sum2 = _mm256_fmadd_ps(_w0, _val20, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w1, _val21, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w2, _val22, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w3, _val23, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w4, _val24, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w5, _val25, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w6, _val26, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w7, _val27, _sum2);
-                _sum3 = _mm256_fmadd_ps(_w0, _val30, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w1, _val31, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w2, _val32, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w3, _val33, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w4, _val34, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w5, _val35, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w6, _val36, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w7, _val37, _sum3);
+                _sum2 = _mm256_comp_fmadd_ps(_w0, _val20, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w1, _val21, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w2, _val22, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w3, _val23, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w4, _val24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w5, _val25, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w6, _val26, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w7, _val27, _sum2);
+                _sum3 = _mm256_comp_fmadd_ps(_w0, _val30, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w1, _val31, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w2, _val32, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w3, _val33, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w4, _val34, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w5, _val35, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w6, _val36, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w7, _val37, _sum3);
 
                 __m256 _val40 = _mm256_broadcast_ss(tmpptr + 32);
                 __m256 _val41 = _mm256_broadcast_ss(tmpptr + 33);
@@ -722,22 +722,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val56 = _mm256_broadcast_ss(tmpptr + 46);
                 __m256 _val57 = _mm256_broadcast_ss(tmpptr + 47);
 
-                _sum4 = _mm256_fmadd_ps(_w0, _val40, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w1, _val41, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w2, _val42, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w3, _val43, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w4, _val44, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w5, _val45, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w6, _val46, _sum4);
-                _sum4 = _mm256_fmadd_ps(_w7, _val47, _sum4);
-                _sum5 = _mm256_fmadd_ps(_w0, _val50, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w1, _val51, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w2, _val52, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w3, _val53, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w4, _val54, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w5, _val55, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w6, _val56, _sum5);
-                _sum5 = _mm256_fmadd_ps(_w7, _val57, _sum5);
+                _sum4 = _mm256_comp_fmadd_ps(_w0, _val40, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w1, _val41, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w2, _val42, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w3, _val43, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w4, _val44, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w5, _val45, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w6, _val46, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_w7, _val47, _sum4);
+                _sum5 = _mm256_comp_fmadd_ps(_w0, _val50, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w1, _val51, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w2, _val52, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w3, _val53, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w4, _val54, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w5, _val55, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w6, _val56, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_w7, _val57, _sum5);
 
                 __m256 _val60 = _mm256_broadcast_ss(tmpptr + 48);
                 __m256 _val61 = _mm256_broadcast_ss(tmpptr + 49);
@@ -756,22 +756,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val76 = _mm256_broadcast_ss(tmpptr + 62);
                 __m256 _val77 = _mm256_broadcast_ss(tmpptr + 63);
 
-                _sum6 = _mm256_fmadd_ps(_w0, _val60, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w1, _val61, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w2, _val62, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w3, _val63, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w4, _val64, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w5, _val65, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w6, _val66, _sum6);
-                _sum6 = _mm256_fmadd_ps(_w7, _val67, _sum6);
-                _sum7 = _mm256_fmadd_ps(_w0, _val70, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w1, _val71, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w2, _val72, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w3, _val73, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w4, _val74, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w5, _val75, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w6, _val76, _sum7);
-                _sum7 = _mm256_fmadd_ps(_w7, _val77, _sum7);
+                _sum6 = _mm256_comp_fmadd_ps(_w0, _val60, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w1, _val61, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w2, _val62, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w3, _val63, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w4, _val64, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w5, _val65, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w6, _val66, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_w7, _val67, _sum6);
+                _sum7 = _mm256_comp_fmadd_ps(_w0, _val70, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w1, _val71, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w2, _val72, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w3, _val73, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w4, _val74, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w5, _val75, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w6, _val76, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_w7, _val77, _sum7);
 
                 tmpptr += 64;
 
@@ -826,22 +826,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val16 = _mm256_broadcast_ss(tmpptr + 14);
                 __m256 _val17 = _mm256_broadcast_ss(tmpptr + 15);
 
-                _sum0 = _mm256_fmadd_ps(_w0, _val00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w1, _val01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w2, _val02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w3, _val03, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w4, _val04, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w5, _val05, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w6, _val06, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w7, _val07, _sum0);
-                _sum1 = _mm256_fmadd_ps(_w0, _val10, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w1, _val11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w2, _val12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w3, _val13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w4, _val14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w5, _val15, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w6, _val16, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w7, _val17, _sum1);
+                _sum0 = _mm256_comp_fmadd_ps(_w0, _val00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w1, _val01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w2, _val02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w3, _val03, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w4, _val04, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w5, _val05, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w6, _val06, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w7, _val07, _sum0);
+                _sum1 = _mm256_comp_fmadd_ps(_w0, _val10, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w1, _val11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w2, _val12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w3, _val13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w4, _val14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w5, _val15, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w6, _val16, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w7, _val17, _sum1);
 
                 __m256 _val20 = _mm256_broadcast_ss(tmpptr + 16);
                 __m256 _val21 = _mm256_broadcast_ss(tmpptr + 17);
@@ -860,22 +860,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _val36 = _mm256_broadcast_ss(tmpptr + 30);
                 __m256 _val37 = _mm256_broadcast_ss(tmpptr + 31);
 
-                _sum2 = _mm256_fmadd_ps(_w0, _val20, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w1, _val21, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w2, _val22, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w3, _val23, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w4, _val24, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w5, _val25, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w6, _val26, _sum2);
-                _sum2 = _mm256_fmadd_ps(_w7, _val27, _sum2);
-                _sum3 = _mm256_fmadd_ps(_w0, _val30, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w1, _val31, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w2, _val32, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w3, _val33, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w4, _val34, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w5, _val35, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w6, _val36, _sum3);
-                _sum3 = _mm256_fmadd_ps(_w7, _val37, _sum3);
+                _sum2 = _mm256_comp_fmadd_ps(_w0, _val20, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w1, _val21, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w2, _val22, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w3, _val23, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w4, _val24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w5, _val25, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w6, _val26, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_w7, _val27, _sum2);
+                _sum3 = _mm256_comp_fmadd_ps(_w0, _val30, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w1, _val31, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w2, _val32, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w3, _val33, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w4, _val34, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w5, _val35, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w6, _val36, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_w7, _val37, _sum3);
 
                 tmpptr += 32;
 
@@ -924,22 +924,22 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _w6 = loadfp16(kptr + 48);
                 __m256 _w7 = loadfp16(kptr + 56);
 
-                _sum0 = _mm256_fmadd_ps(_w0, _val00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w1, _val01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w2, _val02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w3, _val03, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w4, _val04, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w5, _val05, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w6, _val06, _sum0);
-                _sum0 = _mm256_fmadd_ps(_w7, _val07, _sum0);
-                _sum1 = _mm256_fmadd_ps(_w0, _val10, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w1, _val11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w2, _val12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w3, _val13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w4, _val14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w5, _val15, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w6, _val16, _sum1);
-                _sum1 = _mm256_fmadd_ps(_w7, _val17, _sum1);
+                _sum0 = _mm256_comp_fmadd_ps(_w0, _val00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w1, _val01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w2, _val02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w3, _val03, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w4, _val04, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w5, _val05, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w6, _val06, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_w7, _val07, _sum0);
+                _sum1 = _mm256_comp_fmadd_ps(_w0, _val10, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w1, _val11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w2, _val12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w3, _val13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w4, _val14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w5, _val15, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w6, _val16, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_w7, _val17, _sum1);
 
                 tmpptr += 16;
 
@@ -977,14 +977,14 @@ static void conv1x1s1_sgemm_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                 __m256 _w6 = loadfp16(kptr + 48);
                 __m256 _w7 = loadfp16(kptr + 56);
 
-                _sum = _mm256_fmadd_ps(_w0, _val0, _sum);
-                _sum = _mm256_fmadd_ps(_w1, _val1, _sum);
-                _sum = _mm256_fmadd_ps(_w2, _val2, _sum);
-                _sum = _mm256_fmadd_ps(_w3, _val3, _sum);
-                _sum = _mm256_fmadd_ps(_w4, _val4, _sum);
-                _sum = _mm256_fmadd_ps(_w5, _val5, _sum);
-                _sum = _mm256_fmadd_ps(_w6, _val6, _sum);
-                _sum = _mm256_fmadd_ps(_w7, _val7, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w0, _val0, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w1, _val1, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w2, _val2, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w3, _val3, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w4, _val4, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w5, _val5, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w6, _val6, _sum);
+                _sum = _mm256_comp_fmadd_ps(_w7, _val7, _sum);
 
                 tmpptr += 8;
 
@@ -1012,7 +1012,7 @@ static void conv1x1s2_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
     Mat bottom_blob_shrinked;
     bottom_blob_shrinked.create(outw, outh, channels, elemsize, elempack, opt.workspace_allocator);
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < channels; p++)
     {
         const float* r0 = bottom_blob.channel(p);
diff --git a/src/layer/x86/convolution_2x2_pack8.h b/src/layer/x86/convolution_2x2_pack8.h
index 79e6716a..8bb6372b 100644
--- a/src/layer/x86/convolution_2x2_pack8.h
+++ b/src/layer/x86/convolution_2x2_pack8.h
@@ -20,7 +20,7 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
     int outch = top_blob.c;
     const float* bias = _bias;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < outch; p++)
     {
         Mat out0 = top_blob.channel(p);
@@ -66,10 +66,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k03 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k03, _r03, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k03, _r03, _sum0);
 
                     __m256 _k04 = _mm256_loadu_ps(kptr);
                     __m256 _k05 = _mm256_loadu_ps(kptr + 8);
@@ -77,10 +77,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k07 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k04, _r04, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k05, _r05, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k06, _r06, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k07, _r07, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k04, _r04, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k05, _r05, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k06, _r06, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k07, _r07, _sum0);
 
                     //========================================
 
@@ -94,14 +94,14 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _r07 = _mm256_broadcast_ss(r0 + 7);
                     r0 += 8;
 
-                    _sum1 = _mm256_fmadd_ps(_k00, _r00, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k02, _r02, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k03, _r03, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k04, _r04, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k05, _r05, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k06, _r06, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k07, _r07, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k00, _r00, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k02, _r02, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k03, _r03, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k04, _r04, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k05, _r05, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k06, _r06, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k07, _r07, _sum1);
 
                     _k00 = _mm256_loadu_ps(kptr);
                     _k01 = _mm256_loadu_ps(kptr + 8);
@@ -109,10 +109,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _k03 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k03, _r03, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k03, _r03, _sum0);
 
                     _k04 = _mm256_loadu_ps(kptr);
                     _k05 = _mm256_loadu_ps(kptr + 8);
@@ -120,10 +120,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _k07 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k04, _r04, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k05, _r05, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k06, _r06, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k07, _r07, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k04, _r04, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k05, _r05, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k06, _r06, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k07, _r07, _sum0);
 
                     _r00 = _mm256_broadcast_ss(r0);
                     _r01 = _mm256_broadcast_ss(r0 + 1);
@@ -134,14 +134,14 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _r06 = _mm256_broadcast_ss(r0 + 6);
                     _r07 = _mm256_broadcast_ss(r0 + 7);
 
-                    _sum1 = _mm256_fmadd_ps(_k00, _r00, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k02, _r02, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k03, _r03, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k04, _r04, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k05, _r05, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k06, _r06, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k07, _r07, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k00, _r00, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k02, _r02, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k03, _r03, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k04, _r04, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k05, _r05, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k06, _r06, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k07, _r07, _sum1);
                     //===============
 
                     __m256 _r10 = _mm256_broadcast_ss(r1);
@@ -159,10 +159,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k13 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k13, _r13, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k13, _r13, _sum0);
 
                     __m256 _k14 = _mm256_loadu_ps(kptr);
                     __m256 _k15 = _mm256_loadu_ps(kptr + 8);
@@ -170,10 +170,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k17 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k14, _r14, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k15, _r15, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k16, _r16, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k17, _r17, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k14, _r14, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k15, _r15, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k16, _r16, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k17, _r17, _sum0);
 
                     //=======================================
                     r1 += 8;
@@ -186,14 +186,14 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _r16 = _mm256_broadcast_ss(r1 + 6);
                     _r17 = _mm256_broadcast_ss(r1 + 7);
 
-                    _sum1 = _mm256_fmadd_ps(_k10, _r10, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k11, _r11, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k12, _r12, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k13, _r13, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k14, _r14, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k15, _r15, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k16, _r16, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k17, _r17, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k10, _r10, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k11, _r11, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k12, _r12, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k13, _r13, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k14, _r14, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k15, _r15, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k16, _r16, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k17, _r17, _sum1);
 
                     _k10 = _mm256_loadu_ps(kptr);
                     _k11 = _mm256_loadu_ps(kptr + 8);
@@ -201,19 +201,19 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _k13 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k13, _r13, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k13, _r13, _sum0);
 
                     _k14 = _mm256_loadu_ps(kptr);
                     _k15 = _mm256_loadu_ps(kptr + 8);
                     _k16 = _mm256_loadu_ps(kptr + 16);
                     _k17 = _mm256_loadu_ps(kptr + 24);
-                    _sum0 = _mm256_fmadd_ps(_k14, _r14, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k15, _r15, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k16, _r16, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k17, _r17, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k14, _r14, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k15, _r15, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k16, _r16, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k17, _r17, _sum0);
 
                     r1 += 8;
                     _r10 = _mm256_broadcast_ss(r1);
@@ -225,14 +225,14 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _r16 = _mm256_broadcast_ss(r1 + 6);
                     _r17 = _mm256_broadcast_ss(r1 + 7);
 
-                    _sum1 = _mm256_fmadd_ps(_k10, _r10, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k11, _r11, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k12, _r12, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k13, _r13, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k14, _r14, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k15, _r15, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k16, _r16, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k17, _r17, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k10, _r10, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k11, _r11, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k12, _r12, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k13, _r13, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k14, _r14, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k15, _r15, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k16, _r16, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k17, _r17, _sum1);
 
                     kptr -= 224;
                     _mm256_storeu_ps(outptr0, _sum0);
@@ -259,10 +259,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k03 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k00, _r00, _sum);
-                    _sum = _mm256_fmadd_ps(_k01, _r01, _sum);
-                    _sum = _mm256_fmadd_ps(_k02, _r02, _sum);
-                    _sum = _mm256_fmadd_ps(_k03, _r03, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k00, _r00, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k01, _r01, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k02, _r02, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k03, _r03, _sum);
 
                     __m256 _k04 = _mm256_loadu_ps(kptr);
                     __m256 _k05 = _mm256_loadu_ps(kptr + 8);
@@ -270,10 +270,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k07 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k04, _r04, _sum);
-                    _sum = _mm256_fmadd_ps(_k05, _r05, _sum);
-                    _sum = _mm256_fmadd_ps(_k06, _r06, _sum);
-                    _sum = _mm256_fmadd_ps(_k07, _r07, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k04, _r04, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k05, _r05, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k06, _r06, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k07, _r07, _sum);
 
                     //========================================
                     r0 += 8;
@@ -292,10 +292,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _k03 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k00, _r00, _sum);
-                    _sum = _mm256_fmadd_ps(_k01, _r01, _sum);
-                    _sum = _mm256_fmadd_ps(_k02, _r02, _sum);
-                    _sum = _mm256_fmadd_ps(_k03, _r03, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k00, _r00, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k01, _r01, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k02, _r02, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k03, _r03, _sum);
 
                     _k04 = _mm256_loadu_ps(kptr);
                     _k05 = _mm256_loadu_ps(kptr + 8);
@@ -303,10 +303,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _k07 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k04, _r04, _sum);
-                    _sum = _mm256_fmadd_ps(_k05, _r05, _sum);
-                    _sum = _mm256_fmadd_ps(_k06, _r06, _sum);
-                    _sum = _mm256_fmadd_ps(_k07, _r07, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k04, _r04, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k05, _r05, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k06, _r06, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k07, _r07, _sum);
                     //===============
 
                     __m256 _r10 = _mm256_broadcast_ss(r1);
@@ -324,10 +324,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k13 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k10, _r10, _sum);
-                    _sum = _mm256_fmadd_ps(_k11, _r11, _sum);
-                    _sum = _mm256_fmadd_ps(_k12, _r12, _sum);
-                    _sum = _mm256_fmadd_ps(_k13, _r13, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k10, _r10, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k11, _r11, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k12, _r12, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k13, _r13, _sum);
 
                     __m256 _k14 = _mm256_loadu_ps(kptr);
                     __m256 _k15 = _mm256_loadu_ps(kptr + 8);
@@ -335,10 +335,10 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k17 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k14, _r14, _sum);
-                    _sum = _mm256_fmadd_ps(_k15, _r15, _sum);
-                    _sum = _mm256_fmadd_ps(_k16, _r16, _sum);
-                    _sum = _mm256_fmadd_ps(_k17, _r17, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k14, _r14, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k15, _r15, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k16, _r16, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k17, _r17, _sum);
 
                     //=======================================
                     r1 += 8;
@@ -357,19 +357,19 @@ static void conv2x2s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     _k13 = _mm256_loadu_ps(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k10, _r10, _sum);
-                    _sum = _mm256_fmadd_ps(_k11, _r11, _sum);
-                    _sum = _mm256_fmadd_ps(_k12, _r12, _sum);
-                    _sum = _mm256_fmadd_ps(_k13, _r13, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k10, _r10, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k11, _r11, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k12, _r12, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k13, _r13, _sum);
 
                     _k14 = _mm256_loadu_ps(kptr);
                     _k15 = _mm256_loadu_ps(kptr + 8);
                     _k16 = _mm256_loadu_ps(kptr + 16);
                     _k17 = _mm256_loadu_ps(kptr + 24);
-                    _sum = _mm256_fmadd_ps(_k14, _r14, _sum);
-                    _sum = _mm256_fmadd_ps(_k15, _r15, _sum);
-                    _sum = _mm256_fmadd_ps(_k16, _r16, _sum);
-                    _sum = _mm256_fmadd_ps(_k17, _r17, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k14, _r14, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k15, _r15, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k16, _r16, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k17, _r17, _sum);
 
                     kptr -= 224;
                     _mm256_storeu_ps(outptr0, _sum);
diff --git a/src/layer/x86/convolution_2x2_pack8_fp16.h b/src/layer/x86/convolution_2x2_pack8_fp16.h
index 68bbcfe0..df4a0ed5 100644
--- a/src/layer/x86/convolution_2x2_pack8_fp16.h
+++ b/src/layer/x86/convolution_2x2_pack8_fp16.h
@@ -201,7 +201,7 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
     int outch = top_blob.c;
     const float* bias = _bias;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < outch; p++)
     {
         Mat out0 = top_blob.channel(p);
@@ -247,10 +247,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k03 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k03, _r03, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k03, _r03, _sum0);
 
                     __m256 _k04 = loadfp16(kptr);
                     __m256 _k05 = loadfp16(kptr + 8);
@@ -258,10 +258,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k07 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k04, _r04, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k05, _r05, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k06, _r06, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k07, _r07, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k04, _r04, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k05, _r05, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k06, _r06, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k07, _r07, _sum0);
 
                     //========================================
 
@@ -275,14 +275,14 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _r07 = _mm256_broadcast_ss(r0 + 7);
                     r0 += 8;
 
-                    _sum1 = _mm256_fmadd_ps(_k00, _r00, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k02, _r02, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k03, _r03, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k04, _r04, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k05, _r05, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k06, _r06, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k07, _r07, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k00, _r00, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k02, _r02, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k03, _r03, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k04, _r04, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k05, _r05, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k06, _r06, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k07, _r07, _sum1);
 
                     _k00 = loadfp16(kptr);
                     _k01 = loadfp16(kptr + 8);
@@ -290,10 +290,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _k03 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k03, _r03, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k03, _r03, _sum0);
 
                     _k04 = loadfp16(kptr);
                     _k05 = loadfp16(kptr + 8);
@@ -301,10 +301,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _k07 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k04, _r04, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k05, _r05, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k06, _r06, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k07, _r07, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k04, _r04, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k05, _r05, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k06, _r06, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k07, _r07, _sum0);
 
                     _r00 = _mm256_broadcast_ss(r0);
                     _r01 = _mm256_broadcast_ss(r0 + 1);
@@ -315,14 +315,14 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _r06 = _mm256_broadcast_ss(r0 + 6);
                     _r07 = _mm256_broadcast_ss(r0 + 7);
 
-                    _sum1 = _mm256_fmadd_ps(_k00, _r00, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k02, _r02, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k03, _r03, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k04, _r04, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k05, _r05, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k06, _r06, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k07, _r07, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k00, _r00, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k02, _r02, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k03, _r03, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k04, _r04, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k05, _r05, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k06, _r06, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k07, _r07, _sum1);
                     //===============
 
                     __m256 _r10 = _mm256_broadcast_ss(r1);
@@ -340,10 +340,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k13 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k13, _r13, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k13, _r13, _sum0);
 
                     __m256 _k14 = loadfp16(kptr);
                     __m256 _k15 = loadfp16(kptr + 8);
@@ -351,10 +351,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k17 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k14, _r14, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k15, _r15, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k16, _r16, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k17, _r17, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k14, _r14, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k15, _r15, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k16, _r16, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k17, _r17, _sum0);
 
                     //=======================================
                     r1 += 8;
@@ -367,14 +367,14 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _r16 = _mm256_broadcast_ss(r1 + 6);
                     _r17 = _mm256_broadcast_ss(r1 + 7);
 
-                    _sum1 = _mm256_fmadd_ps(_k10, _r10, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k11, _r11, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k12, _r12, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k13, _r13, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k14, _r14, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k15, _r15, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k16, _r16, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k17, _r17, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k10, _r10, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k11, _r11, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k12, _r12, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k13, _r13, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k14, _r14, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k15, _r15, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k16, _r16, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k17, _r17, _sum1);
 
                     _k10 = loadfp16(kptr);
                     _k11 = loadfp16(kptr + 8);
@@ -382,19 +382,19 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _k13 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k13, _r13, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k13, _r13, _sum0);
 
                     _k14 = loadfp16(kptr);
                     _k15 = loadfp16(kptr + 8);
                     _k16 = loadfp16(kptr + 16);
                     _k17 = loadfp16(kptr + 24);
-                    _sum0 = _mm256_fmadd_ps(_k14, _r14, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k15, _r15, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k16, _r16, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_k17, _r17, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k14, _r14, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k15, _r15, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k16, _r16, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_k17, _r17, _sum0);
 
                     r1 += 8;
                     _r10 = _mm256_broadcast_ss(r1);
@@ -406,14 +406,14 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _r16 = _mm256_broadcast_ss(r1 + 6);
                     _r17 = _mm256_broadcast_ss(r1 + 7);
 
-                    _sum1 = _mm256_fmadd_ps(_k10, _r10, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k11, _r11, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k12, _r12, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k13, _r13, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k14, _r14, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k15, _r15, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k16, _r16, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_k17, _r17, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k10, _r10, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k11, _r11, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k12, _r12, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k13, _r13, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k14, _r14, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k15, _r15, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k16, _r16, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_k17, _r17, _sum1);
 
                     kptr -= 224;
                     _mm256_storeu_ps(outptr0, _sum0);
@@ -440,10 +440,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k03 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k00, _r00, _sum);
-                    _sum = _mm256_fmadd_ps(_k01, _r01, _sum);
-                    _sum = _mm256_fmadd_ps(_k02, _r02, _sum);
-                    _sum = _mm256_fmadd_ps(_k03, _r03, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k00, _r00, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k01, _r01, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k02, _r02, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k03, _r03, _sum);
 
                     __m256 _k04 = loadfp16(kptr);
                     __m256 _k05 = loadfp16(kptr + 8);
@@ -451,10 +451,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k07 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k04, _r04, _sum);
-                    _sum = _mm256_fmadd_ps(_k05, _r05, _sum);
-                    _sum = _mm256_fmadd_ps(_k06, _r06, _sum);
-                    _sum = _mm256_fmadd_ps(_k07, _r07, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k04, _r04, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k05, _r05, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k06, _r06, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k07, _r07, _sum);
 
                     //========================================
                     r0 += 8;
@@ -473,10 +473,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _k03 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k00, _r00, _sum);
-                    _sum = _mm256_fmadd_ps(_k01, _r01, _sum);
-                    _sum = _mm256_fmadd_ps(_k02, _r02, _sum);
-                    _sum = _mm256_fmadd_ps(_k03, _r03, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k00, _r00, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k01, _r01, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k02, _r02, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k03, _r03, _sum);
 
                     _k04 = loadfp16(kptr);
                     _k05 = loadfp16(kptr + 8);
@@ -484,10 +484,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _k07 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k04, _r04, _sum);
-                    _sum = _mm256_fmadd_ps(_k05, _r05, _sum);
-                    _sum = _mm256_fmadd_ps(_k06, _r06, _sum);
-                    _sum = _mm256_fmadd_ps(_k07, _r07, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k04, _r04, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k05, _r05, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k06, _r06, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k07, _r07, _sum);
                     //===============
 
                     __m256 _r10 = _mm256_broadcast_ss(r1);
@@ -505,10 +505,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k13 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k10, _r10, _sum);
-                    _sum = _mm256_fmadd_ps(_k11, _r11, _sum);
-                    _sum = _mm256_fmadd_ps(_k12, _r12, _sum);
-                    _sum = _mm256_fmadd_ps(_k13, _r13, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k10, _r10, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k11, _r11, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k12, _r12, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k13, _r13, _sum);
 
                     __m256 _k14 = loadfp16(kptr);
                     __m256 _k15 = loadfp16(kptr + 8);
@@ -516,10 +516,10 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k17 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k14, _r14, _sum);
-                    _sum = _mm256_fmadd_ps(_k15, _r15, _sum);
-                    _sum = _mm256_fmadd_ps(_k16, _r16, _sum);
-                    _sum = _mm256_fmadd_ps(_k17, _r17, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k14, _r14, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k15, _r15, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k16, _r16, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k17, _r17, _sum);
 
                     //=======================================
                     r1 += 8;
@@ -538,19 +538,19 @@ static void conv2x2s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, cons
                     _k13 = loadfp16(kptr + 24);
                     kptr += 32;
 
-                    _sum = _mm256_fmadd_ps(_k10, _r10, _sum);
-                    _sum = _mm256_fmadd_ps(_k11, _r11, _sum);
-                    _sum = _mm256_fmadd_ps(_k12, _r12, _sum);
-                    _sum = _mm256_fmadd_ps(_k13, _r13, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k10, _r10, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k11, _r11, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k12, _r12, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k13, _r13, _sum);
 
                     _k14 = loadfp16(kptr);
                     _k15 = loadfp16(kptr + 8);
                     _k16 = loadfp16(kptr + 16);
                     _k17 = loadfp16(kptr + 24);
-                    _sum = _mm256_fmadd_ps(_k14, _r14, _sum);
-                    _sum = _mm256_fmadd_ps(_k15, _r15, _sum);
-                    _sum = _mm256_fmadd_ps(_k16, _r16, _sum);
-                    _sum = _mm256_fmadd_ps(_k17, _r17, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k14, _r14, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k15, _r15, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k16, _r16, _sum);
+                    _sum = _mm256_comp_fmadd_ps(_k17, _r17, _sum);
 
                     kptr -= 224;
                     _mm256_storeu_ps(outptr0, _sum);
diff --git a/src/layer/x86/convolution_3x3.h b/src/layer/x86/convolution_3x3.h
index 07273acd..7b788d63 100644
--- a/src/layer/x86/convolution_3x3.h
+++ b/src/layer/x86/convolution_3x3.h
@@ -25,7 +25,7 @@ static void conv3x3s1_sse(const Mat& bottom_blob, Mat& top_blob, const Mat& _ker
     const float* kernel = _kernel;
     const float* bias = _bias;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < outch; p++)
     {
         Mat out = top_blob.channel(p);
@@ -146,10 +146,9 @@ static void conv3x3s1_winograd23_transform_kernel_sse(const Mat& kernel, Mat& ke
         {1.0f, 0.0f, 0.0f},
         {1.0f / 2, 1.0f / 2, 1.0f / 2},
         {1.0f / 2, -1.0f / 2, 1.0f / 2},
-        {0.0f, 0.0f, 1.0f}
-    };
+        {0.0f, 0.0f, 1.0f}};
 
-    #pragma omp parallel for
+#pragma omp parallel for
     for (int p = 0; p < outch; p++)
     {
         for (int q = 0; q < inch; q++)
@@ -222,14 +221,14 @@ static void conv3x3s1_winograd23_sse(const Mat& bottom_blob, Mat& top_blob, cons
 
         bottom_blob_tm.create(4 * 4, tiles, inch, 4u, opt.workspace_allocator);
 
-        // BT
-        // const float itm[4][4] = {
-        //     {1.0f,  0.0f, -1.0f,  0.0f},
-        //     {0.0f,  1.0f,  1.00f, 0.0f},
-        //     {0.0f, -1.0f,  1.00f, 0.0f},
-        //     {0.0f, -1.0f,  0.00f, 1.0f}
-        // };
-        #pragma omp parallel for num_threads(opt.num_threads)
+// BT
+// const float itm[4][4] = {
+//     {1.0f,  0.0f, -1.0f,  0.0f},
+//     {0.0f,  1.0f,  1.00f, 0.0f},
+//     {0.0f, -1.0f,  1.00f, 0.0f},
+//     {0.0f, -1.0f,  0.00f, 1.0f}
+// };
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < inch; q++)
         {
             const float* img = bottom_blob_bordered.channel(q);
@@ -358,7 +357,7 @@ static void conv3x3s1_winograd23_sse(const Mat& bottom_blob, Mat& top_blob, cons
         int nn_outch = outch >> 2;
         int remain_outch_start = nn_outch << 2;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int pp = 0; pp < nn_outch; pp++)
         {
             int p = pp * 4;
@@ -417,14 +416,14 @@ static void conv3x3s1_winograd23_sse(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k2n = _mm256_loadu_ps(k2 + 8);
                     __m256 _k3 = _mm256_loadu_ps(k3);
                     __m256 _k3n = _mm256_loadu_ps(k3 + 8);
-                    _sum0 = _mm256_fmadd_ps(_r0, _k0, _sum0);
-                    _sum0n = _mm256_fmadd_ps(_r0n, _k0n, _sum0n);
-                    _sum1 = _mm256_fmadd_ps(_r0, _k1, _sum1);
-                    _sum1n = _mm256_fmadd_ps(_r0n, _k1n, _sum1n);
-                    _sum2 = _mm256_fmadd_ps(_r0, _k2, _sum2);
-                    _sum2n = _mm256_fmadd_ps(_r0n, _k2n, _sum2n);
-                    _sum3 = _mm256_fmadd_ps(_r0, _k3, _sum3);
-                    _sum3n = _mm256_fmadd_ps(_r0n, _k3n, _sum3n);
+                    _sum0 = _mm256_comp_fmadd_ps(_r0, _k0, _sum0);
+                    _sum0n = _mm256_comp_fmadd_ps(_r0n, _k0n, _sum0n);
+                    _sum1 = _mm256_comp_fmadd_ps(_r0, _k1, _sum1);
+                    _sum1n = _mm256_comp_fmadd_ps(_r0n, _k1n, _sum1n);
+                    _sum2 = _mm256_comp_fmadd_ps(_r0, _k2, _sum2);
+                    _sum2n = _mm256_comp_fmadd_ps(_r0n, _k2n, _sum2n);
+                    _sum3 = _mm256_comp_fmadd_ps(_r0, _k3, _sum3);
+                    _sum3n = _mm256_comp_fmadd_ps(_r0n, _k3n, _sum3n);
 
                     // k1
                     _r0 = _mm256_loadu_ps(r1);
@@ -437,14 +436,14 @@ static void conv3x3s1_winograd23_sse(const Mat& bottom_blob, Mat& top_blob, cons
                     _k2n = _mm256_loadu_ps(k2 + 24);
                     _k3 = _mm256_loadu_ps(k3 + 16);
                     _k3n = _mm256_loadu_ps(k3 + 24);
-                    _sum0 = _mm256_fmadd_ps(_r0, _k0, _sum0);
-                    _sum0n = _mm256_fmadd_ps(_r0n, _k0n, _sum0n);
-                    _sum1 = _mm256_fmadd_ps(_r0, _k1, _sum1);
-                    _sum1n = _mm256_fmadd_ps(_r0n, _k1n, _sum1n);
-                    _sum2 = _mm256_fmadd_ps(_r0, _k2, _sum2);
-                    _sum2n = _mm256_fmadd_ps(_r0n, _k2n, _sum2n);
-                    _sum3 = _mm256_fmadd_ps(_r0, _k3, _sum3);
-                    _sum3n = _mm256_fmadd_ps(_r0n, _k3n, _sum3n);
+                    _sum0 = _mm256_comp_fmadd_ps(_r0, _k0, _sum0);
+                    _sum0n = _mm256_comp_fmadd_ps(_r0n, _k0n, _sum0n);
+                    _sum1 = _mm256_comp_fmadd_ps(_r0, _k1, _sum1);
+                    _sum1n = _mm256_comp_fmadd_ps(_r0n, _k1n, _sum1n);
+                    _sum2 = _mm256_comp_fmadd_ps(_r0, _k2, _sum2);
+                    _sum2n = _mm256_comp_fmadd_ps(_r0n, _k2n, _sum2n);
+                    _sum3 = _mm256_comp_fmadd_ps(_r0, _k3, _sum3);
+                    _sum3n = _mm256_comp_fmadd_ps(_r0n, _k3n, _sum3n);
                     // k2
                     _r0 = _mm256_loadu_ps(r2);
                     _r0n = _mm256_loadu_ps(r2 + 8);
@@ -456,14 +455,14 @@ static void conv3x3s1_winograd23_sse(const Mat& bottom_blob, Mat& top_blob, cons
                     _k2n = _mm256_loadu_ps(k2 + 40);
                     _k3 = _mm256_loadu_ps(k3 + 32);
                     _k3n = _mm256_loadu_ps(k3 + 40);
-                    _sum0 = _mm256_fmadd_ps(_r0, _k0, _sum0);
-                    _sum0n = _mm256_fmadd_ps(_r0n, _k0n, _sum0n);
-                    _sum1 = _mm256_fmadd_ps(_r0, _k1, _sum1);
-                    _sum1n = _mm256_fmadd_ps(_r0n, _k1n, _sum1n);
-                    _sum2 = _mm256_fmadd_ps(_r0, _k2, _sum2);
-                    _sum2n = _mm256_fmadd_ps(_r0n, _k2n, _sum2n);
-                    _sum3 = _mm256_fmadd_ps(_r0, _k3, _sum3);
-                    _sum3n = _mm256_fmadd_ps(_r0n, _k3n, _sum3n);
+                    _sum0 = _mm256_comp_fmadd_ps(_r0, _k0, _sum0);
+                    _sum0n = _mm256_comp_fmadd_ps(_r0n, _k0n, _sum0n);
+                    _sum1 = _mm256_comp_fmadd_ps(_r0, _k1, _sum1);
+                    _sum1n = _mm256_comp_fmadd_ps(_r0n, _k1n, _sum1n);
+                    _sum2 = _mm256_comp_fmadd_ps(_r0, _k2, _sum2);
+                    _sum2n = _mm256_comp_fmadd_ps(_r0n, _k2n, _sum2n);
+                    _sum3 = _mm256_comp_fmadd_ps(_r0, _k3, _sum3);
+                    _sum3n = _mm256_comp_fmadd_ps(_r0n, _k3n, _sum3n);
                     // k3
                     _r0 = _mm256_loadu_ps(r3);
                     _r0n = _mm256_loadu_ps(r3 + 8);
@@ -475,14 +474,14 @@ static void conv3x3s1_winograd23_sse(const Mat& bottom_blob, Mat& top_blob, cons
                     _k2n = _mm256_loadu_ps(k2 + 56);
                     _k3 = _mm256_loadu_ps(k3 + 48);
                     _k3n = _mm256_loadu_ps(k3 + 56);
-                    _sum0 = _mm256_fmadd_ps(_r0, _k0, _sum0);
-                    _sum0n = _mm256_fmadd_ps(_r0n, _k0n, _sum0n);
-                    _sum1 = _mm256_fmadd_ps(_r0, _k1, _sum1);
-                    _sum1n = _mm256_fmadd_ps(_r0n, _k1n, _sum1n);
-                    _sum2 = _mm256_fmadd_ps(_r0, _k2, _sum2);
-                    _sum2n = _mm256_fmadd_ps(_r0n, _k2n, _sum2n);
-                    _sum3 = _mm256_fmadd_ps(_r0, _k3, _sum3);
-                    _sum3n = _mm256_fmadd_ps(_r0n, _k3n, _sum3n);
+                    _sum0 = _mm256_comp_fmadd_ps(_r0, _k0, _sum0);
+                    _sum0n = _mm256_comp_fmadd_ps(_r0n, _k0n, _sum0n);
+                    _sum1 = _mm256_comp_fmadd_ps(_r0, _k1, _sum1);
+                    _sum1n = _mm256_comp_fmadd_ps(_r0n, _k1n, _sum1n);
+                    _sum2 = _mm256_comp_fmadd_ps(_r0, _k2, _sum2);
+                    _sum2n = _mm256_comp_fmadd_ps(_r0n, _k2n, _sum2n);
+                    _sum3 = _mm256_comp_fmadd_ps(_r0, _k3, _sum3);
+                    _sum3n = _mm256_comp_fmadd_ps(_r0n, _k3n, _sum3n);
                 }
 
                 for (; q < inch; q++)
@@ -505,14 +504,14 @@ static void conv3x3s1_winograd23_sse(const Mat& bottom_blob, Mat& top_blob, cons
                     __m256 _k3 = _mm256_loadu_ps(k3);
                     __m256 _k3n = _mm256_loadu_ps(k3 + 8);
 
-                    _sum0 = _mm256_fmadd_ps(_r0, _k0, _sum0);
-                    _sum0n = _mm256_fmadd_ps(_r0n, _k0n, _sum0n);
-                    _sum1 = _mm256_fmadd_ps(_r0, _k1, _sum1);
-                    _sum1n = _mm256_fmadd_ps(_r0n, _k1n, _sum1n);
-                    _sum2 = _mm256_fmadd_ps(_r0, _k2, _sum2);
-                    _sum2n = _mm256_fmadd_ps(_r0n, _k2n, _sum2n);
-                    _sum3 = _mm256_fmadd_ps(_r0, _k3, _sum3);
-                    _sum3n = _mm256_fmadd_ps(_r0n, _k3n, _sum3n);
+                    _sum0 = _mm256_comp_fmadd_ps(_r0, _k0, _sum0);
+                    _sum0n = _mm256_comp_fmadd_ps(_r0n, _k0n, _sum0n);
+                    _sum1 = _mm256_comp_fmadd_ps(_r0, _k1, _sum1);
+                    _sum1n = _mm256_comp_fmadd_ps(_r0n, _k1n, _sum1n);
+                    _sum2 = _mm256_comp_fmadd_ps(_r0, _k2, _sum2);
+                    _sum2n = _mm256_comp_fmadd_ps(_r0n, _k2n, _sum2n);
+                    _sum3 = _mm256_comp_fmadd_ps(_r0, _k3, _sum3);
+                    _sum3n = _mm256_comp_fmadd_ps(_r0n, _k3n, _sum3n);
                 }
 
                 _mm256_storeu_ps(output0_tm, _sum0);
@@ -611,7 +610,7 @@ static void conv3x3s1_winograd23_sse(const Mat& bottom_blob, Mat& top_blob, cons
             }
         }
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = remain_outch_start; p < outch; p++)
         {
             Mat out0_tm = top_blob_tm.channel(p);
@@ -689,7 +688,7 @@ static void conv3x3s1_winograd23_sse(const Mat& bottom_blob, Mat& top_blob, cons
         int nColBlocks = h_tm / 4; // may be the block num in Feathercnn
         int nRowBlocks = w_tm / 4;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < outch; p++)
         {
             Mat out_tm = top_blob_tm.channel(p);
@@ -770,10 +769,9 @@ static void conv3x3s1_winograd43_transform_kernel_sse(const Mat& kernel, std::ve
         {-1.0f / 6, 1.0f / 6, -1.0f / 6},
         {1.0f / 24, 1.0f / 12, 1.0f / 6},
         {1.0f / 24, -1.0f / 12, 1.0f / 6},
-        {0.0f, 0.0f, 1.0f}
-    };
+        {0.0f, 0.0f, 1.0f}};
 
-    #pragma omp parallel for
+#pragma omp parallel for
     for (int p = 0; p < outch; p++)
     {
         for (int q = 0; q < inch; q++)
@@ -1012,7 +1010,7 @@ static void conv3x3s1_winograd43_sse(const Mat& bottom_blob, Mat& top_blob, cons
         __m256 _5_n = _mm256_set1_ps(-5);
 #endif
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < inch; q++)
         {
             const float* img = bottom_blob_bordered.channel(q);
@@ -1052,31 +1050,31 @@ static void conv3x3s1_winograd43_sse(const Mat& bottom_blob, Mat& top_blob, cons
 
                     // w = B_t * d
                     _w0 = _mm256_mul_ps(_d0, _4_p);
-                    _w0 = _mm256_fmadd_ps(_d2, _5_n, _w0);
+                    _w0 = _mm256_comp_fmadd_ps(_d2, _5_n, _w0);
                     _w0 = _mm256_add_ps(_w0, _d4);
 
                     _w1 = _mm256_mul_ps(_d1, _4_n);
-                    _w1 = _mm256_fmadd_ps(_d2, _4_n, _w1);
+                    _w1 = _mm256_comp_fmadd_ps(_d2, _4_n, _w1);
                     _w1 = _mm256_add_ps(_w1, _d3);
                     _w1 = _mm256_add_ps(_w1, _d4);
 
                     _w2 = _mm256_mul_ps(_d1, _4_p);
-                    _w2 = _mm256_fmadd_ps(_d2, _4_n, _w2);
-                    _w2 = _mm256_fmadd_ps(_d3, _1_n, _w2);
+                    _w2 = _mm256_comp_fmadd_ps(_d2, _4_n, _w2);
+                    _w2 = _mm256_comp_fmadd_ps(_d3, _1_n, _w2);
                     _w2 = _mm256_add_ps(_w2, _d4);
 
                     _w3 = _mm256_mul_ps(_d1, _2_n);
-                    _w3 = _mm256_fmadd_ps(_d2, _1_n, _w3);
-                    _w3 = _mm256_fmadd_ps(_d3, _2_p, _w3);
+                    _w3 = _mm256_comp_fmadd_ps(_d2, _1_n, _w3);
+                    _w3 = _mm256_comp_fmadd_ps(_d3, _2_p, _w3);
                     _w3 = _mm256_add_ps(_w3, _d4);
 
                     _w4 = _mm256_mul_ps(_d1, _2_p);
-                    _w4 = _mm256_fmadd_ps(_d2, _1_n, _w4);
-                    _w4 = _mm256_fmadd_ps(_d3, _2_n, _w4);
+                    _w4 = _mm256_comp_fmadd_ps(_d2, _1_n, _w4);
+                    _w4 = _mm256_comp_fmadd_ps(_d3, _2_n, _w4);
                     _w4 = _mm256_add_ps(_w4, _d4);
 
                     _w5 = _mm256_mul_ps(_d1, _4_p);
-                    _w5 = _mm256_fmadd_ps(_d3, _5_n, _w5);
+                    _w5 = _mm256_comp_fmadd_ps(_d3, _5_n, _w5);
                     _w5 = _mm256_add_ps(_w5, _d5);
                     // transpose d to d_t
 #if (defined _WIN32 && !(defined __MINGW32__) && !__clang__)
@@ -1160,31 +1158,31 @@ static void conv3x3s1_winograd43_sse(const Mat& bottom_blob, Mat& top_blob, cons
 #endif
                     // d = B_t * d_t
                     _n0 = _mm256_mul_ps(_t0, _4_p);
-                    _n0 = _mm256_fmadd_ps(_t2, _5_n, _n0);
+                    _n0 = _mm256_comp_fmadd_ps(_t2, _5_n, _n0);
                     _n0 = _mm256_add_ps(_n0, _t4);
 
                     _n1 = _mm256_mul_ps(_t1, _4_n);
-                    _n1 = _mm256_fmadd_ps(_t2, _4_n, _n1);
+                    _n1 = _mm256_comp_fmadd_ps(_t2, _4_n, _n1);
                     _n1 = _mm256_add_ps(_n1, _t3);
                     _n1 = _mm256_add_ps(_n1, _t4);
 
                     _n2 = _mm256_mul_ps(_t1, _4_p);
-                    _n2 = _mm256_fmadd_ps(_t2, _4_n, _n2);
-                    _n2 = _mm256_fmadd_ps(_t3, _1_n, _n2);
+                    _n2 = _mm256_comp_fmadd_ps(_t2, _4_n, _n2);
+                    _n2 = _mm256_comp_fmadd_ps(_t3, _1_n, _n2);
                     _n2 = _mm256_add_ps(_n2, _t4);
 
                     _n3 = _mm256_mul_ps(_t1, _2_n);
-                    _n3 = _mm256_fmadd_ps(_t2, _1_n, _n3);
-                    _n3 = _mm256_fmadd_ps(_t3, _2_p, _n3);
+                    _n3 = _mm256_comp_fmadd_ps(_t2, _1_n, _n3);
+                    _n3 = _mm256_comp_fmadd_ps(_t3, _2_p, _n3);
                     _n3 = _mm256_add_ps(_n3, _t4);
 
                     _n4 = _mm256_mul_ps(_t1, _2_p);
-                    _n4 = _mm256_fmadd_ps(_t2, _1_n, _n4);
-                    _n4 = _mm256_fmadd_ps(_t3, _2_n, _n4);
+                    _n4 = _mm256_comp_fmadd_ps(_t2, _1_n, _n4);
+                    _n4 = _mm256_comp_fmadd_ps(_t3, _2_n, _n4);
                     _n4 = _mm256_add_ps(_n4, _t4);
 
                     _n5 = _mm256_mul_ps(_t1, _4_p);
-                    _n5 = _mm256_fmadd_ps(_t3, _5_n, _n5);
+                    _n5 = _mm256_comp_fmadd_ps(_t3, _5_n, _n5);
                     _n5 = _mm256_add_ps(_n5, _t5);
                     // save to out_tm
                     float output_n0[8] = {0.f};
@@ -1379,7 +1377,7 @@ static void conv3x3s1_winograd43_sse(const Mat& bottom_blob, Mat& top_blob, cons
 
         top_blob_tm.create(36, tiles, outch, elemsize, opt.workspace_allocator);
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int r = 0; r < 9; r++)
         {
             int nn_outch = 0;
@@ -1864,7 +1862,7 @@ static void conv3x3s1_winograd43_sse(const Mat& bottom_blob, Mat& top_blob, cons
         int nColBlocks = h_tm / 6; // may be the block num in Feathercnn
         int nRowBlocks = w_tm / 6;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < outch; p++)
         {
             float* out_tile = top_blob_tm.channel(p);
@@ -1982,7 +1980,7 @@ static void conv3x3s2_sse(const Mat& bottom_blob, Mat& top_blob, const Mat& _ker
     const float* kernel = _kernel;
     const float* bias = _bias;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < outch; p++)
     {
         Mat out = top_blob.channel(p);
diff --git a/src/layer/x86/convolution_3x3_pack1to8.h b/src/layer/x86/convolution_3x3_pack1to8.h
index b2b7b788..7f30cde2 100644
--- a/src/layer/x86/convolution_3x3_pack1to8.h
+++ b/src/layer/x86/convolution_3x3_pack1to8.h
@@ -23,7 +23,7 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
     int nn_outch = outch >> 1;
     int remain_outch_start = nn_outch << 1;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int pp = 0; pp < nn_outch; pp++)
     {
         int p = pp * 2;
@@ -90,25 +90,25 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
-
-                    _sum10 = _mm256_fmadd_ps(_r01, _k00_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r02, _k01_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r03, _k02_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r11, _k10_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r12, _k11_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r13, _k12_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r21, _k20_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r22, _k21_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r23, _k22_1, _sum10);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
+
+                    _sum10 = _mm256_comp_fmadd_ps(_r01, _k00_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r02, _k01_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r03, _k02_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r11, _k10_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r12, _k11_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r13, _k12_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r21, _k20_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r22, _k21_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r23, _k22_1, _sum10);
 
                     _mm256_storeu_ps(outptr0, _sum00);
                     _mm256_storeu_ps(outptr1, _sum10);
@@ -120,25 +120,25 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r14 = _mm256_broadcast_ss(r1 + 3);
                     __m256 _r24 = _mm256_broadcast_ss(r2 + 3);
 
-                    _sum01 = _mm256_fmadd_ps(_r02, _k00_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r03, _k01_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r04, _k02_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r12, _k10_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r13, _k11_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r14, _k12_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r22, _k20_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r23, _k21_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r24, _k22_0, _sum01);
-
-                    _sum11 = _mm256_fmadd_ps(_r02, _k00_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r03, _k01_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r04, _k02_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r12, _k10_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r13, _k11_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r14, _k12_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r22, _k20_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r23, _k21_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r24, _k22_1, _sum11);
+                    _sum01 = _mm256_comp_fmadd_ps(_r02, _k00_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r03, _k01_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r04, _k02_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r12, _k10_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r13, _k11_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r14, _k12_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r22, _k20_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r23, _k21_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r24, _k22_0, _sum01);
+
+                    _sum11 = _mm256_comp_fmadd_ps(_r02, _k00_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r03, _k01_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r04, _k02_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r12, _k10_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r13, _k11_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r14, _k12_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r22, _k20_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r23, _k21_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r24, _k22_1, _sum11);
 
                     _mm256_storeu_ps(outptr0 + 8, _sum01);
                     _mm256_storeu_ps(outptr1 + 8, _sum11);
@@ -150,25 +150,25 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r15 = _mm256_broadcast_ss(r1 + 4);
                     __m256 _r25 = _mm256_broadcast_ss(r2 + 4);
 
-                    _sum02 = _mm256_fmadd_ps(_r03, _k00_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r04, _k01_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r05, _k02_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r13, _k10_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r14, _k11_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r15, _k12_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r23, _k20_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r24, _k21_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r25, _k22_0, _sum02);
-
-                    _sum12 = _mm256_fmadd_ps(_r03, _k00_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r04, _k01_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r05, _k02_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r13, _k10_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r14, _k11_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r15, _k12_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r23, _k20_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r24, _k21_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r25, _k22_1, _sum12);
+                    _sum02 = _mm256_comp_fmadd_ps(_r03, _k00_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r04, _k01_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r05, _k02_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r13, _k10_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r14, _k11_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r15, _k12_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r23, _k20_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r24, _k21_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r25, _k22_0, _sum02);
+
+                    _sum12 = _mm256_comp_fmadd_ps(_r03, _k00_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r04, _k01_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r05, _k02_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r13, _k10_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r14, _k11_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r15, _k12_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r23, _k20_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r24, _k21_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r25, _k22_1, _sum12);
 
                     _mm256_storeu_ps(outptr0 + 16, _sum02);
                     _mm256_storeu_ps(outptr1 + 16, _sum12);
@@ -180,25 +180,25 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _sum03 = _mm256_loadu_ps(outptr0 + 24);
                     __m256 _sum13 = _mm256_loadu_ps(outptr1 + 24);
 
-                    _sum03 = _mm256_fmadd_ps(_r04, _k00_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r05, _k01_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r06, _k02_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r14, _k10_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r15, _k11_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r16, _k12_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r24, _k20_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r25, _k21_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r26, _k22_0, _sum03);
-
-                    _sum13 = _mm256_fmadd_ps(_r04, _k00_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r05, _k01_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r06, _k02_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r14, _k10_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r15, _k11_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r16, _k12_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r24, _k20_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r25, _k21_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r26, _k22_1, _sum13);
+                    _sum03 = _mm256_comp_fmadd_ps(_r04, _k00_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r05, _k01_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r06, _k02_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r14, _k10_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r15, _k11_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r16, _k12_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r24, _k20_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r25, _k21_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r26, _k22_0, _sum03);
+
+                    _sum13 = _mm256_comp_fmadd_ps(_r04, _k00_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r05, _k01_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r06, _k02_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r14, _k10_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r15, _k11_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r16, _k12_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r24, _k20_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r25, _k21_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r26, _k22_1, _sum13);
 
                     _mm256_storeu_ps(outptr0 + 24, _sum03);
                     _mm256_storeu_ps(outptr1 + 24, _sum13);
@@ -225,25 +225,25 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
-
-                    _sum10 = _mm256_fmadd_ps(_r01, _k00_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r02, _k01_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r03, _k02_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r11, _k10_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r12, _k11_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r13, _k12_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r21, _k20_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r22, _k21_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r23, _k22_1, _sum10);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
+
+                    _sum10 = _mm256_comp_fmadd_ps(_r01, _k00_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r02, _k01_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r03, _k02_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r11, _k10_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r12, _k11_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r13, _k12_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r21, _k20_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r22, _k21_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r23, _k22_1, _sum10);
 
                     _mm256_storeu_ps(outptr0, _sum00);
                     _mm256_storeu_ps(outptr1, _sum10);
@@ -255,25 +255,25 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r14 = _mm256_broadcast_ss(r1 + 3);
                     __m256 _r24 = _mm256_broadcast_ss(r2 + 3);
 
-                    _sum01 = _mm256_fmadd_ps(_r02, _k00_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r03, _k01_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r04, _k02_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r12, _k10_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r13, _k11_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r14, _k12_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r22, _k20_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r23, _k21_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r24, _k22_0, _sum01);
-
-                    _sum11 = _mm256_fmadd_ps(_r02, _k00_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r03, _k01_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r04, _k02_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r12, _k10_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r13, _k11_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r14, _k12_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r22, _k20_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r23, _k21_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r24, _k22_1, _sum11);
+                    _sum01 = _mm256_comp_fmadd_ps(_r02, _k00_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r03, _k01_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r04, _k02_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r12, _k10_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r13, _k11_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r14, _k12_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r22, _k20_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r23, _k21_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r24, _k22_0, _sum01);
+
+                    _sum11 = _mm256_comp_fmadd_ps(_r02, _k00_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r03, _k01_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r04, _k02_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r12, _k10_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r13, _k11_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r14, _k12_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r22, _k20_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r23, _k21_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r24, _k22_1, _sum11);
 
                     _mm256_storeu_ps(outptr0 + 8, _sum01);
                     _mm256_storeu_ps(outptr1 + 8, _sum11);
@@ -300,25 +300,25 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
-
-                    _sum10 = _mm256_fmadd_ps(_r01, _k00_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r02, _k01_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r03, _k02_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r11, _k10_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r12, _k11_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r13, _k12_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r21, _k20_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r22, _k21_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r23, _k22_1, _sum10);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
+
+                    _sum10 = _mm256_comp_fmadd_ps(_r01, _k00_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r02, _k01_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r03, _k02_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r11, _k10_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r12, _k11_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r13, _k12_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r21, _k20_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r22, _k21_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r23, _k22_1, _sum10);
 
                     _mm256_storeu_ps(outptr0, _sum00);
                     _mm256_storeu_ps(outptr1, _sum10);
@@ -340,7 +340,7 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
         }
     }
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = remain_outch_start; p < outch; p++)
     {
         Mat out0 = top_blob.channel(p);
@@ -389,15 +389,15 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum0 = _mm256_fmadd_ps(_r01, _k00, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r02, _k01, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r03, _k02, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r11, _k10, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r12, _k11, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r13, _k12, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r21, _k20, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r22, _k21, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r23, _k22, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r01, _k00, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r02, _k01, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r03, _k02, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r11, _k10, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r12, _k11, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r13, _k12, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r21, _k20, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r22, _k21, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r23, _k22, _sum0);
 
                     __m256 _sum1 = _mm256_loadu_ps(outptr0 + 8);
                     __m256 _r04 = _mm256_broadcast_ss(r0 + 3);
@@ -405,15 +405,15 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r24 = _mm256_broadcast_ss(r2 + 3);
                     _mm256_storeu_ps(outptr0, _sum0);
 
-                    _sum1 = _mm256_fmadd_ps(_r02, _k00, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r03, _k01, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r04, _k02, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r12, _k10, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r13, _k11, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r14, _k12, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r22, _k20, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r23, _k21, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r24, _k22, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r02, _k00, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r03, _k01, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r04, _k02, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r12, _k10, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r13, _k11, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r14, _k12, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r22, _k20, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r23, _k21, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r24, _k22, _sum1);
 
                     __m256 _sum2 = _mm256_loadu_ps(outptr0 + 16);
                     __m256 _r05 = _mm256_broadcast_ss(r0 + 4);
@@ -421,15 +421,15 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r25 = _mm256_broadcast_ss(r2 + 4);
                     _mm256_storeu_ps(outptr0 + 8, _sum1);
 
-                    _sum2 = _mm256_fmadd_ps(_r03, _k00, _sum2);
-                    _sum2 = _mm256_fmadd_ps(_r04, _k01, _sum2);
-                    _sum2 = _mm256_fmadd_ps(_r05, _k02, _sum2);
-                    _sum2 = _mm256_fmadd_ps(_r13, _k10, _sum2);
-                    _sum2 = _mm256_fmadd_ps(_r14, _k11, _sum2);
-                    _sum2 = _mm256_fmadd_ps(_r15, _k12, _sum2);
-                    _sum2 = _mm256_fmadd_ps(_r23, _k20, _sum2);
-                    _sum2 = _mm256_fmadd_ps(_r24, _k21, _sum2);
-                    _sum2 = _mm256_fmadd_ps(_r25, _k22, _sum2);
+                    _sum2 = _mm256_comp_fmadd_ps(_r03, _k00, _sum2);
+                    _sum2 = _mm256_comp_fmadd_ps(_r04, _k01, _sum2);
+                    _sum2 = _mm256_comp_fmadd_ps(_r05, _k02, _sum2);
+                    _sum2 = _mm256_comp_fmadd_ps(_r13, _k10, _sum2);
+                    _sum2 = _mm256_comp_fmadd_ps(_r14, _k11, _sum2);
+                    _sum2 = _mm256_comp_fmadd_ps(_r15, _k12, _sum2);
+                    _sum2 = _mm256_comp_fmadd_ps(_r23, _k20, _sum2);
+                    _sum2 = _mm256_comp_fmadd_ps(_r24, _k21, _sum2);
+                    _sum2 = _mm256_comp_fmadd_ps(_r25, _k22, _sum2);
 
                     __m256 _sum3 = _mm256_loadu_ps(outptr0 + 24);
                     __m256 _r06 = _mm256_broadcast_ss(r0 + 5);
@@ -437,15 +437,15 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r26 = _mm256_broadcast_ss(r2 + 5);
                     _mm256_storeu_ps(outptr0 + 16, _sum2);
 
-                    _sum3 = _mm256_fmadd_ps(_r04, _k00, _sum3);
-                    _sum3 = _mm256_fmadd_ps(_r05, _k01, _sum3);
-                    _sum3 = _mm256_fmadd_ps(_r06, _k02, _sum3);
-                    _sum3 = _mm256_fmadd_ps(_r14, _k10, _sum3);
-                    _sum3 = _mm256_fmadd_ps(_r15, _k11, _sum3);
-                    _sum3 = _mm256_fmadd_ps(_r16, _k12, _sum3);
-                    _sum3 = _mm256_fmadd_ps(_r24, _k20, _sum3);
-                    _sum3 = _mm256_fmadd_ps(_r25, _k21, _sum3);
-                    _sum3 = _mm256_fmadd_ps(_r26, _k22, _sum3);
+                    _sum3 = _mm256_comp_fmadd_ps(_r04, _k00, _sum3);
+                    _sum3 = _mm256_comp_fmadd_ps(_r05, _k01, _sum3);
+                    _sum3 = _mm256_comp_fmadd_ps(_r06, _k02, _sum3);
+                    _sum3 = _mm256_comp_fmadd_ps(_r14, _k10, _sum3);
+                    _sum3 = _mm256_comp_fmadd_ps(_r15, _k11, _sum3);
+                    _sum3 = _mm256_comp_fmadd_ps(_r16, _k12, _sum3);
+                    _sum3 = _mm256_comp_fmadd_ps(_r24, _k20, _sum3);
+                    _sum3 = _mm256_comp_fmadd_ps(_r25, _k21, _sum3);
+                    _sum3 = _mm256_comp_fmadd_ps(_r26, _k22, _sum3);
 
                     _mm256_storeu_ps(outptr0 + 24, _sum3);
 
@@ -468,15 +468,15 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum0 = _mm256_fmadd_ps(_r01, _k00, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r02, _k01, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r03, _k02, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r11, _k10, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r12, _k11, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r13, _k12, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r21, _k20, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r22, _k21, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r23, _k22, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r01, _k00, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r02, _k01, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r03, _k02, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r11, _k10, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r12, _k11, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r13, _k12, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r21, _k20, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r22, _k21, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r23, _k22, _sum0);
 
                     __m256 _sum1 = _mm256_loadu_ps(outptr0 + 8);
                     __m256 _r04 = _mm256_broadcast_ss(r0 + 3);
@@ -484,15 +484,15 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r24 = _mm256_broadcast_ss(r2 + 3);
                     _mm256_storeu_ps(outptr0, _sum0);
 
-                    _sum1 = _mm256_fmadd_ps(_r02, _k00, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r03, _k01, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r04, _k02, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r12, _k10, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r13, _k11, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r14, _k12, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r22, _k20, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r23, _k21, _sum1);
-                    _sum1 = _mm256_fmadd_ps(_r24, _k22, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r02, _k00, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r03, _k01, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r04, _k02, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r12, _k10, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r13, _k11, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r14, _k12, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r22, _k20, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r23, _k21, _sum1);
+                    _sum1 = _mm256_comp_fmadd_ps(_r24, _k22, _sum1);
 
                     _mm256_storeu_ps(outptr0 + 8, _sum1);
 
@@ -515,15 +515,15 @@ static void conv3x3s1_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum0 = _mm256_fmadd_ps(_r01, _k00, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r02, _k01, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r03, _k02, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r11, _k10, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r12, _k11, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r13, _k12, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r21, _k20, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r22, _k21, _sum0);
-                    _sum0 = _mm256_fmadd_ps(_r23, _k22, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r01, _k00, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r02, _k01, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r03, _k02, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r11, _k10, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r12, _k11, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r13, _k12, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r21, _k20, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r22, _k21, _sum0);
+                    _sum0 = _mm256_comp_fmadd_ps(_r23, _k22, _sum0);
 
                     _mm256_storeu_ps(outptr0, _sum0);
                     r0 += 1;
@@ -557,7 +557,7 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
     int nn_outch = outch >> 1;
     int remain_outch_start = nn_outch << 1;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int pp = 0; pp < nn_outch; pp++)
     {
         int p = pp * 2;
@@ -624,25 +624,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
-
-                    _sum10 = _mm256_fmadd_ps(_r01, _k00_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r02, _k01_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r03, _k02_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r11, _k10_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r12, _k11_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r13, _k12_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r21, _k20_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r22, _k21_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r23, _k22_1, _sum10);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
+
+                    _sum10 = _mm256_comp_fmadd_ps(_r01, _k00_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r02, _k01_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r03, _k02_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r11, _k10_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r12, _k11_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r13, _k12_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r21, _k20_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r22, _k21_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r23, _k22_1, _sum10);
 
                     _mm256_storeu_ps(outptr0, _sum00);
                     _mm256_storeu_ps(outptr1, _sum10);
@@ -657,25 +657,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r15 = _mm256_broadcast_ss(r1 + 4);
                     __m256 _r25 = _mm256_broadcast_ss(r2 + 4);
 
-                    _sum01 = _mm256_fmadd_ps(_r03, _k00_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r04, _k01_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r05, _k02_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r13, _k10_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r14, _k11_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r15, _k12_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r23, _k20_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r24, _k21_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r25, _k22_0, _sum01);
-
-                    _sum11 = _mm256_fmadd_ps(_r03, _k00_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r04, _k01_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r05, _k02_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r13, _k10_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r14, _k11_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r15, _k12_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r23, _k20_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r24, _k21_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r25, _k22_1, _sum11);
+                    _sum01 = _mm256_comp_fmadd_ps(_r03, _k00_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r04, _k01_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r05, _k02_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r13, _k10_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r14, _k11_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r15, _k12_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r23, _k20_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r24, _k21_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r25, _k22_0, _sum01);
+
+                    _sum11 = _mm256_comp_fmadd_ps(_r03, _k00_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r04, _k01_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r05, _k02_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r13, _k10_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r14, _k11_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r15, _k12_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r23, _k20_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r24, _k21_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r25, _k22_1, _sum11);
 
                     _mm256_storeu_ps(outptr0 + 8, _sum01);
                     _mm256_storeu_ps(outptr1 + 8, _sum11);
@@ -690,25 +690,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r17 = _mm256_broadcast_ss(r1 + 6);
                     __m256 _r27 = _mm256_broadcast_ss(r2 + 6);
 
-                    _sum02 = _mm256_fmadd_ps(_r05, _k00_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r06, _k01_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r07, _k02_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r15, _k10_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r16, _k11_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r17, _k12_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r25, _k20_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r26, _k21_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r27, _k22_0, _sum02);
-
-                    _sum12 = _mm256_fmadd_ps(_r05, _k00_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r06, _k01_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r07, _k02_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r15, _k10_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r16, _k11_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r17, _k12_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r25, _k20_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r26, _k21_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r27, _k22_1, _sum12);
+                    _sum02 = _mm256_comp_fmadd_ps(_r05, _k00_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r06, _k01_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r07, _k02_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r15, _k10_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r16, _k11_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r17, _k12_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r25, _k20_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r26, _k21_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r27, _k22_0, _sum02);
+
+                    _sum12 = _mm256_comp_fmadd_ps(_r05, _k00_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r06, _k01_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r07, _k02_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r15, _k10_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r16, _k11_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r17, _k12_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r25, _k20_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r26, _k21_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r27, _k22_1, _sum12);
 
                     _mm256_storeu_ps(outptr0 + 16, _sum02);
                     _mm256_storeu_ps(outptr1 + 16, _sum12);
@@ -723,25 +723,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _sum03 = _mm256_loadu_ps(outptr0 + 24);
                     __m256 _sum13 = _mm256_loadu_ps(outptr1 + 24);
 
-                    _sum03 = _mm256_fmadd_ps(_r07, _k00_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r08, _k01_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r09, _k02_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r17, _k10_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r18, _k11_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r19, _k12_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r27, _k20_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r28, _k21_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r29, _k22_0, _sum03);
-
-                    _sum13 = _mm256_fmadd_ps(_r07, _k00_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r08, _k01_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r09, _k02_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r17, _k10_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r18, _k11_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r19, _k12_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r27, _k20_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r28, _k21_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r29, _k22_1, _sum13);
+                    _sum03 = _mm256_comp_fmadd_ps(_r07, _k00_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r08, _k01_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r09, _k02_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r17, _k10_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r18, _k11_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r19, _k12_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r27, _k20_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r28, _k21_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r29, _k22_0, _sum03);
+
+                    _sum13 = _mm256_comp_fmadd_ps(_r07, _k00_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r08, _k01_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r09, _k02_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r17, _k10_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r18, _k11_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r19, _k12_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r27, _k20_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r28, _k21_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r29, _k22_1, _sum13);
 
                     _mm256_storeu_ps(outptr0 + 24, _sum03);
                     _mm256_storeu_ps(outptr1 + 24, _sum13);
@@ -756,25 +756,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _sum04 = _mm256_loadu_ps(outptr0 + 32);
                     __m256 _sum14 = _mm256_loadu_ps(outptr1 + 32);
 
-                    _sum04 = _mm256_fmadd_ps(_r09, _k00_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r010, _k01_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r011, _k02_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r19, _k10_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r110, _k11_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r111, _k12_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r29, _k20_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r210, _k21_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r211, _k22_0, _sum04);
-
-                    _sum14 = _mm256_fmadd_ps(_r09, _k00_1, _sum14);
-                    _sum14 = _mm256_fmadd_ps(_r010, _k01_1, _sum14);
-                    _sum14 = _mm256_fmadd_ps(_r011, _k02_1, _sum14);
-                    _sum14 = _mm256_fmadd_ps(_r19, _k10_1, _sum14);
-                    _sum14 = _mm256_fmadd_ps(_r110, _k11_1, _sum14);
-                    _sum14 = _mm256_fmadd_ps(_r111, _k12_1, _sum14);
-                    _sum14 = _mm256_fmadd_ps(_r29, _k20_1, _sum14);
-                    _sum14 = _mm256_fmadd_ps(_r210, _k21_1, _sum14);
-                    _sum14 = _mm256_fmadd_ps(_r211, _k22_1, _sum14);
+                    _sum04 = _mm256_comp_fmadd_ps(_r09, _k00_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r010, _k01_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r011, _k02_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r19, _k10_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r110, _k11_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r111, _k12_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r29, _k20_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r210, _k21_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r211, _k22_0, _sum04);
+
+                    _sum14 = _mm256_comp_fmadd_ps(_r09, _k00_1, _sum14);
+                    _sum14 = _mm256_comp_fmadd_ps(_r010, _k01_1, _sum14);
+                    _sum14 = _mm256_comp_fmadd_ps(_r011, _k02_1, _sum14);
+                    _sum14 = _mm256_comp_fmadd_ps(_r19, _k10_1, _sum14);
+                    _sum14 = _mm256_comp_fmadd_ps(_r110, _k11_1, _sum14);
+                    _sum14 = _mm256_comp_fmadd_ps(_r111, _k12_1, _sum14);
+                    _sum14 = _mm256_comp_fmadd_ps(_r29, _k20_1, _sum14);
+                    _sum14 = _mm256_comp_fmadd_ps(_r210, _k21_1, _sum14);
+                    _sum14 = _mm256_comp_fmadd_ps(_r211, _k22_1, _sum14);
 
                     _mm256_storeu_ps(outptr0 + 32, _sum04);
                     _mm256_storeu_ps(outptr1 + 32, _sum14);
@@ -789,24 +789,24 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _sum05 = _mm256_loadu_ps(outptr0 + 40);
                     __m256 _sum15 = _mm256_loadu_ps(outptr1 + 40);
 
-                    _sum05 = _mm256_fmadd_ps(_r011, _k00_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r012, _k01_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r013, _k02_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r111, _k10_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r112, _k11_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r113, _k12_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r211, _k20_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r212, _k21_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r213, _k22_0, _sum05);
-                    _sum15 = _mm256_fmadd_ps(_r011, _k00_1, _sum15);
-                    _sum15 = _mm256_fmadd_ps(_r012, _k01_1, _sum15);
-                    _sum15 = _mm256_fmadd_ps(_r013, _k02_1, _sum15);
-                    _sum15 = _mm256_fmadd_ps(_r111, _k10_1, _sum15);
-                    _sum15 = _mm256_fmadd_ps(_r112, _k11_1, _sum15);
-                    _sum15 = _mm256_fmadd_ps(_r113, _k12_1, _sum15);
-                    _sum15 = _mm256_fmadd_ps(_r211, _k20_1, _sum15);
-                    _sum15 = _mm256_fmadd_ps(_r212, _k21_1, _sum15);
-                    _sum15 = _mm256_fmadd_ps(_r213, _k22_1, _sum15);
+                    _sum05 = _mm256_comp_fmadd_ps(_r011, _k00_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r012, _k01_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r013, _k02_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r111, _k10_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r112, _k11_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r113, _k12_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r211, _k20_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r212, _k21_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r213, _k22_0, _sum05);
+                    _sum15 = _mm256_comp_fmadd_ps(_r011, _k00_1, _sum15);
+                    _sum15 = _mm256_comp_fmadd_ps(_r012, _k01_1, _sum15);
+                    _sum15 = _mm256_comp_fmadd_ps(_r013, _k02_1, _sum15);
+                    _sum15 = _mm256_comp_fmadd_ps(_r111, _k10_1, _sum15);
+                    _sum15 = _mm256_comp_fmadd_ps(_r112, _k11_1, _sum15);
+                    _sum15 = _mm256_comp_fmadd_ps(_r113, _k12_1, _sum15);
+                    _sum15 = _mm256_comp_fmadd_ps(_r211, _k20_1, _sum15);
+                    _sum15 = _mm256_comp_fmadd_ps(_r212, _k21_1, _sum15);
+                    _sum15 = _mm256_comp_fmadd_ps(_r213, _k22_1, _sum15);
 
                     _mm256_storeu_ps(outptr0 + 40, _sum05);
                     _mm256_storeu_ps(outptr1 + 40, _sum15);
@@ -821,24 +821,24 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _sum06 = _mm256_loadu_ps(outptr0 + 48);
                     __m256 _sum16 = _mm256_loadu_ps(outptr1 + 48);
 
-                    _sum06 = _mm256_fmadd_ps(_r013, _k00_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r014, _k01_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r015, _k02_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r113, _k10_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r114, _k11_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r115, _k12_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r213, _k20_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r214, _k21_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r215, _k22_0, _sum06);
-                    _sum16 = _mm256_fmadd_ps(_r013, _k00_1, _sum16);
-                    _sum16 = _mm256_fmadd_ps(_r014, _k01_1, _sum16);
-                    _sum16 = _mm256_fmadd_ps(_r015, _k02_1, _sum16);
-                    _sum16 = _mm256_fmadd_ps(_r113, _k10_1, _sum16);
-                    _sum16 = _mm256_fmadd_ps(_r114, _k11_1, _sum16);
-                    _sum16 = _mm256_fmadd_ps(_r115, _k12_1, _sum16);
-                    _sum16 = _mm256_fmadd_ps(_r213, _k20_1, _sum16);
-                    _sum16 = _mm256_fmadd_ps(_r214, _k21_1, _sum16);
-                    _sum16 = _mm256_fmadd_ps(_r215, _k22_1, _sum16);
+                    _sum06 = _mm256_comp_fmadd_ps(_r013, _k00_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r014, _k01_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r015, _k02_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r113, _k10_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r114, _k11_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r115, _k12_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r213, _k20_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r214, _k21_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r215, _k22_0, _sum06);
+                    _sum16 = _mm256_comp_fmadd_ps(_r013, _k00_1, _sum16);
+                    _sum16 = _mm256_comp_fmadd_ps(_r014, _k01_1, _sum16);
+                    _sum16 = _mm256_comp_fmadd_ps(_r015, _k02_1, _sum16);
+                    _sum16 = _mm256_comp_fmadd_ps(_r113, _k10_1, _sum16);
+                    _sum16 = _mm256_comp_fmadd_ps(_r114, _k11_1, _sum16);
+                    _sum16 = _mm256_comp_fmadd_ps(_r115, _k12_1, _sum16);
+                    _sum16 = _mm256_comp_fmadd_ps(_r213, _k20_1, _sum16);
+                    _sum16 = _mm256_comp_fmadd_ps(_r214, _k21_1, _sum16);
+                    _sum16 = _mm256_comp_fmadd_ps(_r215, _k22_1, _sum16);
 
                     _mm256_storeu_ps(outptr0 + 48, _sum06);
                     _mm256_storeu_ps(outptr1 + 48, _sum16);
@@ -853,24 +853,24 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _sum07 = _mm256_loadu_ps(outptr0 + 56);
                     __m256 _sum17 = _mm256_loadu_ps(outptr1 + 56);
 
-                    _sum07 = _mm256_fmadd_ps(_r015, _k00_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r016, _k01_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r017, _k02_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r115, _k10_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r116, _k11_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r117, _k12_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r215, _k20_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r216, _k21_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r217, _k22_0, _sum07);
-                    _sum17 = _mm256_fmadd_ps(_r015, _k00_1, _sum17);
-                    _sum17 = _mm256_fmadd_ps(_r016, _k01_1, _sum17);
-                    _sum17 = _mm256_fmadd_ps(_r017, _k02_1, _sum17);
-                    _sum17 = _mm256_fmadd_ps(_r115, _k10_1, _sum17);
-                    _sum17 = _mm256_fmadd_ps(_r116, _k11_1, _sum17);
-                    _sum17 = _mm256_fmadd_ps(_r117, _k12_1, _sum17);
-                    _sum17 = _mm256_fmadd_ps(_r215, _k20_1, _sum17);
-                    _sum17 = _mm256_fmadd_ps(_r216, _k21_1, _sum17);
-                    _sum17 = _mm256_fmadd_ps(_r217, _k22_1, _sum17);
+                    _sum07 = _mm256_comp_fmadd_ps(_r015, _k00_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r016, _k01_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r017, _k02_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r115, _k10_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r116, _k11_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r117, _k12_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r215, _k20_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r216, _k21_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r217, _k22_0, _sum07);
+                    _sum17 = _mm256_comp_fmadd_ps(_r015, _k00_1, _sum17);
+                    _sum17 = _mm256_comp_fmadd_ps(_r016, _k01_1, _sum17);
+                    _sum17 = _mm256_comp_fmadd_ps(_r017, _k02_1, _sum17);
+                    _sum17 = _mm256_comp_fmadd_ps(_r115, _k10_1, _sum17);
+                    _sum17 = _mm256_comp_fmadd_ps(_r116, _k11_1, _sum17);
+                    _sum17 = _mm256_comp_fmadd_ps(_r117, _k12_1, _sum17);
+                    _sum17 = _mm256_comp_fmadd_ps(_r215, _k20_1, _sum17);
+                    _sum17 = _mm256_comp_fmadd_ps(_r216, _k21_1, _sum17);
+                    _sum17 = _mm256_comp_fmadd_ps(_r217, _k22_1, _sum17);
 
                     _mm256_storeu_ps(outptr0 + 56, _sum07);
                     _mm256_storeu_ps(outptr1 + 56, _sum17);
@@ -897,25 +897,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
-
-                    _sum10 = _mm256_fmadd_ps(_r01, _k00_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r02, _k01_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r03, _k02_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r11, _k10_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r12, _k11_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r13, _k12_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r21, _k20_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r22, _k21_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r23, _k22_1, _sum10);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
+
+                    _sum10 = _mm256_comp_fmadd_ps(_r01, _k00_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r02, _k01_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r03, _k02_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r11, _k10_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r12, _k11_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r13, _k12_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r21, _k20_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r22, _k21_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r23, _k22_1, _sum10);
 
                     _mm256_storeu_ps(outptr0, _sum00);
                     _mm256_storeu_ps(outptr1, _sum10);
@@ -930,25 +930,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r15 = _mm256_broadcast_ss(r1 + 4);
                     __m256 _r25 = _mm256_broadcast_ss(r2 + 4);
 
-                    _sum01 = _mm256_fmadd_ps(_r03, _k00_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r04, _k01_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r05, _k02_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r13, _k10_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r14, _k11_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r15, _k12_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r23, _k20_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r24, _k21_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r25, _k22_0, _sum01);
-
-                    _sum11 = _mm256_fmadd_ps(_r03, _k00_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r04, _k01_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r05, _k02_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r13, _k10_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r14, _k11_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r15, _k12_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r23, _k20_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r24, _k21_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r25, _k22_1, _sum11);
+                    _sum01 = _mm256_comp_fmadd_ps(_r03, _k00_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r04, _k01_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r05, _k02_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r13, _k10_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r14, _k11_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r15, _k12_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r23, _k20_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r24, _k21_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r25, _k22_0, _sum01);
+
+                    _sum11 = _mm256_comp_fmadd_ps(_r03, _k00_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r04, _k01_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r05, _k02_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r13, _k10_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r14, _k11_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r15, _k12_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r23, _k20_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r24, _k21_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r25, _k22_1, _sum11);
 
                     _mm256_storeu_ps(outptr0 + 8, _sum01);
                     _mm256_storeu_ps(outptr1 + 8, _sum11);
@@ -963,25 +963,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r17 = _mm256_broadcast_ss(r1 + 6);
                     __m256 _r27 = _mm256_broadcast_ss(r2 + 6);
 
-                    _sum02 = _mm256_fmadd_ps(_r05, _k00_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r06, _k01_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r07, _k02_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r15, _k10_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r16, _k11_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r17, _k12_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r25, _k20_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r26, _k21_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r27, _k22_0, _sum02);
-
-                    _sum12 = _mm256_fmadd_ps(_r05, _k00_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r06, _k01_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r07, _k02_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r15, _k10_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r16, _k11_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r17, _k12_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r25, _k20_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r26, _k21_1, _sum12);
-                    _sum12 = _mm256_fmadd_ps(_r27, _k22_1, _sum12);
+                    _sum02 = _mm256_comp_fmadd_ps(_r05, _k00_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r06, _k01_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r07, _k02_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r15, _k10_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r16, _k11_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r17, _k12_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r25, _k20_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r26, _k21_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r27, _k22_0, _sum02);
+
+                    _sum12 = _mm256_comp_fmadd_ps(_r05, _k00_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r06, _k01_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r07, _k02_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r15, _k10_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r16, _k11_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r17, _k12_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r25, _k20_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r26, _k21_1, _sum12);
+                    _sum12 = _mm256_comp_fmadd_ps(_r27, _k22_1, _sum12);
 
                     _mm256_storeu_ps(outptr0 + 16, _sum02);
                     _mm256_storeu_ps(outptr1 + 16, _sum12);
@@ -996,25 +996,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _sum03 = _mm256_loadu_ps(outptr0 + 24);
                     __m256 _sum13 = _mm256_loadu_ps(outptr1 + 24);
 
-                    _sum03 = _mm256_fmadd_ps(_r07, _k00_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r08, _k01_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r09, _k02_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r17, _k10_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r18, _k11_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r19, _k12_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r27, _k20_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r28, _k21_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r29, _k22_0, _sum03);
-
-                    _sum13 = _mm256_fmadd_ps(_r07, _k00_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r08, _k01_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r09, _k02_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r17, _k10_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r18, _k11_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r19, _k12_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r27, _k20_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r28, _k21_1, _sum13);
-                    _sum13 = _mm256_fmadd_ps(_r29, _k22_1, _sum13);
+                    _sum03 = _mm256_comp_fmadd_ps(_r07, _k00_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r08, _k01_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r09, _k02_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r17, _k10_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r18, _k11_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r19, _k12_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r27, _k20_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r28, _k21_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r29, _k22_0, _sum03);
+
+                    _sum13 = _mm256_comp_fmadd_ps(_r07, _k00_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r08, _k01_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r09, _k02_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r17, _k10_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r18, _k11_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r19, _k12_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r27, _k20_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r28, _k21_1, _sum13);
+                    _sum13 = _mm256_comp_fmadd_ps(_r29, _k22_1, _sum13);
 
                     _mm256_storeu_ps(outptr0 + 24, _sum03);
                     _mm256_storeu_ps(outptr1 + 24, _sum13);
@@ -1040,25 +1040,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
-
-                    _sum10 = _mm256_fmadd_ps(_r01, _k00_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r02, _k01_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r03, _k02_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r11, _k10_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r12, _k11_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r13, _k12_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r21, _k20_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r22, _k21_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r23, _k22_1, _sum10);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
+
+                    _sum10 = _mm256_comp_fmadd_ps(_r01, _k00_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r02, _k01_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r03, _k02_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r11, _k10_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r12, _k11_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r13, _k12_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r21, _k20_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r22, _k21_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r23, _k22_1, _sum10);
 
                     _mm256_storeu_ps(outptr0, _sum00);
                     _mm256_storeu_ps(outptr1, _sum10);
@@ -1073,25 +1073,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r15 = _mm256_broadcast_ss(r1 + 4);
                     __m256 _r25 = _mm256_broadcast_ss(r2 + 4);
 
-                    _sum01 = _mm256_fmadd_ps(_r03, _k00_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r04, _k01_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r05, _k02_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r13, _k10_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r14, _k11_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r15, _k12_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r23, _k20_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r24, _k21_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r25, _k22_0, _sum01);
-
-                    _sum11 = _mm256_fmadd_ps(_r03, _k00_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r04, _k01_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r05, _k02_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r13, _k10_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r14, _k11_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r15, _k12_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r23, _k20_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r24, _k21_1, _sum11);
-                    _sum11 = _mm256_fmadd_ps(_r25, _k22_1, _sum11);
+                    _sum01 = _mm256_comp_fmadd_ps(_r03, _k00_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r04, _k01_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r05, _k02_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r13, _k10_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r14, _k11_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r15, _k12_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r23, _k20_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r24, _k21_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r25, _k22_0, _sum01);
+
+                    _sum11 = _mm256_comp_fmadd_ps(_r03, _k00_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r04, _k01_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r05, _k02_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r13, _k10_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r14, _k11_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r15, _k12_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r23, _k20_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r24, _k21_1, _sum11);
+                    _sum11 = _mm256_comp_fmadd_ps(_r25, _k22_1, _sum11);
 
                     _mm256_storeu_ps(outptr0 + 8, _sum01);
                     _mm256_storeu_ps(outptr1 + 8, _sum11);
@@ -1117,25 +1117,25 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
-
-                    _sum10 = _mm256_fmadd_ps(_r01, _k00_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r02, _k01_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r03, _k02_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r11, _k10_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r12, _k11_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r13, _k12_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r21, _k20_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r22, _k21_1, _sum10);
-                    _sum10 = _mm256_fmadd_ps(_r23, _k22_1, _sum10);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
+
+                    _sum10 = _mm256_comp_fmadd_ps(_r01, _k00_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r02, _k01_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r03, _k02_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r11, _k10_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r12, _k11_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r13, _k12_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r21, _k20_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r22, _k21_1, _sum10);
+                    _sum10 = _mm256_comp_fmadd_ps(_r23, _k22_1, _sum10);
 
                     _mm256_storeu_ps(outptr0, _sum00);
                     _mm256_storeu_ps(outptr1, _sum10);
@@ -1156,7 +1156,7 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
         }
     }
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = remain_outch_start; p < outch; p++)
     {
         Mat out0 = top_blob.channel(p);
@@ -1205,15 +1205,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
 
                     _mm256_storeu_ps(outptr0, _sum00);
 
@@ -1226,15 +1226,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r15 = _mm256_broadcast_ss(r1 + 4);
                     __m256 _r25 = _mm256_broadcast_ss(r2 + 4);
 
-                    _sum01 = _mm256_fmadd_ps(_r03, _k00_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r04, _k01_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r05, _k02_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r13, _k10_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r14, _k11_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r15, _k12_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r23, _k20_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r24, _k21_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r25, _k22_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r03, _k00_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r04, _k01_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r05, _k02_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r13, _k10_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r14, _k11_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r15, _k12_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r23, _k20_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r24, _k21_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r25, _k22_0, _sum01);
 
                     _mm256_storeu_ps(outptr0 + 8, _sum01);
 
@@ -1247,15 +1247,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r17 = _mm256_broadcast_ss(r1 + 6);
                     __m256 _r27 = _mm256_broadcast_ss(r2 + 6);
 
-                    _sum02 = _mm256_fmadd_ps(_r05, _k00_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r06, _k01_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r07, _k02_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r15, _k10_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r16, _k11_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r17, _k12_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r25, _k20_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r26, _k21_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r27, _k22_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r05, _k00_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r06, _k01_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r07, _k02_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r15, _k10_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r16, _k11_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r17, _k12_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r25, _k20_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r26, _k21_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r27, _k22_0, _sum02);
 
                     _mm256_storeu_ps(outptr0 + 16, _sum02);
 
@@ -1268,15 +1268,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
 
                     __m256 _sum03 = _mm256_loadu_ps(outptr0 + 24);
 
-                    _sum03 = _mm256_fmadd_ps(_r07, _k00_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r08, _k01_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r09, _k02_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r17, _k10_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r18, _k11_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r19, _k12_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r27, _k20_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r28, _k21_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r29, _k22_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r07, _k00_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r08, _k01_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r09, _k02_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r17, _k10_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r18, _k11_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r19, _k12_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r27, _k20_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r28, _k21_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r29, _k22_0, _sum03);
 
                     _mm256_storeu_ps(outptr0 + 24, _sum03);
 
@@ -1289,15 +1289,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
 
                     __m256 _sum04 = _mm256_loadu_ps(outptr0 + 32);
 
-                    _sum04 = _mm256_fmadd_ps(_r09, _k00_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r010, _k01_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r011, _k02_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r19, _k10_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r110, _k11_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r111, _k12_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r29, _k20_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r210, _k21_0, _sum04);
-                    _sum04 = _mm256_fmadd_ps(_r211, _k22_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r09, _k00_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r010, _k01_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r011, _k02_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r19, _k10_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r110, _k11_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r111, _k12_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r29, _k20_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r210, _k21_0, _sum04);
+                    _sum04 = _mm256_comp_fmadd_ps(_r211, _k22_0, _sum04);
 
                     _mm256_storeu_ps(outptr0 + 32, _sum04);
 
@@ -1310,15 +1310,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
 
                     __m256 _sum05 = _mm256_loadu_ps(outptr0 + 40);
 
-                    _sum05 = _mm256_fmadd_ps(_r011, _k00_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r012, _k01_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r013, _k02_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r111, _k10_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r112, _k11_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r113, _k12_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r211, _k20_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r212, _k21_0, _sum05);
-                    _sum05 = _mm256_fmadd_ps(_r213, _k22_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r011, _k00_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r012, _k01_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r013, _k02_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r111, _k10_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r112, _k11_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r113, _k12_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r211, _k20_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r212, _k21_0, _sum05);
+                    _sum05 = _mm256_comp_fmadd_ps(_r213, _k22_0, _sum05);
 
                     _mm256_storeu_ps(outptr0 + 40, _sum05);
 
@@ -1331,15 +1331,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
 
                     __m256 _sum06 = _mm256_loadu_ps(outptr0 + 48);
 
-                    _sum06 = _mm256_fmadd_ps(_r013, _k00_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r014, _k01_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r015, _k02_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r113, _k10_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r114, _k11_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r115, _k12_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r213, _k20_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r214, _k21_0, _sum06);
-                    _sum06 = _mm256_fmadd_ps(_r215, _k22_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r013, _k00_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r014, _k01_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r015, _k02_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r113, _k10_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r114, _k11_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r115, _k12_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r213, _k20_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r214, _k21_0, _sum06);
+                    _sum06 = _mm256_comp_fmadd_ps(_r215, _k22_0, _sum06);
 
                     _mm256_storeu_ps(outptr0 + 48, _sum06);
 
@@ -1352,15 +1352,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
 
                     __m256 _sum07 = _mm256_loadu_ps(outptr0 + 56);
 
-                    _sum07 = _mm256_fmadd_ps(_r015, _k00_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r016, _k01_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r017, _k02_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r115, _k10_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r116, _k11_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r117, _k12_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r215, _k20_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r216, _k21_0, _sum07);
-                    _sum07 = _mm256_fmadd_ps(_r217, _k22_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r015, _k00_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r016, _k01_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r017, _k02_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r115, _k10_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r116, _k11_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r117, _k12_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r215, _k20_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r216, _k21_0, _sum07);
+                    _sum07 = _mm256_comp_fmadd_ps(_r217, _k22_0, _sum07);
 
                     _mm256_storeu_ps(outptr0 + 56, _sum07);
 
@@ -1383,15 +1383,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
 
                     _mm256_storeu_ps(outptr0, _sum00);
 
@@ -1404,15 +1404,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r15 = _mm256_broadcast_ss(r1 + 4);
                     __m256 _r25 = _mm256_broadcast_ss(r2 + 4);
 
-                    _sum01 = _mm256_fmadd_ps(_r03, _k00_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r04, _k01_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r05, _k02_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r13, _k10_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r14, _k11_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r15, _k12_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r23, _k20_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r24, _k21_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r25, _k22_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r03, _k00_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r04, _k01_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r05, _k02_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r13, _k10_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r14, _k11_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r15, _k12_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r23, _k20_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r24, _k21_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r25, _k22_0, _sum01);
 
                     _mm256_storeu_ps(outptr0 + 8, _sum01);
 
@@ -1425,15 +1425,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r17 = _mm256_broadcast_ss(r1 + 6);
                     __m256 _r27 = _mm256_broadcast_ss(r2 + 6);
 
-                    _sum02 = _mm256_fmadd_ps(_r05, _k00_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r06, _k01_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r07, _k02_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r15, _k10_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r16, _k11_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r17, _k12_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r25, _k20_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r26, _k21_0, _sum02);
-                    _sum02 = _mm256_fmadd_ps(_r27, _k22_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r05, _k00_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r06, _k01_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r07, _k02_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r15, _k10_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r16, _k11_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r17, _k12_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r25, _k20_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r26, _k21_0, _sum02);
+                    _sum02 = _mm256_comp_fmadd_ps(_r27, _k22_0, _sum02);
 
                     _mm256_storeu_ps(outptr0 + 16, _sum02);
 
@@ -1446,15 +1446,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
 
                     __m256 _sum03 = _mm256_loadu_ps(outptr0 + 24);
 
-                    _sum03 = _mm256_fmadd_ps(_r07, _k00_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r08, _k01_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r09, _k02_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r17, _k10_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r18, _k11_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r19, _k12_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r27, _k20_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r28, _k21_0, _sum03);
-                    _sum03 = _mm256_fmadd_ps(_r29, _k22_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r07, _k00_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r08, _k01_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r09, _k02_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r17, _k10_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r18, _k11_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r19, _k12_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r27, _k20_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r28, _k21_0, _sum03);
+                    _sum03 = _mm256_comp_fmadd_ps(_r29, _k22_0, _sum03);
 
                     _mm256_storeu_ps(outptr0 + 24, _sum03);
                     r0 += 8;
@@ -1477,15 +1477,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
 
                     _mm256_storeu_ps(outptr0, _sum00);
 
@@ -1498,15 +1498,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r15 = _mm256_broadcast_ss(r1 + 4);
                     __m256 _r25 = _mm256_broadcast_ss(r2 + 4);
 
-                    _sum01 = _mm256_fmadd_ps(_r03, _k00_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r04, _k01_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r05, _k02_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r13, _k10_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r14, _k11_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r15, _k12_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r23, _k20_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r24, _k21_0, _sum01);
-                    _sum01 = _mm256_fmadd_ps(_r25, _k22_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r03, _k00_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r04, _k01_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r05, _k02_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r13, _k10_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r14, _k11_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r15, _k12_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r23, _k20_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r24, _k21_0, _sum01);
+                    _sum01 = _mm256_comp_fmadd_ps(_r25, _k22_0, _sum01);
 
                     _mm256_storeu_ps(outptr0 + 8, _sum01);
 
@@ -1528,15 +1528,15 @@ static void conv3x3s2_pack1to8_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r22 = _mm256_broadcast_ss(r2 + 1);
                     __m256 _r23 = _mm256_broadcast_ss(r2 + 2);
 
-                    _sum00 = _mm256_fmadd_ps(_r01, _k00_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r02, _k01_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r03, _k02_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r11, _k10_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r12, _k11_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r13, _k12_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r21, _k20_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r22, _k21_0, _sum00);
-                    _sum00 = _mm256_fmadd_ps(_r23, _k22_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r01, _k00_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r02, _k01_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r03, _k02_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r11, _k10_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r12, _k11_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r13, _k12_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r21, _k20_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r22, _k21_0, _sum00);
+                    _sum00 = _mm256_comp_fmadd_ps(_r23, _k22_0, _sum00);
                     _mm256_storeu_ps(outptr0, _sum00);
 
                     r0 += 2;
diff --git a/src/layer/x86/convolution_3x3_pack8.h b/src/layer/x86/convolution_3x3_pack8.h
index 2cd5afe9..bccab8f6 100644
--- a/src/layer/x86/convolution_3x3_pack8.h
+++ b/src/layer/x86/convolution_3x3_pack8.h
@@ -22,7 +22,7 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
     const float* bias = _bias;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = 0; p < outch; p++)
     {
         Mat out = top_blob.channel(p);
@@ -73,14 +73,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum00 = _mm256_fmadd_ps(_r000, _k00, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r001, _k01, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r002, _k02, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r003, _k03, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r004, _k04, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r005, _k05, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r006, _k06, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r007, _k07, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r000, _k00, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r001, _k01, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r002, _k02, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r003, _k03, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r004, _k04, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r005, _k05, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r006, _k06, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r007, _k07, _sum01);
 
                     __m256 _r010 = _mm256_broadcast_ss(r0 + 8);
                     __m256 _r011 = _mm256_broadcast_ss(r0 + 9);
@@ -91,14 +91,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _r016 = _mm256_broadcast_ss(r0 + 14);
                     __m256 _r017 = _mm256_broadcast_ss(r0 + 15);
 
-                    _sum10 = _mm256_fmadd_ps(_r010, _k00, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r011, _k01, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r012, _k02, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r013, _k03, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r014, _k04, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r015, _k05, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r016, _k06, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r017, _k07, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r010, _k00, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r011, _k01, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r012, _k02, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r013, _k03, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r014, _k04, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r015, _k05, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r016, _k06, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r017, _k07, _sum11);
 
                     __m256 _k10 = _mm256_loadu_ps(kptr);
                     __m256 _k11 = _mm256_loadu_ps(kptr + 8);
@@ -111,14 +111,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum00 = _mm256_fmadd_ps(_r010, _k10, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r011, _k11, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r012, _k12, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r013, _k13, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r014, _k14, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r015, _k15, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r016, _k16, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r017, _k17, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r010, _k10, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r011, _k11, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r012, _k12, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r013, _k13, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r014, _k14, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r015, _k15, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r016, _k16, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r017, _k17, _sum01);
 
                     __m256 _r020 = _mm256_broadcast_ss(r0 + 16);
                     __m256 _r021 = _mm256_broadcast_ss(r0 + 17);
@@ -129,14 +129,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _r026 = _mm256_broadcast_ss(r0 + 22);
                     __m256 _r027 = _mm256_broadcast_ss(r0 + 23);
 
-                    _sum10 = _mm256_fmadd_ps(_r020, _k10, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r021, _k11, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r022, _k12, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r023, _k13, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r024, _k14, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r025, _k15, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r026, _k16, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r027, _k17, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r020, _k10, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r021, _k11, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r022, _k12, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r023, _k13, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r024, _k14, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r025, _k15, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r026, _k16, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r027, _k17, _sum11);
 
                     __m256 _k20 = _mm256_loadu_ps(kptr);
                     __m256 _k21 = _mm256_loadu_ps(kptr + 8);
@@ -149,14 +149,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum00 = _mm256_fmadd_ps(_r020, _k20, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r021, _k21, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r022, _k22, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r023, _k23, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r024, _k24, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r025, _k25, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r026, _k26, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r027, _k27, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r020, _k20, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r021, _k21, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r022, _k22, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r023, _k23, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r024, _k24, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r025, _k25, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r026, _k26, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r027, _k27, _sum01);
 
                     __m256 _r030 = _mm256_broadcast_ss(r0 + 24);
                     __m256 _r031 = _mm256_broadcast_ss(r0 + 25);
@@ -167,14 +167,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _r036 = _mm256_broadcast_ss(r0 + 30);
                     __m256 _r037 = _mm256_broadcast_ss(r0 + 31);
 
-                    _sum10 = _mm256_fmadd_ps(_r030, _k20, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r031, _k21, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r032, _k22, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r033, _k23, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r034, _k24, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r035, _k25, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r036, _k26, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r037, _k27, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r030, _k20, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r031, _k21, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r032, _k22, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r033, _k23, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r034, _k24, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r035, _k25, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r036, _k26, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r037, _k27, _sum11);
 
                     __m256 _r100 = _mm256_broadcast_ss(r1 + 0);
                     __m256 _r101 = _mm256_broadcast_ss(r1 + 1);
@@ -196,14 +196,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum00 = _mm256_fmadd_ps(_r100, _k30, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r101, _k31, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r102, _k32, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r103, _k33, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r104, _k34, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r105, _k35, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r106, _k36, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r107, _k37, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r100, _k30, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r101, _k31, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r102, _k32, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r103, _k33, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r104, _k34, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r105, _k35, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r106, _k36, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r107, _k37, _sum01);
 
                     __m256 _r110 = _mm256_broadcast_ss(r1 + 8);
                     __m256 _r111 = _mm256_broadcast_ss(r1 + 9);
@@ -214,14 +214,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _r116 = _mm256_broadcast_ss(r1 + 14);
                     __m256 _r117 = _mm256_broadcast_ss(r1 + 15);
 
-                    _sum10 = _mm256_fmadd_ps(_r110, _k30, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r111, _k31, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r112, _k32, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r113, _k33, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r114, _k34, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r115, _k35, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r116, _k36, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r117, _k37, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r110, _k30, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r111, _k31, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r112, _k32, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r113, _k33, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r114, _k34, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r115, _k35, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r116, _k36, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r117, _k37, _sum11);
 
                     __m256 _k40 = _mm256_loadu_ps(kptr);
                     __m256 _k41 = _mm256_loadu_ps(kptr + 8);
@@ -234,14 +234,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum00 = _mm256_fmadd_ps(_r110, _k40, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r111, _k41, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r112, _k42, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r113, _k43, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r114, _k44, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r115, _k45, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r116, _k46, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r117, _k47, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r110, _k40, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r111, _k41, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r112, _k42, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r113, _k43, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r114, _k44, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r115, _k45, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r116, _k46, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r117, _k47, _sum01);
 
                     __m256 _r120 = _mm256_broadcast_ss(r1 + 16);
                     __m256 _r121 = _mm256_broadcast_ss(r1 + 17);
@@ -252,14 +252,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _r126 = _mm256_broadcast_ss(r1 + 22);
                     __m256 _r127 = _mm256_broadcast_ss(r1 + 23);
 
-                    _sum10 = _mm256_fmadd_ps(_r120, _k40, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r121, _k41, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r122, _k42, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r123, _k43, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r124, _k44, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r125, _k45, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r126, _k46, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r127, _k47, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r120, _k40, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r121, _k41, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r122, _k42, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r123, _k43, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r124, _k44, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r125, _k45, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r126, _k46, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r127, _k47, _sum11);
 
                     __m256 _k50 = _mm256_loadu_ps(kptr);
                     __m256 _k51 = _mm256_loadu_ps(kptr + 8);
@@ -272,14 +272,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum00 = _mm256_fmadd_ps(_r120, _k50, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r121, _k51, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r122, _k52, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r123, _k53, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r124, _k54, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r125, _k55, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r126, _k56, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r127, _k57, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r120, _k50, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r121, _k51, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r122, _k52, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r123, _k53, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r124, _k54, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r125, _k55, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r126, _k56, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r127, _k57, _sum01);
 
                     __m256 _r130 = _mm256_broadcast_ss(r1 + 24);
                     __m256 _r131 = _mm256_broadcast_ss(r1 + 25);
@@ -290,14 +290,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _r136 = _mm256_broadcast_ss(r1 + 30);
                     __m256 _r137 = _mm256_broadcast_ss(r1 + 31);
 
-                    _sum10 = _mm256_fmadd_ps(_r130, _k50, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r131, _k51, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r132, _k52, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r133, _k53, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r134, _k54, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r135, _k55, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r136, _k56, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r137, _k57, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r130, _k50, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r131, _k51, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r132, _k52, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r133, _k53, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r134, _k54, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r135, _k55, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r136, _k56, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r137, _k57, _sum11);
 
                     __m256 _r200 = _mm256_broadcast_ss(r2 + 0);
                     __m256 _r201 = _mm256_broadcast_ss(r2 + 1);
@@ -319,14 +319,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum00 = _mm256_fmadd_ps(_r200, _k60, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r201, _k61, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r202, _k62, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r203, _k63, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r204, _k64, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r205, _k65, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r206, _k66, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r207, _k67, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r200, _k60, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r201, _k61, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r202, _k62, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r203, _k63, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r204, _k64, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r205, _k65, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r206, _k66, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r207, _k67, _sum01);
 
                     __m256 _r210 = _mm256_broadcast_ss(r2 + 8);
                     __m256 _r211 = _mm256_broadcast_ss(r2 + 9);
@@ -337,14 +337,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _r216 = _mm256_broadcast_ss(r2 + 14);
                     __m256 _r217 = _mm256_broadcast_ss(r2 + 15);
 
-                    _sum10 = _mm256_fmadd_ps(_r210, _k60, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r211, _k61, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r212, _k62, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r213, _k63, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r214, _k64, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r215, _k65, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r216, _k66, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r217, _k67, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r210, _k60, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r211, _k61, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r212, _k62, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r213, _k63, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r214, _k64, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r215, _k65, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r216, _k66, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r217, _k67, _sum11);
 
                     __m256 _k70 = _mm256_loadu_ps(kptr);
                     __m256 _k71 = _mm256_loadu_ps(kptr + 8);
@@ -357,14 +357,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum00 = _mm256_fmadd_ps(_r210, _k70, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r211, _k71, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r212, _k72, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r213, _k73, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r214, _k74, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r215, _k75, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r216, _k76, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r217, _k77, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r210, _k70, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r211, _k71, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r212, _k72, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r213, _k73, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r214, _k74, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r215, _k75, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r216, _k76, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r217, _k77, _sum01);
 
                     __m256 _r220 = _mm256_broadcast_ss(r2 + 16);
                     __m256 _r221 = _mm256_broadcast_ss(r2 + 17);
@@ -375,14 +375,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _r226 = _mm256_broadcast_ss(r2 + 22);
                     __m256 _r227 = _mm256_broadcast_ss(r2 + 23);
 
-                    _sum10 = _mm256_fmadd_ps(_r220, _k70, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r221, _k71, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r222, _k72, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r223, _k73, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r224, _k74, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r225, _k75, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r226, _k76, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r227, _k77, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r220, _k70, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r221, _k71, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r222, _k72, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r223, _k73, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r224, _k74, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r225, _k75, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r226, _k76, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r227, _k77, _sum11);
 
                     __m256 _k80 = _mm256_loadu_ps(kptr);
                     __m256 _k81 = _mm256_loadu_ps(kptr + 8);
@@ -393,14 +393,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k86 = _mm256_loadu_ps(kptr + 48);
                     __m256 _k87 = _mm256_loadu_ps(kptr + 56);
 
-                    _sum00 = _mm256_fmadd_ps(_r220, _k80, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r221, _k81, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r222, _k82, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r223, _k83, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r224, _k84, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r225, _k85, _sum01);
-                    _sum00 = _mm256_fmadd_ps(_r226, _k86, _sum00);
-                    _sum01 = _mm256_fmadd_ps(_r227, _k87, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r220, _k80, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r221, _k81, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r222, _k82, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r223, _k83, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r224, _k84, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r225, _k85, _sum01);
+                    _sum00 = _mm256_comp_fmadd_ps(_r226, _k86, _sum00);
+                    _sum01 = _mm256_comp_fmadd_ps(_r227, _k87, _sum01);
 
                     __m256 _r230 = _mm256_broadcast_ss(r2 + 24);
                     __m256 _r231 = _mm256_broadcast_ss(r2 + 25);
@@ -411,14 +411,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _r236 = _mm256_broadcast_ss(r2 + 30);
                     __m256 _r237 = _mm256_broadcast_ss(r2 + 31);
 
-                    _sum10 = _mm256_fmadd_ps(_r230, _k80, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r231, _k81, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r232, _k82, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r233, _k83, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r234, _k84, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r235, _k85, _sum11);
-                    _sum10 = _mm256_fmadd_ps(_r236, _k86, _sum10);
-                    _sum11 = _mm256_fmadd_ps(_r237, _k87, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r230, _k80, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r231, _k81, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r232, _k82, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r233, _k83, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r234, _k84, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r235, _k85, _sum11);
+                    _sum10 = _mm256_comp_fmadd_ps(_r236, _k86, _sum10);
+                    _sum11 = _mm256_comp_fmadd_ps(_r237, _k87, _sum11);
 
                     kptr -= 64 * 8;
 
@@ -458,14 +458,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum0 = _mm256_fmadd_ps(_r000, _k00, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r001, _k01, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r002, _k02, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r003, _k03, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r004, _k04, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r005, _k05, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r006, _k06, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r007, _k07, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r000, _k00, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r001, _k01, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r002, _k02, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r003, _k03, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r004, _k04, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r005, _k05, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r006, _k06, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r007, _k07, _sum1);
 
                     __m256 _r010 = _mm256_broadcast_ss(r0 + 8);
                     __m256 _r011 = _mm256_broadcast_ss(r0 + 9);
@@ -487,14 +487,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum0 = _mm256_fmadd_ps(_r010, _k10, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r011, _k11, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r012, _k12, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r013, _k13, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r014, _k14, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r015, _k15, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r016, _k16, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r017, _k17, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r010, _k10, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r011, _k11, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r012, _k12, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r013, _k13, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r014, _k14, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r015, _k15, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r016, _k16, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r017, _k17, _sum1);
 
                     __m256 _r020 = _mm256_broadcast_ss(r0 + 16);
                     __m256 _r021 = _mm256_broadcast_ss(r0 + 17);
@@ -516,14 +516,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum0 = _mm256_fmadd_ps(_r020, _k20, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r021, _k21, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r022, _k22, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r023, _k23, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r024, _k24, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r025, _k25, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r026, _k26, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r027, _k27, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r020, _k20, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r021, _k21, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r022, _k22, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r023, _k23, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r024, _k24, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r025, _k25, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r026, _k26, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r027, _k27, _sum1);
 
                     __m256 _r100 = _mm256_broadcast_ss(r1 + 0);
                     __m256 _r101 = _mm256_broadcast_ss(r1 + 1);
@@ -545,14 +545,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum0 = _mm256_fmadd_ps(_r100, _k30, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r101, _k31, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r102, _k32, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r103, _k33, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r104, _k34, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r105, _k35, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r106, _k36, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r107, _k37, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r100, _k30, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r101, _k31, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r102, _k32, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r103, _k33, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r104, _k34, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r105, _k35, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r106, _k36, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r107, _k37, _sum1);
 
                     __m256 _r110 = _mm256_broadcast_ss(r1 + 8);
                     __m256 _r111 = _mm256_broadcast_ss(r1 + 9);
@@ -574,14 +574,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum0 = _mm256_fmadd_ps(_r110, _k40, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r111, _k41, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r112, _k42, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r113, _k43, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r114, _k44, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r115, _k45, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r116, _k46, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r117, _k47, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r110, _k40, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r111, _k41, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r112, _k42, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r113, _k43, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r114, _k44, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r115, _k45, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r116, _k46, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r117, _k47, _sum1);
 
                     __m256 _r120 = _mm256_broadcast_ss(r1 + 16);
                     __m256 _r121 = _mm256_broadcast_ss(r1 + 17);
@@ -603,14 +603,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum0 = _mm256_fmadd_ps(_r120, _k50, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r121, _k51, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r122, _k52, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r123, _k53, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r124, _k54, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r125, _k55, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r126, _k56, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r127, _k57, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r120, _k50, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r121, _k51, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r122, _k52, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r123, _k53, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r124, _k54, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r125, _k55, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r126, _k56, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r127, _k57, _sum1);
 
                     __m256 _r200 = _mm256_broadcast_ss(r2 + 0);
                     __m256 _r201 = _mm256_broadcast_ss(r2 + 1);
@@ -632,14 +632,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum0 = _mm256_fmadd_ps(_r200, _k60, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r201, _k61, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r202, _k62, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r203, _k63, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r204, _k64, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r205, _k65, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r206, _k66, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r207, _k67, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r200, _k60, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r201, _k61, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r202, _k62, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r203, _k63, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r204, _k64, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r205, _k65, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r206, _k66, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r207, _k67, _sum1);
 
                     __m256 _r210 = _mm256_broadcast_ss(r2 + 8);
                     __m256 _r211 = _mm256_broadcast_ss(r2 + 9);
@@ -661,14 +661,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
 
                     kptr += 64;
 
-                    _sum0 = _mm256_fmadd_ps(_r210, _k70, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r211, _k71, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r212, _k72, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r213, _k73, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r214, _k74, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r215, _k75, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r216, _k76, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r217, _k77, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r210, _k70, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r211, _k71, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r212, _k72, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r213, _k73, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r214, _k74, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r215, _k75, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r216, _k76, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r217, _k77, _sum1);
 
                     __m256 _r220 = _mm256_broadcast_ss(r2 + 16);
                     __m256 _r221 = _mm256_broadcast_ss(r2 + 17);
@@ -688,14 +688,14 @@ static void conv3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const Mat
                     __m256 _k86 = _mm256_loadu_ps(kptr + 48);
                     __m256 _k87 = _mm256_loadu_ps(kptr + 56);
 
-                    _sum0 = _mm256_fmadd_ps(_r220, _k80, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r221, _k81, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r222, _k82, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r223, _k83, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r224, _k84, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r225, _k85, _sum1);
-                    _sum0 = _mm256_fmadd_ps(_r226, _k86, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_r227, _k87, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r220, _k80, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r221, _k81, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r222, _k82, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r223, _k83, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r224, _k84, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r225, _k85, _sum1);
+                    _sum0 = _mm256_comp_fmadd_ps(_r226, _k86, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_r227, _k87, _sum1);
 
                     kptr -= 64 * 8;
 
@@ -731,10 +731,9 @@ static void conv3x3s1_winograd64_transform_kernel_pack8_avx(const Mat& kernel, M
         {1.0f / 90, -1.0f / 45, 2.0f / 45},
         {1.0f / 45, 1.0f / 90, 1.0f / 180},
         {1.0f / 45, -1.0f / 90, 1.0f / 180},
-        {0.0f, 0.0f, 1.0f}
-    };
+        {0.0f, 0.0f, 1.0f}};
 
-    #pragma omp parallel for
+#pragma omp parallel for
     for (int p = 0; p < outch; p++)
     {
         for (int q = 0; q < inch; q++)
@@ -1004,7 +1003,7 @@ static void conv3x3s1_winograd64_pack8_avx(const Mat& bottom_blob, Mat& top_blob
         // 5 = (r06 + (r02 - r04 * 1.25) * 4) + (r01 * 2 - r03 * 2.5 + r05 * 0.5)
         // 6 = (r06 + (r02 - r04 * 1.25) * 4) - (r01 * 2 - r03 * 2.5 + r05 * 0.5)
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < inch; q++)
         {
             const Mat img0 = bottom_blob_bordered.channel(q);
@@ -1190,7 +1189,7 @@ static void conv3x3s1_winograd64_pack8_avx(const Mat& bottom_blob, Mat& top_blob
         else // if (tiles >= 1)
             bottom_blob_tm2.create(1 * inch, tiles, 64, elemsize, elempack, opt.workspace_allocator);
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int r = 0; r < 64; r++)
         {
             Mat tm2 = bottom_blob_tm2.channel(r);
@@ -1329,7 +1328,7 @@ static void conv3x3s1_winograd64_pack8_avx(const Mat& bottom_blob, Mat& top_blob
 
         top_blob_tm.create(tiles, 64, outch, elemsize, elempack, opt.workspace_allocator);
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < outch; p++)
         {
             float* output0_tm = top_blob_tm.channel(p);
@@ -1367,225 +1366,225 @@ static void conv3x3s1_winograd64_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                         __m256 _r01 = _mm256_broadcast_ss(r0 + 8);
                         __m256 _r02 = _mm256_broadcast_ss(r0 + 16);
                         __m256 _r03 = _mm256_broadcast_ss(r0 + 24);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
                         __m256 _r04 = _mm256_broadcast_ss(r0 + 32);
                         __m256 _r05 = _mm256_broadcast_ss(r0 + 40);
                         __m256 _r06 = _mm256_broadcast_ss(r0 + 48);
                         __m256 _r07 = _mm256_broadcast_ss(r0 + 56);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
                         __m256 _r08 = _mm256_broadcast_ss(r0 + 64);
                         __m256 _r09 = _mm256_broadcast_ss(r0 + 72);
                         __m256 _r010 = _mm256_broadcast_ss(r0 + 80);
                         __m256 _r011 = _mm256_broadcast_ss(r0 + 88);
 
-                        _sum8 = _mm256_fmadd_ps(_k01, _r08, _sum8);
-                        _sum9 = _mm256_fmadd_ps(_k01, _r09, _sum9);
-                        _sum10 = _mm256_fmadd_ps(_k01, _r010, _sum10);
-                        _sum11 = _mm256_fmadd_ps(_k01, _r011, _sum11);
+                        _sum8 = _mm256_comp_fmadd_ps(_k01, _r08, _sum8);
+                        _sum9 = _mm256_comp_fmadd_ps(_k01, _r09, _sum9);
+                        _sum10 = _mm256_comp_fmadd_ps(_k01, _r010, _sum10);
+                        _sum11 = _mm256_comp_fmadd_ps(_k01, _r011, _sum11);
 
                         _k01 = _mm256_loadu_ps(k01 + 8);
                         _r00 = _mm256_broadcast_ss(r0 + 1);
                         _r01 = _mm256_broadcast_ss(r0 + 9);
                         _r02 = _mm256_broadcast_ss(r0 + 17);
                         _r03 = _mm256_broadcast_ss(r0 + 25);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
                         _r04 = _mm256_broadcast_ss(r0 + 33);
                         _r05 = _mm256_broadcast_ss(r0 + 41);
                         _r06 = _mm256_broadcast_ss(r0 + 49);
                         _r07 = _mm256_broadcast_ss(r0 + 57);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _r08 = _mm256_broadcast_ss(r0 + 65);
                         _r09 = _mm256_broadcast_ss(r0 + 73);
                         _r010 = _mm256_broadcast_ss(r0 + 81);
                         _r011 = _mm256_broadcast_ss(r0 + 89);
 
-                        _sum8 = _mm256_fmadd_ps(_k01, _r08, _sum8);
-                        _sum9 = _mm256_fmadd_ps(_k01, _r09, _sum9);
-                        _sum10 = _mm256_fmadd_ps(_k01, _r010, _sum10);
-                        _sum11 = _mm256_fmadd_ps(_k01, _r011, _sum11);
+                        _sum8 = _mm256_comp_fmadd_ps(_k01, _r08, _sum8);
+                        _sum9 = _mm256_comp_fmadd_ps(_k01, _r09, _sum9);
+                        _sum10 = _mm256_comp_fmadd_ps(_k01, _r010, _sum10);
+                        _sum11 = _mm256_comp_fmadd_ps(_k01, _r011, _sum11);
 
                         _k01 = _mm256_loadu_ps(k01 + 16);
                         _r00 = _mm256_broadcast_ss(r0 + 2);
                         _r01 = _mm256_broadcast_ss(r0 + 10);
                         _r02 = _mm256_broadcast_ss(r0 + 18);
                         _r03 = _mm256_broadcast_ss(r0 + 26);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
                         _r04 = _mm256_broadcast_ss(r0 + 34);
                         _r05 = _mm256_broadcast_ss(r0 + 42);
                         _r06 = _mm256_broadcast_ss(r0 + 50);
                         _r07 = _mm256_broadcast_ss(r0 + 58);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
                         _r08 = _mm256_broadcast_ss(r0 + 66);
                         _r09 = _mm256_broadcast_ss(r0 + 74);
                         _r010 = _mm256_broadcast_ss(r0 + 82);
                         _r011 = _mm256_broadcast_ss(r0 + 90);
 
-                        _sum8 = _mm256_fmadd_ps(_k01, _r08, _sum8);
-                        _sum9 = _mm256_fmadd_ps(_k01, _r09, _sum9);
-                        _sum10 = _mm256_fmadd_ps(_k01, _r010, _sum10);
-                        _sum11 = _mm256_fmadd_ps(_k01, _r011, _sum11);
+                        _sum8 = _mm256_comp_fmadd_ps(_k01, _r08, _sum8);
+                        _sum9 = _mm256_comp_fmadd_ps(_k01, _r09, _sum9);
+                        _sum10 = _mm256_comp_fmadd_ps(_k01, _r010, _sum10);
+                        _sum11 = _mm256_comp_fmadd_ps(_k01, _r011, _sum11);
 
                         _k01 = _mm256_loadu_ps(k01 + 24);
                         _r00 = _mm256_broadcast_ss(r0 + 3);
                         _r01 = _mm256_broadcast_ss(r0 + 11);
                         _r02 = _mm256_broadcast_ss(r0 + 19);
                         _r03 = _mm256_broadcast_ss(r0 + 27);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 35);
                         _r05 = _mm256_broadcast_ss(r0 + 43);
                         _r06 = _mm256_broadcast_ss(r0 + 51);
                         _r07 = _mm256_broadcast_ss(r0 + 59);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _r08 = _mm256_broadcast_ss(r0 + 67);
                         _r09 = _mm256_broadcast_ss(r0 + 75);
                         _r010 = _mm256_broadcast_ss(r0 + 83);
                         _r011 = _mm256_broadcast_ss(r0 + 91);
 
-                        _sum8 = _mm256_fmadd_ps(_k01, _r08, _sum8);
-                        _sum9 = _mm256_fmadd_ps(_k01, _r09, _sum9);
-                        _sum10 = _mm256_fmadd_ps(_k01, _r010, _sum10);
-                        _sum11 = _mm256_fmadd_ps(_k01, _r011, _sum11);
+                        _sum8 = _mm256_comp_fmadd_ps(_k01, _r08, _sum8);
+                        _sum9 = _mm256_comp_fmadd_ps(_k01, _r09, _sum9);
+                        _sum10 = _mm256_comp_fmadd_ps(_k01, _r010, _sum10);
+                        _sum11 = _mm256_comp_fmadd_ps(_k01, _r011, _sum11);
 
                         _k01 = _mm256_loadu_ps(k01 + 32);
                         _r00 = _mm256_broadcast_ss(r0 + 4);
                         _r01 = _mm256_broadcast_ss(r0 + 12);
                         _r02 = _mm256_broadcast_ss(r0 + 20);
                         _r03 = _mm256_broadcast_ss(r0 + 28);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 36);
                         _r05 = _mm256_broadcast_ss(r0 + 44);
                         _r06 = _mm256_broadcast_ss(r0 + 52);
                         _r07 = _mm256_broadcast_ss(r0 + 60);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _r08 = _mm256_broadcast_ss(r0 + 68);
                         _r09 = _mm256_broadcast_ss(r0 + 76);
                         _r010 = _mm256_broadcast_ss(r0 + 84);
                         _r011 = _mm256_broadcast_ss(r0 + 92);
 
-                        _sum8 = _mm256_fmadd_ps(_k01, _r08, _sum8);
-                        _sum9 = _mm256_fmadd_ps(_k01, _r09, _sum9);
-                        _sum10 = _mm256_fmadd_ps(_k01, _r010, _sum10);
-                        _sum11 = _mm256_fmadd_ps(_k01, _r011, _sum11);
+                        _sum8 = _mm256_comp_fmadd_ps(_k01, _r08, _sum8);
+                        _sum9 = _mm256_comp_fmadd_ps(_k01, _r09, _sum9);
+                        _sum10 = _mm256_comp_fmadd_ps(_k01, _r010, _sum10);
+                        _sum11 = _mm256_comp_fmadd_ps(_k01, _r011, _sum11);
 
                         _k01 = _mm256_loadu_ps(k01 + 40);
                         _r00 = _mm256_broadcast_ss(r0 + 5);
                         _r01 = _mm256_broadcast_ss(r0 + 13);
                         _r02 = _mm256_broadcast_ss(r0 + 21);
                         _r03 = _mm256_broadcast_ss(r0 + 29);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 37);
                         _r05 = _mm256_broadcast_ss(r0 + 45);
                         _r06 = _mm256_broadcast_ss(r0 + 53);
                         _r07 = _mm256_broadcast_ss(r0 + 61);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _r08 = _mm256_broadcast_ss(r0 + 69);
                         _r09 = _mm256_broadcast_ss(r0 + 77);
                         _r010 = _mm256_broadcast_ss(r0 + 85);
                         _r011 = _mm256_broadcast_ss(r0 + 93);
 
-                        _sum8 = _mm256_fmadd_ps(_k01, _r08, _sum8);
-                        _sum9 = _mm256_fmadd_ps(_k01, _r09, _sum9);
-                        _sum10 = _mm256_fmadd_ps(_k01, _r010, _sum10);
-                        _sum11 = _mm256_fmadd_ps(_k01, _r011, _sum11);
+                        _sum8 = _mm256_comp_fmadd_ps(_k01, _r08, _sum8);
+                        _sum9 = _mm256_comp_fmadd_ps(_k01, _r09, _sum9);
+                        _sum10 = _mm256_comp_fmadd_ps(_k01, _r010, _sum10);
+                        _sum11 = _mm256_comp_fmadd_ps(_k01, _r011, _sum11);
 
                         _k01 = _mm256_loadu_ps(k01 + 48);
                         _r00 = _mm256_broadcast_ss(r0 + 6);
                         _r01 = _mm256_broadcast_ss(r0 + 14);
                         _r02 = _mm256_broadcast_ss(r0 + 22);
                         _r03 = _mm256_broadcast_ss(r0 + 30);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 38);
                         _r05 = _mm256_broadcast_ss(r0 + 46);
                         _r06 = _mm256_broadcast_ss(r0 + 54);
                         _r07 = _mm256_broadcast_ss(r0 + 62);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
                         _r08 = _mm256_broadcast_ss(r0 + 70);
                         _r09 = _mm256_broadcast_ss(r0 + 78);
                         _r010 = _mm256_broadcast_ss(r0 + 86);
                         _r011 = _mm256_broadcast_ss(r0 + 94);
 
-                        _sum8 = _mm256_fmadd_ps(_k01, _r08, _sum8);
-                        _sum9 = _mm256_fmadd_ps(_k01, _r09, _sum9);
-                        _sum10 = _mm256_fmadd_ps(_k01, _r010, _sum10);
-                        _sum11 = _mm256_fmadd_ps(_k01, _r011, _sum11);
+                        _sum8 = _mm256_comp_fmadd_ps(_k01, _r08, _sum8);
+                        _sum9 = _mm256_comp_fmadd_ps(_k01, _r09, _sum9);
+                        _sum10 = _mm256_comp_fmadd_ps(_k01, _r010, _sum10);
+                        _sum11 = _mm256_comp_fmadd_ps(_k01, _r011, _sum11);
 
                         _k01 = _mm256_loadu_ps(k01 + 56);
                         _r00 = _mm256_broadcast_ss(r0 + 7);
                         _r01 = _mm256_broadcast_ss(r0 + 15);
                         _r02 = _mm256_broadcast_ss(r0 + 23);
                         _r03 = _mm256_broadcast_ss(r0 + 31);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 39);
                         _r05 = _mm256_broadcast_ss(r0 + 47);
                         _r06 = _mm256_broadcast_ss(r0 + 55);
                         _r07 = _mm256_broadcast_ss(r0 + 63);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _r08 = _mm256_broadcast_ss(r0 + 71);
                         _r09 = _mm256_broadcast_ss(r0 + 79);
                         _r010 = _mm256_broadcast_ss(r0 + 87);
                         _r011 = _mm256_broadcast_ss(r0 + 95);
-                        _sum8 = _mm256_fmadd_ps(_k01, _r08, _sum8);
-                        _sum9 = _mm256_fmadd_ps(_k01, _r09, _sum9);
-                        _sum10 = _mm256_fmadd_ps(_k01, _r010, _sum10);
-                        _sum11 = _mm256_fmadd_ps(_k01, _r011, _sum11);
+                        _sum8 = _mm256_comp_fmadd_ps(_k01, _r08, _sum8);
+                        _sum9 = _mm256_comp_fmadd_ps(_k01, _r09, _sum9);
+                        _sum10 = _mm256_comp_fmadd_ps(_k01, _r010, _sum10);
+                        _sum11 = _mm256_comp_fmadd_ps(_k01, _r011, _sum11);
 
                         k01 += 64;
                         r0 += 96;
@@ -1626,18 +1625,18 @@ static void conv3x3s1_winograd64_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                         __m256 _r01 = _mm256_broadcast_ss(r0 + 8);
                         __m256 _r02 = _mm256_broadcast_ss(r0 + 16);
                         __m256 _r03 = _mm256_broadcast_ss(r0 + 24);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
                         __m256 _r04 = _mm256_broadcast_ss(r0 + 32);
                         __m256 _r05 = _mm256_broadcast_ss(r0 + 40);
                         __m256 _r06 = _mm256_broadcast_ss(r0 + 48);
                         __m256 _r07 = _mm256_broadcast_ss(r0 + 56);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _k01 = _mm256_loadu_ps(k01 + 8);
                         _r00 = _mm256_broadcast_ss(r0 + 1);
@@ -1645,132 +1644,132 @@ static void conv3x3s1_winograd64_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                         _r02 = _mm256_broadcast_ss(r0 + 17);
                         _r03 = _mm256_broadcast_ss(r0 + 25);
 
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
                         _r04 = _mm256_broadcast_ss(r0 + 33);
                         _r05 = _mm256_broadcast_ss(r0 + 41);
                         _r06 = _mm256_broadcast_ss(r0 + 49);
                         _r07 = _mm256_broadcast_ss(r0 + 57);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _k01 = _mm256_loadu_ps(k01 + 16);
                         _r00 = _mm256_broadcast_ss(r0 + 2);
                         _r01 = _mm256_broadcast_ss(r0 + 10);
                         _r02 = _mm256_broadcast_ss(r0 + 18);
                         _r03 = _mm256_broadcast_ss(r0 + 26);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 34);
                         _r05 = _mm256_broadcast_ss(r0 + 42);
                         _r06 = _mm256_broadcast_ss(r0 + 50);
                         _r07 = _mm256_broadcast_ss(r0 + 58);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _k01 = _mm256_loadu_ps(k01 + 24);
                         _r00 = _mm256_broadcast_ss(r0 + 3);
                         _r01 = _mm256_broadcast_ss(r0 + 11);
                         _r02 = _mm256_broadcast_ss(r0 + 19);
                         _r03 = _mm256_broadcast_ss(r0 + 27);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 35);
                         _r05 = _mm256_broadcast_ss(r0 + 43);
                         _r06 = _mm256_broadcast_ss(r0 + 51);
                         _r07 = _mm256_broadcast_ss(r0 + 59);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _k01 = _mm256_loadu_ps(k01 + 32);
                         _r00 = _mm256_broadcast_ss(r0 + 4);
                         _r01 = _mm256_broadcast_ss(r0 + 12);
                         _r02 = _mm256_broadcast_ss(r0 + 20);
                         _r03 = _mm256_broadcast_ss(r0 + 28);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 36);
                         _r05 = _mm256_broadcast_ss(r0 + 44);
                         _r06 = _mm256_broadcast_ss(r0 + 52);
                         _r07 = _mm256_broadcast_ss(r0 + 60);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _k01 = _mm256_loadu_ps(k01 + 40);
                         _r00 = _mm256_broadcast_ss(r0 + 5);
                         _r01 = _mm256_broadcast_ss(r0 + 13);
                         _r02 = _mm256_broadcast_ss(r0 + 21);
                         _r03 = _mm256_broadcast_ss(r0 + 29);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 37);
                         _r05 = _mm256_broadcast_ss(r0 + 45);
                         _r06 = _mm256_broadcast_ss(r0 + 53);
                         _r07 = _mm256_broadcast_ss(r0 + 61);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _k01 = _mm256_loadu_ps(k01 + 48);
                         _r00 = _mm256_broadcast_ss(r0 + 6);
                         _r01 = _mm256_broadcast_ss(r0 + 14);
                         _r02 = _mm256_broadcast_ss(r0 + 22);
                         _r03 = _mm256_broadcast_ss(r0 + 30);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 38);
                         _r05 = _mm256_broadcast_ss(r0 + 46);
                         _r06 = _mm256_broadcast_ss(r0 + 54);
                         _r07 = _mm256_broadcast_ss(r0 + 62);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         _k01 = _mm256_loadu_ps(k01 + 56);
                         _r00 = _mm256_broadcast_ss(r0 + 7);
                         _r01 = _mm256_broadcast_ss(r0 + 15);
                         _r02 = _mm256_broadcast_ss(r0 + 23);
                         _r03 = _mm256_broadcast_ss(r0 + 31);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _r04 = _mm256_broadcast_ss(r0 + 39);
                         _r05 = _mm256_broadcast_ss(r0 + 47);
                         _r06 = _mm256_broadcast_ss(r0 + 55);
                         _r07 = _mm256_broadcast_ss(r0 + 63);
-                        _sum4 = _mm256_fmadd_ps(_k01, _r04, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_k01, _r05, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_k01, _r06, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_k01, _r07, _sum7);
+                        _sum4 = _mm256_comp_fmadd_ps(_k01, _r04, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_k01, _r05, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_k01, _r06, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_k01, _r07, _sum7);
 
                         k01 += 64;
                         r0 += 64;
@@ -1804,80 +1803,80 @@ static void conv3x3s1_winograd64_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                         __m256 _r01 = _mm256_broadcast_ss(r0 + 8);
                         __m256 _r02 = _mm256_broadcast_ss(r0 + 16);
                         __m256 _r03 = _mm256_broadcast_ss(r0 + 24);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _k01 = _mm256_loadu_ps(k01 + 8);
                         _r00 = _mm256_broadcast_ss(r0 + 1);
                         _r01 = _mm256_broadcast_ss(r0 + 9);
                         _r02 = _mm256_broadcast_ss(r0 + 17);
                         _r03 = _mm256_broadcast_ss(r0 + 25);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _k01 = _mm256_loadu_ps(k01 + 16);
                         _r00 = _mm256_broadcast_ss(r0 + 2);
                         _r01 = _mm256_broadcast_ss(r0 + 10);
                         _r02 = _mm256_broadcast_ss(r0 + 18);
                         _r03 = _mm256_broadcast_ss(r0 + 26);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _k01 = _mm256_loadu_ps(k01 + 24);
                         _r00 = _mm256_broadcast_ss(r0 + 3);
                         _r01 = _mm256_broadcast_ss(r0 + 11);
                         _r02 = _mm256_broadcast_ss(r0 + 19);
                         _r03 = _mm256_broadcast_ss(r0 + 27);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _k01 = _mm256_loadu_ps(k01 + 32);
                         _r00 = _mm256_broadcast_ss(r0 + 4);
                         _r01 = _mm256_broadcast_ss(r0 + 12);
                         _r02 = _mm256_broadcast_ss(r0 + 20);
                         _r03 = _mm256_broadcast_ss(r0 + 28);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _k01 = _mm256_loadu_ps(k01 + 40);
                         _r00 = _mm256_broadcast_ss(r0 + 5);
                         _r01 = _mm256_broadcast_ss(r0 + 13);
                         _r02 = _mm256_broadcast_ss(r0 + 21);
                         _r03 = _mm256_broadcast_ss(r0 + 29);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _k01 = _mm256_loadu_ps(k01 + 48);
                         _r00 = _mm256_broadcast_ss(r0 + 6);
                         _r01 = _mm256_broadcast_ss(r0 + 14);
                         _r02 = _mm256_broadcast_ss(r0 + 22);
                         _r03 = _mm256_broadcast_ss(r0 + 30);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
 
                         _k01 = _mm256_loadu_ps(k01 + 56);
                         _r00 = _mm256_broadcast_ss(r0 + 7);
                         _r01 = _mm256_broadcast_ss(r0 + 15);
                         _r02 = _mm256_broadcast_ss(r0 + 23);
                         _r03 = _mm256_broadcast_ss(r0 + 31);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r00, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_k01, _r02, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_k01, _r03, _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r00, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_k01, _r02, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_k01, _r03, _sum3);
                         k01 += 64;
                         r0 += 32;
                     }
@@ -1902,50 +1901,50 @@ static void conv3x3s1_winograd64_pack8_avx(const Mat& bottom_blob, Mat& top_blob
                         __m256 _k01 = _mm256_loadu_ps(k01);
                         __m256 _r0 = _mm256_broadcast_ss(r0);
                         __m256 _r01 = _mm256_broadcast_ss(r0 + 8);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r0, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r0, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
 
                         _k01 = _mm256_loadu_ps(k01 + 8);
                         _r0 = _mm256_broadcast_ss(r0 + 1);
                         _r01 = _mm256_broadcast_ss(r0 + 9);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r0, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r0, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
 
                         _k01 = _mm256_loadu_ps(k01 + 16);
                         _r0 = _mm256_broadcast_ss(r0 + 2);
                         _r01 = _mm256_broadcast_ss(r0 + 10);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r0, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r0, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
 
                         _k01 = _mm256_loadu_ps(k01 + 24);
                         _r0 = _mm256_broadcast_ss(r0 + 3);
                         _r01 = _mm256_broadcast_ss(r0 + 11);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r0, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r0, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
 
                         _k01 = _mm256_loadu_ps(k01 + 32);
                         _r0 = _mm256_broadcast_ss(r0 + 4);
                         _r01 = _mm256_broadcast_ss(r0 + 12);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r0, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r0, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
 
                         _k01 = _mm256_loadu_ps(k01 + 40);
                         _r0 = _mm256_broadcast_ss(r0 + 5);
                         _r01 = _mm256_broadcast_ss(r0 + 13);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r0, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r0, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
 
                         _k01 = _mm256_loadu_ps(k01 + 48);
                         _r0 = _mm256_broadcast_ss(r0 + 6);
                         _r01 = _mm256_broadcast_ss(r0 + 14);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r0, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r0, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
 
                         _k01 = _mm256_loadu_ps(k01 + 56);
                         _r0 = _mm256_broadcast_ss(r0 + 7);
                         _r01 = _mm256_broadcast_ss(r0 + 15);
-                        _sum0 = _mm256_fmadd_ps(_k01, _r0, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_k01, _r01, _sum1);
+                        _sum0 = _mm256_comp_fmadd_ps(_k01, _r0, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_k01, _r01, _sum1);
 
                         k01 += 64;
                         r0 += 16;
@@ -2050,7 +2049,7 @@ static void conv3x3s1_winograd64_pack8_avx(const Mat& bottom_blob, Mat& top_blob
         int w_tm = outw / 6 * 8;
         int h_tm = outh / 6 * 8;
         const int tiles = w_tm / 8 * h_tm / 8;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < outch; p++)
         {
             const Mat out0_tm = top_blob_tm.channel(p);
diff --git a/src/layer/x86/convolution_3x3_pack8to1.h b/src/layer/x86/convolution_3x3_pack8to1.h
index 05d2050a..8b44e00d 100644
--- a/src/layer/x86/convolution_3x3_pack8to1.h
+++ b/src/layer/x86/convolution_3x3_pack8to1.h
@@ -23,7 +23,7 @@ static void conv3x3s1_pack8to1_avx(const Mat& bottom_blob, Mat& top_blob, const
 
     int remain_outch_start = 0;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = remain_outch_start; p < outch; p++)
     {
         Mat out0 = top_blob.channel(p);
@@ -71,17 +71,17 @@ static void conv3x3s1_pack8to1_avx(const Mat& bottom_blob, Mat& top_blob, const
                     __m256 _r11 = _mm256_loadu_ps(r1 + 8);
                     __m256 _r12 = _mm256_loadu_ps(r1 + 16);
 
-                    _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_k11, _r11, _sum1);
-                    _sum2 = _mm256_fmadd_ps(_k12, _r12, _sum2);
+                    _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_k11, _r11, _sum1);
+                    _sum2 = _mm256_comp_fmadd_ps(_k12, _r12, _sum2);
 
                     __m256 _r20 = _mm256_loadu_ps(r2);
                     __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                     __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                    _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                    _sum1 = _mm256_fmadd_ps(_k21, _r21, _sum1);
-                    _sum2 = _mm256_fmadd_ps(_k22, _r22, _sum2);
+                    _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                    _sum1 = _mm256_comp_fmadd_ps(_k21, _r21, _sum1);
+                    _sum2 = _mm256_comp_fmadd_ps(_k22, _r22, _sum2);
                     __m128 _sum = HorizontalSums(_sum0, _sum1, _sum2);
 
                     *outptr0 += _mm_reduce_add_ps(_sum); // dot
diff --git a/src/layer/x86/convolution_sgemm.h b/src/layer/x86/convolution_sgemm.h
index cc02136a..5498223c 100644
--- a/src/layer/x86/convolution_sgemm.h
+++ b/src/layer/x86/convolution_sgemm.h
@@ -128,7 +128,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
         const int stride = kernel_h * kernel_w * outw * outh;
         float* ret = (float*)bottom_im2col;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < inch; p++)
         {
             const float* input = bottom_blob.channel(p);
@@ -162,7 +162,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
         int nn_size = out_size >> 3;
         int remain_size_start = nn_size << 3;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = ii * 8;
@@ -191,7 +191,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
             }
         }
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int i = remain_size_start; i < out_size; i++)
         {
             const float* img0 = bottom_im2col.channel(0);
@@ -221,7 +221,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
         nn_outch = outch >> 3;
         remain_outch_start = nn_outch << 3;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int pp = 0; pp < nn_outch; pp++)
         {
             int i = pp * 8;
@@ -265,18 +265,18 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m256 _vb1 = _mm256_loadu_ps(vb + 8);
                     __m256 _vb2 = _mm256_loadu_ps(vb + 16);
                     __m256 _vb3 = _mm256_loadu_ps(vb + 24);
-                    _sum0 = _mm256_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
-                    _sum1 = _mm256_fmadd_ps(_vb0, _va1, _sum1); // sum1 = (a00-a07) * k10
-                    _sum2 = _mm256_fmadd_ps(_vb0, _va2, _sum2); // sum2 = (a00-a07) * k20
-                    _sum3 = _mm256_fmadd_ps(_vb0, _va3, _sum3); // sum3 = (a00-a07) * k30
+                    _sum0 = _mm256_comp_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
+                    _sum1 = _mm256_comp_fmadd_ps(_vb0, _va1, _sum1); // sum1 = (a00-a07) * k10
+                    _sum2 = _mm256_comp_fmadd_ps(_vb0, _va2, _sum2); // sum2 = (a00-a07) * k20
+                    _sum3 = _mm256_comp_fmadd_ps(_vb0, _va3, _sum3); // sum3 = (a00-a07) * k30
                     _va0 = _mm256_broadcast_ss(va + 4);
                     _va1 = _mm256_broadcast_ss(va + 5);
                     _va2 = _mm256_broadcast_ss(va + 6);
                     _va3 = _mm256_broadcast_ss(va + 7);
-                    _sum4 = _mm256_fmadd_ps(_vb0, _va0, _sum4); // sum4 = (a00-a07) * k40
-                    _sum5 = _mm256_fmadd_ps(_vb0, _va1, _sum5); // sum5 = (a00-a07) * k50
-                    _sum6 = _mm256_fmadd_ps(_vb0, _va2, _sum6); // sum6 = (a00-a07) * k60
-                    _sum7 = _mm256_fmadd_ps(_vb0, _va3, _sum7); // sum7 = (a00-a07) * k70
+                    _sum4 = _mm256_comp_fmadd_ps(_vb0, _va0, _sum4); // sum4 = (a00-a07) * k40
+                    _sum5 = _mm256_comp_fmadd_ps(_vb0, _va1, _sum5); // sum5 = (a00-a07) * k50
+                    _sum6 = _mm256_comp_fmadd_ps(_vb0, _va2, _sum6); // sum6 = (a00-a07) * k60
+                    _sum7 = _mm256_comp_fmadd_ps(_vb0, _va3, _sum7); // sum7 = (a00-a07) * k70
 
                     va += 8;
 
@@ -285,18 +285,18 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     _va1 = _mm256_broadcast_ss(va + 1);
                     _va2 = _mm256_broadcast_ss(va + 2);
                     _va3 = _mm256_broadcast_ss(va + 3);
-                    _sum0 = _mm256_fmadd_ps(_vb1, _va0, _sum0); // sum0 += (a10-a17) * k01
-                    _sum1 = _mm256_fmadd_ps(_vb1, _va1, _sum1); // sum1 += (a10-a17) * k11
-                    _sum2 = _mm256_fmadd_ps(_vb1, _va2, _sum2); // sum2 += (a10-a17) * k21
-                    _sum3 = _mm256_fmadd_ps(_vb1, _va3, _sum3); // sum3 += (a10-a17) * k31
+                    _sum0 = _mm256_comp_fmadd_ps(_vb1, _va0, _sum0); // sum0 += (a10-a17) * k01
+                    _sum1 = _mm256_comp_fmadd_ps(_vb1, _va1, _sum1); // sum1 += (a10-a17) * k11
+                    _sum2 = _mm256_comp_fmadd_ps(_vb1, _va2, _sum2); // sum2 += (a10-a17) * k21
+                    _sum3 = _mm256_comp_fmadd_ps(_vb1, _va3, _sum3); // sum3 += (a10-a17) * k31
                     _va0 = _mm256_broadcast_ss(va + 4);
                     _va1 = _mm256_broadcast_ss(va + 5);
                     _va2 = _mm256_broadcast_ss(va + 6);
                     _va3 = _mm256_broadcast_ss(va + 7);
-                    _sum4 = _mm256_fmadd_ps(_vb1, _va0, _sum4); // sum4 += (a10-a17) * k41
-                    _sum5 = _mm256_fmadd_ps(_vb1, _va1, _sum5); // sum5 += (a10-a17) * k51
-                    _sum6 = _mm256_fmadd_ps(_vb1, _va2, _sum6); // sum6 += (a10-a17) * k61
-                    _sum7 = _mm256_fmadd_ps(_vb1, _va3, _sum7); // sum7 += (a10-a17) * k71
+                    _sum4 = _mm256_comp_fmadd_ps(_vb1, _va0, _sum4); // sum4 += (a10-a17) * k41
+                    _sum5 = _mm256_comp_fmadd_ps(_vb1, _va1, _sum5); // sum5 += (a10-a17) * k51
+                    _sum6 = _mm256_comp_fmadd_ps(_vb1, _va2, _sum6); // sum6 += (a10-a17) * k61
+                    _sum7 = _mm256_comp_fmadd_ps(_vb1, _va3, _sum7); // sum7 += (a10-a17) * k71
 
                     va += 8;
 
@@ -305,18 +305,18 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     _va1 = _mm256_broadcast_ss(va + 1);
                     _va2 = _mm256_broadcast_ss(va + 2);
                     _va3 = _mm256_broadcast_ss(va + 3);
-                    _sum0 = _mm256_fmadd_ps(_vb2, _va0, _sum0); // sum0 += (a20-a27) * k02
-                    _sum1 = _mm256_fmadd_ps(_vb2, _va1, _sum1); // sum1 += (a20-a27) * k12
-                    _sum2 = _mm256_fmadd_ps(_vb2, _va2, _sum2); // sum2 += (a20-a27) * k22
-                    _sum3 = _mm256_fmadd_ps(_vb2, _va3, _sum3); // sum3 += (a20-a27) * k32
+                    _sum0 = _mm256_comp_fmadd_ps(_vb2, _va0, _sum0); // sum0 += (a20-a27) * k02
+                    _sum1 = _mm256_comp_fmadd_ps(_vb2, _va1, _sum1); // sum1 += (a20-a27) * k12
+                    _sum2 = _mm256_comp_fmadd_ps(_vb2, _va2, _sum2); // sum2 += (a20-a27) * k22
+                    _sum3 = _mm256_comp_fmadd_ps(_vb2, _va3, _sum3); // sum3 += (a20-a27) * k32
                     _va0 = _mm256_broadcast_ss(va + 4);
                     _va1 = _mm256_broadcast_ss(va + 5);
                     _va2 = _mm256_broadcast_ss(va + 6);
                     _va3 = _mm256_broadcast_ss(va + 7);
-                    _sum4 = _mm256_fmadd_ps(_vb2, _va0, _sum4); // sum4 += (a20-a27) * k42
-                    _sum5 = _mm256_fmadd_ps(_vb2, _va1, _sum5); // sum5 += (a20-a27) * k52
-                    _sum6 = _mm256_fmadd_ps(_vb2, _va2, _sum6); // sum6 += (a20-a27) * k62
-                    _sum7 = _mm256_fmadd_ps(_vb2, _va3, _sum7); // sum7 += (a20-a27) * k72
+                    _sum4 = _mm256_comp_fmadd_ps(_vb2, _va0, _sum4); // sum4 += (a20-a27) * k42
+                    _sum5 = _mm256_comp_fmadd_ps(_vb2, _va1, _sum5); // sum5 += (a20-a27) * k52
+                    _sum6 = _mm256_comp_fmadd_ps(_vb2, _va2, _sum6); // sum6 += (a20-a27) * k62
+                    _sum7 = _mm256_comp_fmadd_ps(_vb2, _va3, _sum7); // sum7 += (a20-a27) * k72
 
                     va += 8;
 
@@ -325,18 +325,18 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     _va1 = _mm256_broadcast_ss(va + 1);
                     _va2 = _mm256_broadcast_ss(va + 2);
                     _va3 = _mm256_broadcast_ss(va + 3);
-                    _sum0 = _mm256_fmadd_ps(_vb3, _va0, _sum0); // sum0 += (a30-a37) * k03
-                    _sum1 = _mm256_fmadd_ps(_vb3, _va1, _sum1); // sum1 += (a30-a37) * k13
-                    _sum2 = _mm256_fmadd_ps(_vb3, _va2, _sum2); // sum2 += (a30-a37) * k23
-                    _sum3 = _mm256_fmadd_ps(_vb3, _va3, _sum3); // sum3 += (a30-a37) * k33
+                    _sum0 = _mm256_comp_fmadd_ps(_vb3, _va0, _sum0); // sum0 += (a30-a37) * k03
+                    _sum1 = _mm256_comp_fmadd_ps(_vb3, _va1, _sum1); // sum1 += (a30-a37) * k13
+                    _sum2 = _mm256_comp_fmadd_ps(_vb3, _va2, _sum2); // sum2 += (a30-a37) * k23
+                    _sum3 = _mm256_comp_fmadd_ps(_vb3, _va3, _sum3); // sum3 += (a30-a37) * k33
                     _va0 = _mm256_broadcast_ss(va + 4);
                     _va1 = _mm256_broadcast_ss(va + 5);
                     _va2 = _mm256_broadcast_ss(va + 6);
                     _va3 = _mm256_broadcast_ss(va + 7);
-                    _sum4 = _mm256_fmadd_ps(_vb3, _va0, _sum4); // sum4 += (a30-a37) * k43
-                    _sum5 = _mm256_fmadd_ps(_vb3, _va1, _sum5); // sum5 += (a30-a37) * k53
-                    _sum6 = _mm256_fmadd_ps(_vb3, _va2, _sum6); // sum6 += (a30-a37) * k63
-                    _sum7 = _mm256_fmadd_ps(_vb3, _va3, _sum7); // sum7 += (a30-a37) * k73
+                    _sum4 = _mm256_comp_fmadd_ps(_vb3, _va0, _sum4); // sum4 += (a30-a37) * k43
+                    _sum5 = _mm256_comp_fmadd_ps(_vb3, _va1, _sum5); // sum5 += (a30-a37) * k53
+                    _sum6 = _mm256_comp_fmadd_ps(_vb3, _va2, _sum6); // sum6 += (a30-a37) * k63
+                    _sum7 = _mm256_comp_fmadd_ps(_vb3, _va3, _sum7); // sum7 += (a30-a37) * k73
 
                     va += 8;
                     vb += 32;
@@ -354,14 +354,14 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m256 _va6 = _mm256_broadcast_ss(va + 6);
                     __m256 _va7 = _mm256_broadcast_ss(va + 7);
                     __m256 _vb0 = _mm256_loadu_ps(vb);
-                    _sum0 = _mm256_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
-                    _sum1 = _mm256_fmadd_ps(_vb0, _va1, _sum1); // sum1 = (a00-a07) * k10
-                    _sum2 = _mm256_fmadd_ps(_vb0, _va2, _sum2); // sum2 = (a00-a07) * k20
-                    _sum3 = _mm256_fmadd_ps(_vb0, _va3, _sum3); // sum3 = (a00-a07) * k30
-                    _sum4 = _mm256_fmadd_ps(_vb0, _va4, _sum4); // sum4 = (a00-a07) * k40
-                    _sum5 = _mm256_fmadd_ps(_vb0, _va5, _sum5); // sum5 = (a00-a07) * k50
-                    _sum6 = _mm256_fmadd_ps(_vb0, _va6, _sum6); // sum6 = (a00-a07) * k60
-                    _sum7 = _mm256_fmadd_ps(_vb0, _va7, _sum7); // sum7 = (a00-a07) * k70
+                    _sum0 = _mm256_comp_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
+                    _sum1 = _mm256_comp_fmadd_ps(_vb0, _va1, _sum1); // sum1 = (a00-a07) * k10
+                    _sum2 = _mm256_comp_fmadd_ps(_vb0, _va2, _sum2); // sum2 = (a00-a07) * k20
+                    _sum3 = _mm256_comp_fmadd_ps(_vb0, _va3, _sum3); // sum3 = (a00-a07) * k30
+                    _sum4 = _mm256_comp_fmadd_ps(_vb0, _va4, _sum4); // sum4 = (a00-a07) * k40
+                    _sum5 = _mm256_comp_fmadd_ps(_vb0, _va5, _sum5); // sum5 = (a00-a07) * k50
+                    _sum6 = _mm256_comp_fmadd_ps(_vb0, _va6, _sum6); // sum6 = (a00-a07) * k60
+                    _sum7 = _mm256_comp_fmadd_ps(_vb0, _va7, _sum7); // sum7 = (a00-a07) * k70
 
                     va += 8;
                     vb += 8;
@@ -539,10 +539,10 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m256 _va2 = _mm256_loadu_ps(va + 16);
                     __m256 _va3 = _mm256_loadu_ps(va + 24);
 
-                    _sum0 = _mm256_fmadd_ps(_va0, _vb0, _sum0); // sum0 += (k00-k70) * a00
-                    _sum1 = _mm256_fmadd_ps(_va1, _vb1, _sum1); // sum1 += (k01-k71) * a10
-                    _sum2 = _mm256_fmadd_ps(_va2, _vb2, _sum2); // sum2 += (k02-k72) * a20
-                    _sum3 = _mm256_fmadd_ps(_va3, _vb3, _sum3); // sum3 += (k03-k73) * a30
+                    _sum0 = _mm256_comp_fmadd_ps(_va0, _vb0, _sum0); // sum0 += (k00-k70) * a00
+                    _sum1 = _mm256_comp_fmadd_ps(_va1, _vb1, _sum1); // sum1 += (k01-k71) * a10
+                    _sum2 = _mm256_comp_fmadd_ps(_va2, _vb2, _sum2); // sum2 += (k02-k72) * a20
+                    _sum3 = _mm256_comp_fmadd_ps(_va3, _vb3, _sum3); // sum3 += (k03-k73) * a30
 
                     va += 32;
                     vb += 4;
@@ -558,7 +558,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m256 _vb0 = _mm256_broadcast_ss(vb);
                     __m256 _va = _mm256_loadu_ps(va);
 
-                    _sum0_7 = _mm256_fmadd_ps(_va, _vb0, _sum0_7); // sum0 += (k00-k70) * a00
+                    _sum0_7 = _mm256_comp_fmadd_ps(_va, _vb0, _sum0_7); // sum0 += (k00-k70) * a00
 
                     va += 8;
                     vb += 1;
@@ -622,7 +622,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
 
         nn_outch = (outch - remain_outch_start) >> 2;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int pp = 0; pp < nn_outch; pp++)
         {
             int i = remain_outch_start + pp * 4;
@@ -658,10 +658,10 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m256 _vb1 = _mm256_loadu_ps(vb + 8);
                     __m256 _vb2 = _mm256_loadu_ps(vb + 16);
                     __m256 _vb3 = _mm256_loadu_ps(vb + 24);
-                    _sum0 = _mm256_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
-                    _sum1 = _mm256_fmadd_ps(_vb0, _va1, _sum1); // sum1 = (a00-a07) * k10
-                    _sum2 = _mm256_fmadd_ps(_vb0, _va2, _sum2); // sum2 = (a00-a07) * k20
-                    _sum3 = _mm256_fmadd_ps(_vb0, _va3, _sum3); // sum3 = (a00-a07) * k30
+                    _sum0 = _mm256_comp_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
+                    _sum1 = _mm256_comp_fmadd_ps(_vb0, _va1, _sum1); // sum1 = (a00-a07) * k10
+                    _sum2 = _mm256_comp_fmadd_ps(_vb0, _va2, _sum2); // sum2 = (a00-a07) * k20
+                    _sum3 = _mm256_comp_fmadd_ps(_vb0, _va3, _sum3); // sum3 = (a00-a07) * k30
 
                     va += 4;
 
@@ -670,10 +670,10 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     _va1 = _mm256_broadcast_ss(va + 1);
                     _va2 = _mm256_broadcast_ss(va + 2);
                     _va3 = _mm256_broadcast_ss(va + 3);
-                    _sum0 = _mm256_fmadd_ps(_vb1, _va0, _sum0); // sum0 += (a10-a17) * k01
-                    _sum1 = _mm256_fmadd_ps(_vb1, _va1, _sum1); // sum1 += (a10-a17) * k11
-                    _sum2 = _mm256_fmadd_ps(_vb1, _va2, _sum2); // sum2 += (a10-a17) * k21
-                    _sum3 = _mm256_fmadd_ps(_vb1, _va3, _sum3); // sum3 += (a10-a17) * k31
+                    _sum0 = _mm256_comp_fmadd_ps(_vb1, _va0, _sum0); // sum0 += (a10-a17) * k01
+                    _sum1 = _mm256_comp_fmadd_ps(_vb1, _va1, _sum1); // sum1 += (a10-a17) * k11
+                    _sum2 = _mm256_comp_fmadd_ps(_vb1, _va2, _sum2); // sum2 += (a10-a17) * k21
+                    _sum3 = _mm256_comp_fmadd_ps(_vb1, _va3, _sum3); // sum3 += (a10-a17) * k31
 
                     va += 4;
 
@@ -682,10 +682,10 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     _va1 = _mm256_broadcast_ss(va + 1);
                     _va2 = _mm256_broadcast_ss(va + 2);
                     _va3 = _mm256_broadcast_ss(va + 3);
-                    _sum0 = _mm256_fmadd_ps(_vb2, _va0, _sum0); // sum0 += (a20-a27) * k02
-                    _sum1 = _mm256_fmadd_ps(_vb2, _va1, _sum1); // sum1 += (a20-a27) * k12
-                    _sum2 = _mm256_fmadd_ps(_vb2, _va2, _sum2); // sum2 += (a20-a27) * k22
-                    _sum3 = _mm256_fmadd_ps(_vb2, _va3, _sum3); // sum3 += (a20-a27) * k32
+                    _sum0 = _mm256_comp_fmadd_ps(_vb2, _va0, _sum0); // sum0 += (a20-a27) * k02
+                    _sum1 = _mm256_comp_fmadd_ps(_vb2, _va1, _sum1); // sum1 += (a20-a27) * k12
+                    _sum2 = _mm256_comp_fmadd_ps(_vb2, _va2, _sum2); // sum2 += (a20-a27) * k22
+                    _sum3 = _mm256_comp_fmadd_ps(_vb2, _va3, _sum3); // sum3 += (a20-a27) * k32
 
                     va += 4;
 
@@ -694,10 +694,10 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     _va1 = _mm256_broadcast_ss(va + 1);
                     _va2 = _mm256_broadcast_ss(va + 2);
                     _va3 = _mm256_broadcast_ss(va + 3);
-                    _sum0 = _mm256_fmadd_ps(_vb3, _va0, _sum0); // sum0 += (a30-a37) * k03
-                    _sum1 = _mm256_fmadd_ps(_vb3, _va1, _sum1); // sum1 += (a30-a37) * k13
-                    _sum2 = _mm256_fmadd_ps(_vb3, _va2, _sum2); // sum2 += (a30-a37) * k23
-                    _sum3 = _mm256_fmadd_ps(_vb3, _va3, _sum3); // sum3 += (a30-a37) * k33
+                    _sum0 = _mm256_comp_fmadd_ps(_vb3, _va0, _sum0); // sum0 += (a30-a37) * k03
+                    _sum1 = _mm256_comp_fmadd_ps(_vb3, _va1, _sum1); // sum1 += (a30-a37) * k13
+                    _sum2 = _mm256_comp_fmadd_ps(_vb3, _va2, _sum2); // sum2 += (a30-a37) * k23
+                    _sum3 = _mm256_comp_fmadd_ps(_vb3, _va3, _sum3); // sum3 += (a30-a37) * k33
 
                     va += 4;
                     vb += 32;
@@ -711,10 +711,10 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m256 _va2 = _mm256_broadcast_ss(va + 2);
                     __m256 _va3 = _mm256_broadcast_ss(va + 3);
                     __m256 _vb0 = _mm256_loadu_ps(vb);
-                    _sum0 = _mm256_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
-                    _sum1 = _mm256_fmadd_ps(_vb0, _va1, _sum1); // sum1 = (a00-a07) * k10
-                    _sum2 = _mm256_fmadd_ps(_vb0, _va2, _sum2); // sum2 = (a00-a07) * k20
-                    _sum3 = _mm256_fmadd_ps(_vb0, _va3, _sum3); // sum3 = (a00-a07) * k30
+                    _sum0 = _mm256_comp_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
+                    _sum1 = _mm256_comp_fmadd_ps(_vb0, _va1, _sum1); // sum1 = (a00-a07) * k10
+                    _sum2 = _mm256_comp_fmadd_ps(_vb0, _va2, _sum2); // sum2 = (a00-a07) * k20
+                    _sum3 = _mm256_comp_fmadd_ps(_vb0, _va3, _sum3); // sum3 = (a00-a07) * k30
 
                     va += 4;
                     vb += 8;
@@ -839,10 +839,10 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m128 _va2 = _mm_loadu_ps(va + 8);
                     __m128 _va3 = _mm_loadu_ps(va + 12);
 
-                    _sum0 = _mm_fmadd_ps(_va0, _vb0, _sum0); // sum0 += (k00-k30) * a00
-                    _sum1 = _mm_fmadd_ps(_va1, _vb1, _sum1); // sum1 += (k01-k31) * a10
-                    _sum2 = _mm_fmadd_ps(_va2, _vb2, _sum2); // sum2 += (k02-k32) * a20
-                    _sum3 = _mm_fmadd_ps(_va3, _vb3, _sum3); // sum3 += (k03-k33) * a30
+                    _sum0 = _mm_comp_fmadd_ps(_va0, _vb0, _sum0); // sum0 += (k00-k30) * a00
+                    _sum1 = _mm_comp_fmadd_ps(_va1, _vb1, _sum1); // sum1 += (k01-k31) * a10
+                    _sum2 = _mm_comp_fmadd_ps(_va2, _vb2, _sum2); // sum2 += (k02-k32) * a20
+                    _sum3 = _mm_comp_fmadd_ps(_va3, _vb3, _sum3); // sum3 += (k03-k33) * a30
 
                     va += 16;
                     vb += 4;
@@ -858,7 +858,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m128 _vb0 = _mm_set1_ps(vb[0]);
                     __m128 _va = _mm_loadu_ps(va);
 
-                    _sum0_3 = _mm_fmadd_ps(_va, _vb0, _sum0_3); // sum0 += (k00-k30) * a00
+                    _sum0_3 = _mm_comp_fmadd_ps(_va, _vb0, _sum0_3); // sum0 += (k00-k30) * a00
 
                     va += 4;
                     vb += 1;
@@ -901,7 +901,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
 
         remain_outch_start += nn_outch << 2;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int i = remain_outch_start; i < outch; i++)
         {
             float* output = top_blob.channel(i);
@@ -929,10 +929,10 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m256 _vb2 = _mm256_loadu_ps(vb + 16);
                     __m256 _vb3 = _mm256_loadu_ps(vb + 24);
 
-                    _sum0 = _mm256_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
-                    _sum0 = _mm256_fmadd_ps(_vb1, _va1, _sum0); // sum0 += (a10-a17) * k01
-                    _sum0 = _mm256_fmadd_ps(_vb2, _va2, _sum0); // sum0 += (a20-a27) * k02
-                    _sum0 = _mm256_fmadd_ps(_vb3, _va3, _sum0); // sum0 += (a30-a37) * k03
+                    _sum0 = _mm256_comp_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
+                    _sum0 = _mm256_comp_fmadd_ps(_vb1, _va1, _sum0); // sum0 += (a10-a17) * k01
+                    _sum0 = _mm256_comp_fmadd_ps(_vb2, _va2, _sum0); // sum0 += (a20-a27) * k02
+                    _sum0 = _mm256_comp_fmadd_ps(_vb3, _va3, _sum0); // sum0 += (a30-a37) * k03
 
                     va += 4;
                     vb += 32;
@@ -944,7 +944,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m256 _va0 = _mm256_broadcast_ss(va);
                     __m256 _vb0 = _mm256_loadu_ps(vb);
 
-                    _sum0 = _mm256_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
+                    _sum0 = _mm256_comp_fmadd_ps(_vb0, _va0, _sum0); // sum0 = (a00-a07) * k00
 
                     va += 1;
                     vb += 8;
@@ -1009,7 +1009,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
                     __m128 _k0 = _mm_loadu_ps(va);
                     va += 4;
 
-                    _sum0 = _mm_fmadd_ps(_p0, _k0, _sum0);
+                    _sum0 = _mm_comp_fmadd_ps(_p0, _k0, _sum0);
                 }
 
                 float output_sum0[4] = {0.f};
@@ -1108,7 +1108,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
         const int stride = kernel_h * kernel_w * outw * outh;
         float* ret = (float*)bottom_im2col;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < inch; p++)
         {
             const float* input = bottom_blob.channel(p);
@@ -1142,7 +1142,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
         int nn_size = out_size >> 2;
         int remain_size_start = nn_size << 2;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int ii = 0; ii < nn_size; ii++)
         {
             int i = ii * 4;
@@ -1167,7 +1167,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
             }
         }
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int i = remain_size_start; i < out_size; i++)
         {
             const float* img0 = bottom_im2col.channel(0);
@@ -1197,7 +1197,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
         nn_outch = outch >> 2;
         remain_outch_start = nn_outch << 2;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int pp = 0; pp < nn_outch; pp++)
         {
             int i = pp * 4;
@@ -1467,7 +1467,7 @@ static void conv_im2col_sgemm_sse(const Mat& bottom_blob, Mat& top_blob, const M
             }
         }
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int i = remain_outch_start; i < outch; i++)
         {
             float* output = top_blob.channel(i);
diff --git a/src/layer/x86/convolution_x86.cpp b/src/layer/x86/convolution_x86.cpp
index 7e4ddee7..a15d13ab 100644
--- a/src/layer/x86/convolution_x86.cpp
+++ b/src/layer/x86/convolution_x86.cpp
@@ -58,17 +58,19 @@ namespace ncnn {
 #include "convolution_3x3_pack8to1.h"
 #include "convolution_3x3_pack8.h"
 #include "convolution_2x2_pack8.h"
-#include "convolution_2x2_pack8_fp16.h"
 #include "convolution_1x1_pack8.h"
+#if __AVX2__
+#include "convolution_2x2_pack8_fp16.h"
 #include "convolution_1x1_pack8_fp16.h"
 #endif
+#endif
 #endif // __SSE2__
 
 Convolution_x86::Convolution_x86()
 {
 #if __SSE2__
     support_packing = true;
-#if __AVX__
+#if __AVX2__
     support_weight_fp16_storage = true;
 #endif
 #endif // __SSE2__
@@ -183,8 +185,10 @@ int Convolution_x86::create_pipeline(const Option& opt)
     if (opt.use_packing_layout)
     {
 #if __AVX__
-        elempack = num_input % 8 == 0 ? 8 : num_input % 4 == 0 ? 4 : 1;
-        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4 : 1;
+        elempack = num_input % 8 == 0 ? 8 : num_input % 4 == 0 ? 4
+                                                               : 1;
+        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4
+                                                                     : 1;
 #else
         elempack = num_input % 4 == 0 ? 4 : 1;
         out_elempack = num_output % 4 == 0 ? 4 : 1;
@@ -264,6 +268,8 @@ int Convolution_x86::create_pipeline(const Option& opt)
     // pack8
     if (elempack == 8 && out_elempack == 8)
     {
+#if __AVX2__
+
         if (opt.use_weight_fp16_storage && kernel_w == 1 && kernel_h == 1 && dilation_w == 1 && dilation_h == 1 && stride_w == 1 && stride_h == 1)
         {
             conv1x1s1_sgemm_transform_kernel_fp16_pack8_avx(weight_data, weight_data_packed, num_input, num_output);
@@ -277,6 +283,9 @@ int Convolution_x86::create_pipeline(const Option& opt)
             conv2x2s1_weight_fp16_pack8_avx(weight_data, weight_data_packed, num_input, num_output);
         }
         else if (kernel_w == 3 && kernel_h == 3 && dilation_w == 1 && dilation_h == 1 && stride_w == 1 && stride_h == 1 && num_input >= 16 && num_output >= 16)
+#else
+        if (kernel_w == 3 && kernel_h == 3 && dilation_w == 1 && dilation_h == 1 && stride_w == 1 && stride_h == 1 && num_input >= 16 && num_output >= 16)
+#endif
         {
             conv3x3s1_winograd64_transform_kernel_pack8_avx(weight_data, weight_data_packed, num_input, num_output);
         }
@@ -365,7 +374,8 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
     if (opt.use_packing_layout)
     {
 #if __AVX__
-        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4 : 1;
+        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4
+                                                                     : 1;
 #else
         out_elempack = num_output % 4 == 0 ? 4 : 1;
 #endif
@@ -414,11 +424,13 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
     {
         if (kernel_w == 1 && kernel_h == 1 && dilation_w == 1 && dilation_h == 1 && stride_w == 1 && stride_h == 1)
         {
+#if __AVX2__
             if (opt.use_weight_fp16_storage)
             {
                 conv1x1s1_sgemm_fp16_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
             }
-            else
+#endif
+            if (!opt.use_weight_fp16_storage)
             {
                 conv1x1s1_sgemm_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
             }
@@ -430,11 +442,15 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
         }
         else if (kernel_w == 1 && kernel_h == 1 && dilation_w == 1 && dilation_h == 1 && stride_w == 2 && stride_h == 2)
         {
+#if __AVX2__
+
             if (opt.use_weight_fp16_storage)
             {
                 conv1x1s2_fp16_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
             }
-            else
+#endif
+            if (!opt.use_weight_fp16_storage)
+
             {
                 conv1x1s2_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
             }
@@ -461,11 +477,15 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
         }
         else if (kernel_w == 2 && kernel_h == 2 && dilation_w == 1 && dilation_h == 1 && stride_w == 1 && stride_h == 1)
         {
+#if __AVX2__
+
             if (opt.use_weight_fp16_storage)
             {
                 conv2x2s1_fp16_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
             }
-            else
+#endif
+
+            if (!opt.use_weight_fp16_storage)
             {
                 conv2x2s1_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
             }
@@ -478,7 +498,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
         else
         {
             // num_output
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int p = 0; p < num_output / out_elempack; p++)
             {
                 float* outptr = top_blob.channel(p);
@@ -576,7 +596,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
         else
         {
             // num_output
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int p = 0; p < num_output / out_elempack; p++)
             {
                 float* outptr = top_blob.channel(p);
@@ -604,7 +624,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
                             {
                                 __m256 _val = _mm256_set1_ps(sptr[space_ofs[k]]);
                                 __m256 _w = _mm256_loadu_ps(kptr);
-                                _sum = _mm256_fmadd_ps(_val, _w, _sum);
+                                _sum = _mm256_comp_fmadd_ps(_val, _w, _sum);
 
                                 kptr += 8;
                             }
@@ -625,7 +645,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
     {
         {
             // num_output
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int p = 0; p < num_output / out_elempack; p++)
             {
                 float* outptr = top_blob.channel(p);
@@ -657,13 +677,13 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
                                 __m256 _val3 = _mm256_broadcast_ss((sptr + space_ofs[k] * 4) + 3);
 
                                 __m256 _w0 = _mm256_loadu_ps(kptr);
-                                _sum = _mm256_fmadd_ps(_val0, _w0, _sum);
+                                _sum = _mm256_comp_fmadd_ps(_val0, _w0, _sum);
                                 __m256 _w1 = _mm256_loadu_ps(kptr + 8);
-                                _sum = _mm256_fmadd_ps(_val1, _w1, _sum);
+                                _sum = _mm256_comp_fmadd_ps(_val1, _w1, _sum);
                                 __m256 _w2 = _mm256_loadu_ps(kptr + 16);
-                                _sum = _mm256_fmadd_ps(_val2, _w2, _sum);
+                                _sum = _mm256_comp_fmadd_ps(_val2, _w2, _sum);
                                 __m256 _w3 = _mm256_loadu_ps(kptr + 24);
-                                _sum = _mm256_fmadd_ps(_val3, _w3, _sum);
+                                _sum = _mm256_comp_fmadd_ps(_val3, _w3, _sum);
 
                                 kptr += 32;
                             }
@@ -694,7 +714,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
         else
         {
             // num_output
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int p = 0; p < num_output; p++)
             {
                 float* outptr = top_blob.channel(p);
@@ -744,7 +764,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
     {
         {
             // num_output
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int p = 0; p < num_output / out_elempack; p++)
             {
                 float* outptr = top_blob.channel(p);
@@ -780,21 +800,21 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
                                 __m128 _val7 = _mm_broadcast_ss((sptr + space_ofs[k] * 8) + 7);
 
                                 __m128 _w0 = _mm_loadu_ps(kptr);
-                                _sum = _mm_fmadd_ps(_val0, _w0, _sum);
+                                _sum = _mm_comp_fmadd_ps(_val0, _w0, _sum);
                                 __m128 _w1 = _mm_loadu_ps(kptr + 4);
-                                _sum = _mm_fmadd_ps(_val1, _w1, _sum);
+                                _sum = _mm_comp_fmadd_ps(_val1, _w1, _sum);
                                 __m128 _w2 = _mm_loadu_ps(kptr + 8);
-                                _sum = _mm_fmadd_ps(_val2, _w2, _sum);
+                                _sum = _mm_comp_fmadd_ps(_val2, _w2, _sum);
                                 __m128 _w3 = _mm_loadu_ps(kptr + 12);
-                                _sum = _mm_fmadd_ps(_val3, _w3, _sum);
+                                _sum = _mm_comp_fmadd_ps(_val3, _w3, _sum);
                                 __m128 _w4 = _mm_loadu_ps(kptr + 16);
-                                _sum = _mm_fmadd_ps(_val4, _w4, _sum);
+                                _sum = _mm_comp_fmadd_ps(_val4, _w4, _sum);
                                 __m128 _w5 = _mm_loadu_ps(kptr + 20);
-                                _sum = _mm_fmadd_ps(_val5, _w5, _sum);
+                                _sum = _mm_comp_fmadd_ps(_val5, _w5, _sum);
                                 __m128 _w6 = _mm_loadu_ps(kptr + 24);
-                                _sum = _mm_fmadd_ps(_val6, _w6, _sum);
+                                _sum = _mm_comp_fmadd_ps(_val6, _w6, _sum);
                                 __m128 _w7 = _mm_loadu_ps(kptr + 28);
-                                _sum = _mm_fmadd_ps(_val7, _w7, _sum);
+                                _sum = _mm_comp_fmadd_ps(_val7, _w7, _sum);
 
                                 kptr += 32;
                             }
@@ -835,7 +855,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
         else
         {
             // num_output
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int p = 0; p < num_output / out_elempack; p++)
             {
                 float* outptr = top_blob.channel(p);
@@ -894,7 +914,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
     {
         {
             // num_output
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int p = 0; p < num_output / out_elempack; p++)
             {
                 float* outptr = top_blob.channel(p);
@@ -943,7 +963,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
     {
         {
             // num_output
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int p = 0; p < num_output; p++)
             {
                 float* outptr = top_blob.channel(p);
@@ -1020,7 +1040,7 @@ int Convolution_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option
         else
         {
             // num_output
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int p = 0; p < num_output; p++)
             {
                 float* outptr = top_blob.channel(p);
@@ -1474,7 +1494,7 @@ int Convolution_x86::forwardDilation_x86(const Mat& bottom_blob, Mat& top_blob,
             if (inner_top_blob.empty())
                 return -100;
 
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int c = 0; c < bottom_blob.c; c++)
             {
                 float* outptr = inner_bottom_blob.channel(c);
@@ -1494,7 +1514,7 @@ int Convolution_x86::forwardDilation_x86(const Mat& bottom_blob, Mat& top_blob,
             opt_g.blob_allocator = inner_top_blob.allocator;
             convolution_dilation1->forward(inner_bottom_blob, inner_top_blob, opt_g);
 
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int c = 0; c < num_output; c++)
             {
                 float* outptr = (float*)top_blob.channel(c) + x * outw + y;
diff --git a/src/layer/x86/convolutiondepthwise_3x3_pack8.h b/src/layer/x86/convolutiondepthwise_3x3_pack8.h
index 72c1262d..4d49efac 100644
--- a/src/layer/x86/convolutiondepthwise_3x3_pack8.h
+++ b/src/layer/x86/convolutiondepthwise_3x3_pack8.h
@@ -21,7 +21,7 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
 
     const float* bias = _bias;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int g = 0; g < group; g++)
     {
         Mat out = top_blob.channel(g);
@@ -67,15 +67,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -83,15 +83,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r23 = _mm256_loadu_ps(r2 + 24);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r01, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r21, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r01, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r21, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r23, _sum1);
 
                 __m256 _sum2 = _bias0;
                 __m256 _r04 = _mm256_loadu_ps(r0 + 32);
@@ -99,15 +99,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r24 = _mm256_loadu_ps(r2 + 32);
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
-                _sum2 = _mm256_fmadd_ps(_k00, _r02, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k01, _r03, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k02, _r04, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k10, _r12, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k11, _r13, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k12, _r14, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k20, _r22, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k21, _r23, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k22, _r24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k00, _r02, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k01, _r03, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k02, _r04, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k10, _r12, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k11, _r13, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k12, _r14, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k20, _r22, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k21, _r23, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k22, _r24, _sum2);
 
                 __m256 _sum3 = _bias0;
                 __m256 _r05 = _mm256_loadu_ps(r0 + 40);
@@ -115,15 +115,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r25 = _mm256_loadu_ps(r2 + 40);
                 _mm256_storeu_ps(outptr0 + 16, _sum2);
 
-                _sum3 = _mm256_fmadd_ps(_k00, _r03, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k01, _r04, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k02, _r05, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k10, _r13, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k11, _r14, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k12, _r15, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k20, _r23, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k21, _r24, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k22, _r25, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k00, _r03, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k01, _r04, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k02, _r05, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k10, _r13, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k11, _r14, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k12, _r15, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k20, _r23, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k21, _r24, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k22, _r25, _sum3);
 
                 __m256 _sum4 = _bias0;
                 __m256 _r06 = _mm256_loadu_ps(r0 + 48);
@@ -131,15 +131,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r26 = _mm256_loadu_ps(r2 + 48);
                 _mm256_storeu_ps(outptr0 + 24, _sum3);
 
-                _sum4 = _mm256_fmadd_ps(_k00, _r04, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k01, _r05, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k02, _r06, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k10, _r14, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k11, _r15, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k12, _r16, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k20, _r24, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k21, _r25, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k22, _r26, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k00, _r04, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k01, _r05, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k02, _r06, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k10, _r14, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k11, _r15, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k12, _r16, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k20, _r24, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k21, _r25, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k22, _r26, _sum4);
 
                 __m256 _sum5 = _bias0;
                 __m256 _r07 = _mm256_loadu_ps(r0 + 56);
@@ -147,15 +147,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r27 = _mm256_loadu_ps(r2 + 56);
                 _mm256_storeu_ps(outptr0 + 32, _sum4);
 
-                _sum5 = _mm256_fmadd_ps(_k00, _r05, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k01, _r06, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k02, _r07, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k10, _r15, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k11, _r16, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k12, _r17, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k20, _r25, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k21, _r26, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k22, _r27, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k00, _r05, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k01, _r06, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k02, _r07, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k10, _r15, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k11, _r16, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k12, _r17, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k20, _r25, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k21, _r26, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k22, _r27, _sum5);
 
                 __m256 _sum6 = _bias0;
                 __m256 _r08 = _mm256_loadu_ps(r0 + 64);
@@ -163,15 +163,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r28 = _mm256_loadu_ps(r2 + 64);
                 _mm256_storeu_ps(outptr0 + 40, _sum5);
 
-                _sum6 = _mm256_fmadd_ps(_k00, _r06, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k01, _r07, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k02, _r08, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k10, _r16, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k11, _r17, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k12, _r18, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k20, _r26, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k21, _r27, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k22, _r28, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k00, _r06, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k01, _r07, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k02, _r08, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k10, _r16, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k11, _r17, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k12, _r18, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k20, _r26, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k21, _r27, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k22, _r28, _sum6);
 
                 __m256 _sum7 = _bias0;
                 __m256 _r09 = _mm256_loadu_ps(r0 + 72);
@@ -179,15 +179,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r29 = _mm256_loadu_ps(r2 + 72);
                 _mm256_storeu_ps(outptr0 + 48, _sum6);
 
-                _sum7 = _mm256_fmadd_ps(_k00, _r07, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k01, _r08, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k02, _r09, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k10, _r17, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k11, _r18, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k12, _r19, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k20, _r27, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k21, _r28, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k22, _r29, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k00, _r07, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k01, _r08, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k02, _r09, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k10, _r17, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k11, _r18, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k12, _r19, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k20, _r27, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k21, _r28, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k22, _r29, _sum7);
                 _mm256_storeu_ps(outptr0 + 56, _sum7);
 
                 r0 += 64;
@@ -209,15 +209,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -225,15 +225,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r23 = _mm256_loadu_ps(r2 + 24);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r01, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r21, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r01, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r21, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r23, _sum1);
 
                 __m256 _sum2 = _bias0;
                 __m256 _r04 = _mm256_loadu_ps(r0 + 32);
@@ -241,15 +241,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r24 = _mm256_loadu_ps(r2 + 32);
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
-                _sum2 = _mm256_fmadd_ps(_k00, _r02, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k01, _r03, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k02, _r04, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k10, _r12, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k11, _r13, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k12, _r14, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k20, _r22, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k21, _r23, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k22, _r24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k00, _r02, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k01, _r03, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k02, _r04, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k10, _r12, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k11, _r13, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k12, _r14, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k20, _r22, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k21, _r23, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k22, _r24, _sum2);
 
                 __m256 _sum3 = _bias0;
                 __m256 _r05 = _mm256_loadu_ps(r0 + 40);
@@ -257,15 +257,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r25 = _mm256_loadu_ps(r2 + 40);
                 _mm256_storeu_ps(outptr0 + 16, _sum2);
 
-                _sum3 = _mm256_fmadd_ps(_k00, _r03, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k01, _r04, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k02, _r05, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k10, _r13, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k11, _r14, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k12, _r15, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k20, _r23, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k21, _r24, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k22, _r25, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k00, _r03, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k01, _r04, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k02, _r05, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k10, _r13, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k11, _r14, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k12, _r15, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k20, _r23, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k21, _r24, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k22, _r25, _sum3);
 
                 _mm256_storeu_ps(outptr0 + 24, _sum3);
 
@@ -288,15 +288,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -304,15 +304,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r23 = _mm256_loadu_ps(r2 + 24);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r01, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r21, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r01, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r21, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r23, _sum1);
 
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
@@ -335,15 +335,15 @@ static void convdw3x3s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 _mm256_storeu_ps(outptr0, _sum0);
 
@@ -373,7 +373,7 @@ static void convdw3x3s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
 
     const float* bias = _bias;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int g = 0; g < group; g++)
     {
         Mat out = top_blob.channel(g);
@@ -419,15 +419,15 @@ static void convdw3x3s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -438,15 +438,15 @@ static void convdw3x3s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r24 = _mm256_loadu_ps(r2 + 32);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r04, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r23, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r24, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r04, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r24, _sum1);
 
                 __m256 _sum2 = _bias0;
                 __m256 _r05 = _mm256_loadu_ps(r0 + 40);
@@ -457,15 +457,15 @@ static void convdw3x3s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r26 = _mm256_loadu_ps(r2 + 48);
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
-                _sum2 = _mm256_fmadd_ps(_k00, _r04, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k01, _r05, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k02, _r06, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k10, _r14, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k11, _r15, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k12, _r16, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k20, _r24, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k21, _r25, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k22, _r26, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k00, _r04, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k01, _r05, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k02, _r06, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k10, _r14, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k11, _r15, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k12, _r16, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k20, _r24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k21, _r25, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k22, _r26, _sum2);
 
                 __m256 _sum3 = _bias0;
                 __m256 _r07 = _mm256_loadu_ps(r0 + 56);
@@ -476,15 +476,15 @@ static void convdw3x3s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r28 = _mm256_loadu_ps(r2 + 64);
                 _mm256_storeu_ps(outptr0 + 16, _sum2);
 
-                _sum3 = _mm256_fmadd_ps(_k00, _r06, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k01, _r07, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k02, _r08, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k10, _r16, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k11, _r17, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k12, _r18, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k20, _r26, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k21, _r27, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k22, _r28, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k00, _r06, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k01, _r07, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k02, _r08, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k10, _r16, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k11, _r17, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k12, _r18, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k20, _r26, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k21, _r27, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k22, _r28, _sum3);
 
                 _mm256_storeu_ps(outptr0 + 24, _sum3);
 
@@ -507,15 +507,15 @@ static void convdw3x3s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -526,15 +526,15 @@ static void convdw3x3s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r24 = _mm256_loadu_ps(r2 + 32);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r04, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r23, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r24, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r04, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r24, _sum1);
 
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
@@ -557,15 +557,15 @@ static void convdw3x3s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
                 _mm256_storeu_ps(outptr0, _sum0);
                 r0 += 2 * 8;
                 r1 += 2 * 8;
diff --git a/src/layer/x86/convolutiondepthwise_3x3_pack8_fp16.h b/src/layer/x86/convolutiondepthwise_3x3_pack8_fp16.h
index b75e51c7..cb93ed1a 100644
--- a/src/layer/x86/convolutiondepthwise_3x3_pack8_fp16.h
+++ b/src/layer/x86/convolutiondepthwise_3x3_pack8_fp16.h
@@ -21,7 +21,7 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
 
     const float* bias = _bias;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int g = 0; g < group; g++)
     {
         Mat out = top_blob.channel(g);
@@ -67,15 +67,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -83,15 +83,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r23 = _mm256_loadu_ps(r2 + 24);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r01, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r21, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r01, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r21, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r23, _sum1);
 
                 __m256 _sum2 = _bias0;
                 __m256 _r04 = _mm256_loadu_ps(r0 + 32);
@@ -99,15 +99,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r24 = _mm256_loadu_ps(r2 + 32);
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
-                _sum2 = _mm256_fmadd_ps(_k00, _r02, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k01, _r03, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k02, _r04, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k10, _r12, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k11, _r13, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k12, _r14, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k20, _r22, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k21, _r23, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k22, _r24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k00, _r02, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k01, _r03, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k02, _r04, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k10, _r12, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k11, _r13, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k12, _r14, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k20, _r22, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k21, _r23, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k22, _r24, _sum2);
 
                 __m256 _sum3 = _bias0;
                 __m256 _r05 = _mm256_loadu_ps(r0 + 40);
@@ -115,15 +115,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r25 = _mm256_loadu_ps(r2 + 40);
                 _mm256_storeu_ps(outptr0 + 16, _sum2);
 
-                _sum3 = _mm256_fmadd_ps(_k00, _r03, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k01, _r04, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k02, _r05, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k10, _r13, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k11, _r14, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k12, _r15, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k20, _r23, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k21, _r24, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k22, _r25, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k00, _r03, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k01, _r04, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k02, _r05, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k10, _r13, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k11, _r14, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k12, _r15, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k20, _r23, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k21, _r24, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k22, _r25, _sum3);
 
                 __m256 _sum4 = _bias0;
                 __m256 _r06 = _mm256_loadu_ps(r0 + 48);
@@ -131,15 +131,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r26 = _mm256_loadu_ps(r2 + 48);
                 _mm256_storeu_ps(outptr0 + 24, _sum3);
 
-                _sum4 = _mm256_fmadd_ps(_k00, _r04, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k01, _r05, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k02, _r06, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k10, _r14, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k11, _r15, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k12, _r16, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k20, _r24, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k21, _r25, _sum4);
-                _sum4 = _mm256_fmadd_ps(_k22, _r26, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k00, _r04, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k01, _r05, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k02, _r06, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k10, _r14, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k11, _r15, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k12, _r16, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k20, _r24, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k21, _r25, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_k22, _r26, _sum4);
 
                 __m256 _sum5 = _bias0;
                 __m256 _r07 = _mm256_loadu_ps(r0 + 56);
@@ -147,15 +147,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r27 = _mm256_loadu_ps(r2 + 56);
                 _mm256_storeu_ps(outptr0 + 32, _sum4);
 
-                _sum5 = _mm256_fmadd_ps(_k00, _r05, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k01, _r06, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k02, _r07, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k10, _r15, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k11, _r16, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k12, _r17, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k20, _r25, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k21, _r26, _sum5);
-                _sum5 = _mm256_fmadd_ps(_k22, _r27, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k00, _r05, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k01, _r06, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k02, _r07, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k10, _r15, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k11, _r16, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k12, _r17, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k20, _r25, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k21, _r26, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_k22, _r27, _sum5);
 
                 __m256 _sum6 = _bias0;
                 __m256 _r08 = _mm256_loadu_ps(r0 + 64);
@@ -163,15 +163,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r28 = _mm256_loadu_ps(r2 + 64);
                 _mm256_storeu_ps(outptr0 + 40, _sum5);
 
-                _sum6 = _mm256_fmadd_ps(_k00, _r06, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k01, _r07, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k02, _r08, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k10, _r16, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k11, _r17, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k12, _r18, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k20, _r26, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k21, _r27, _sum6);
-                _sum6 = _mm256_fmadd_ps(_k22, _r28, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k00, _r06, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k01, _r07, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k02, _r08, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k10, _r16, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k11, _r17, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k12, _r18, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k20, _r26, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k21, _r27, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_k22, _r28, _sum6);
 
                 __m256 _sum7 = _bias0;
                 __m256 _r09 = _mm256_loadu_ps(r0 + 72);
@@ -179,15 +179,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r29 = _mm256_loadu_ps(r2 + 72);
                 _mm256_storeu_ps(outptr0 + 48, _sum6);
 
-                _sum7 = _mm256_fmadd_ps(_k00, _r07, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k01, _r08, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k02, _r09, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k10, _r17, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k11, _r18, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k12, _r19, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k20, _r27, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k21, _r28, _sum7);
-                _sum7 = _mm256_fmadd_ps(_k22, _r29, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k00, _r07, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k01, _r08, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k02, _r09, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k10, _r17, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k11, _r18, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k12, _r19, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k20, _r27, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k21, _r28, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_k22, _r29, _sum7);
                 _mm256_storeu_ps(outptr0 + 56, _sum7);
 
                 r0 += 64;
@@ -209,15 +209,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -225,15 +225,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r23 = _mm256_loadu_ps(r2 + 24);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r01, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r21, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r01, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r21, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r23, _sum1);
 
                 __m256 _sum2 = _bias0;
                 __m256 _r04 = _mm256_loadu_ps(r0 + 32);
@@ -241,15 +241,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r24 = _mm256_loadu_ps(r2 + 32);
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
-                _sum2 = _mm256_fmadd_ps(_k00, _r02, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k01, _r03, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k02, _r04, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k10, _r12, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k11, _r13, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k12, _r14, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k20, _r22, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k21, _r23, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k22, _r24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k00, _r02, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k01, _r03, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k02, _r04, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k10, _r12, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k11, _r13, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k12, _r14, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k20, _r22, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k21, _r23, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k22, _r24, _sum2);
 
                 __m256 _sum3 = _bias0;
                 __m256 _r05 = _mm256_loadu_ps(r0 + 40);
@@ -257,15 +257,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r25 = _mm256_loadu_ps(r2 + 40);
                 _mm256_storeu_ps(outptr0 + 16, _sum2);
 
-                _sum3 = _mm256_fmadd_ps(_k00, _r03, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k01, _r04, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k02, _r05, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k10, _r13, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k11, _r14, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k12, _r15, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k20, _r23, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k21, _r24, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k22, _r25, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k00, _r03, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k01, _r04, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k02, _r05, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k10, _r13, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k11, _r14, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k12, _r15, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k20, _r23, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k21, _r24, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k22, _r25, _sum3);
 
                 _mm256_storeu_ps(outptr0 + 24, _sum3);
 
@@ -288,15 +288,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -304,15 +304,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r23 = _mm256_loadu_ps(r2 + 24);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r01, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r11, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r21, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r01, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r11, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r21, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r23, _sum1);
 
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
@@ -335,15 +335,15 @@ static void convdw3x3s1_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 _mm256_storeu_ps(outptr0, _sum0);
 
@@ -373,7 +373,7 @@ static void convdw3x3s2_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
 
     const float* bias = _bias;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int g = 0; g < group; g++)
     {
         Mat out = top_blob.channel(g);
@@ -419,15 +419,15 @@ static void convdw3x3s2_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -438,15 +438,15 @@ static void convdw3x3s2_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r24 = _mm256_loadu_ps(r2 + 32);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r04, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r23, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r24, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r04, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r24, _sum1);
 
                 __m256 _sum2 = _bias0;
                 __m256 _r05 = _mm256_loadu_ps(r0 + 40);
@@ -457,15 +457,15 @@ static void convdw3x3s2_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r26 = _mm256_loadu_ps(r2 + 48);
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
-                _sum2 = _mm256_fmadd_ps(_k00, _r04, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k01, _r05, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k02, _r06, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k10, _r14, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k11, _r15, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k12, _r16, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k20, _r24, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k21, _r25, _sum2);
-                _sum2 = _mm256_fmadd_ps(_k22, _r26, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k00, _r04, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k01, _r05, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k02, _r06, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k10, _r14, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k11, _r15, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k12, _r16, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k20, _r24, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k21, _r25, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_k22, _r26, _sum2);
 
                 __m256 _sum3 = _bias0;
                 __m256 _r07 = _mm256_loadu_ps(r0 + 56);
@@ -476,15 +476,15 @@ static void convdw3x3s2_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r28 = _mm256_loadu_ps(r2 + 64);
                 _mm256_storeu_ps(outptr0 + 16, _sum2);
 
-                _sum3 = _mm256_fmadd_ps(_k00, _r06, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k01, _r07, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k02, _r08, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k10, _r16, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k11, _r17, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k12, _r18, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k20, _r26, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k21, _r27, _sum3);
-                _sum3 = _mm256_fmadd_ps(_k22, _r28, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k00, _r06, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k01, _r07, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k02, _r08, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k10, _r16, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k11, _r17, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k12, _r18, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k20, _r26, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k21, _r27, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_k22, _r28, _sum3);
 
                 _mm256_storeu_ps(outptr0 + 24, _sum3);
 
@@ -507,15 +507,15 @@ static void convdw3x3s2_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
 
                 __m256 _sum1 = _bias0;
                 __m256 _r03 = _mm256_loadu_ps(r0 + 24);
@@ -526,15 +526,15 @@ static void convdw3x3s2_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r24 = _mm256_loadu_ps(r2 + 32);
                 _mm256_storeu_ps(outptr0, _sum0);
 
-                _sum1 = _mm256_fmadd_ps(_k00, _r02, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k01, _r03, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k02, _r04, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k10, _r12, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k11, _r13, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k12, _r14, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k20, _r22, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k21, _r23, _sum1);
-                _sum1 = _mm256_fmadd_ps(_k22, _r24, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k00, _r02, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k01, _r03, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k02, _r04, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k10, _r12, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k11, _r13, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k12, _r14, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k20, _r22, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k21, _r23, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_k22, _r24, _sum1);
 
                 _mm256_storeu_ps(outptr0 + 8, _sum1);
 
@@ -557,15 +557,15 @@ static void convdw3x3s2_fp16_pack8_avx(const Mat& bottom_blob, Mat& top_blob, co
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
                 __m256 _r22 = _mm256_loadu_ps(r2 + 16);
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
                 _mm256_storeu_ps(outptr0, _sum0);
                 r0 += 2 * 8;
                 r1 += 2 * 8;
diff --git a/src/layer/x86/convolutiondepthwise_5x5_pack8.h b/src/layer/x86/convolutiondepthwise_5x5_pack8.h
index bd6f373e..bf190ad8 100644
--- a/src/layer/x86/convolutiondepthwise_5x5_pack8.h
+++ b/src/layer/x86/convolutiondepthwise_5x5_pack8.h
@@ -20,7 +20,7 @@ static void convdw5x5s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
     const int group = bottom_blob.c;
 
     const float* bias = _bias;
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int g = 0; g < group; g++)
     {
         Mat out = top_blob.channel(g);
@@ -61,11 +61,11 @@ static void convdw5x5s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k04 = _mm256_loadu_ps(k0 + 32);
                 k0 += 40;
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k03, _r03, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k04, _r04, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k03, _r03, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k04, _r04, _sum0);
 
                 __m256 _r10 = _mm256_loadu_ps(r1);
                 __m256 _r11 = _mm256_loadu_ps(r1 + 8);
@@ -80,11 +80,11 @@ static void convdw5x5s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k14 = _mm256_loadu_ps(k0 + 32);
                 k0 += 40;
 
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k13, _r13, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k14, _r14, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k13, _r13, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k14, _r14, _sum0);
 
                 __m256 _r20 = _mm256_loadu_ps(r2);
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
@@ -99,11 +99,11 @@ static void convdw5x5s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k24 = _mm256_loadu_ps(k0 + 32);
                 k0 += 40;
 
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k23, _r23, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k24, _r24, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k23, _r23, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k24, _r24, _sum0);
 
                 __m256 _r30 = _mm256_loadu_ps(r3);
                 __m256 _r31 = _mm256_loadu_ps(r3 + 8);
@@ -118,11 +118,11 @@ static void convdw5x5s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k34 = _mm256_loadu_ps(k0 + 32);
                 k0 += 40;
 
-                _sum0 = _mm256_fmadd_ps(_k30, _r30, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k31, _r31, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k32, _r32, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k33, _r33, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k34, _r34, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k30, _r30, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k31, _r31, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k32, _r32, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k33, _r33, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k34, _r34, _sum0);
 
                 __m256 _r40 = _mm256_loadu_ps(r4);
                 __m256 _r41 = _mm256_loadu_ps(r4 + 8);
@@ -137,11 +137,11 @@ static void convdw5x5s1_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k44 = _mm256_loadu_ps(k0 + 32);
                 k0 -= 160;
 
-                _sum0 = _mm256_fmadd_ps(_k40, _r40, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k41, _r41, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k42, _r42, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k43, _r43, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k44, _r44, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k40, _r40, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k41, _r41, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k42, _r42, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k43, _r43, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k44, _r44, _sum0);
 
                 _mm256_storeu_ps(outptr0, _sum0);
 
@@ -174,7 +174,7 @@ static void convdw5x5s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
     const int tailstep = (w - 2 * outw + w) * 8;
 
     const float* bias = _bias;
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int g = 0; g < group; g++)
     {
         Mat out = top_blob.channel(g);
@@ -215,11 +215,11 @@ static void convdw5x5s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k04 = _mm256_loadu_ps(k0 + 32);
                 k0 += 40;
 
-                _sum0 = _mm256_fmadd_ps(_k00, _r00, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k01, _r01, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k02, _r02, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k03, _r03, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k04, _r04, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k00, _r00, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k01, _r01, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k02, _r02, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k03, _r03, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k04, _r04, _sum0);
 
                 __m256 _r10 = _mm256_loadu_ps(r1);
                 __m256 _r11 = _mm256_loadu_ps(r1 + 8);
@@ -234,11 +234,11 @@ static void convdw5x5s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k14 = _mm256_loadu_ps(k0 + 32);
                 k0 += 40;
 
-                _sum0 = _mm256_fmadd_ps(_k10, _r10, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k11, _r11, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k12, _r12, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k13, _r13, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k14, _r14, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k10, _r10, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k11, _r11, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k12, _r12, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k13, _r13, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k14, _r14, _sum0);
 
                 __m256 _r20 = _mm256_loadu_ps(r2);
                 __m256 _r21 = _mm256_loadu_ps(r2 + 8);
@@ -253,11 +253,11 @@ static void convdw5x5s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k24 = _mm256_loadu_ps(k0 + 32);
                 k0 += 40;
 
-                _sum0 = _mm256_fmadd_ps(_k20, _r20, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k21, _r21, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k22, _r22, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k23, _r23, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k24, _r24, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k20, _r20, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k21, _r21, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k22, _r22, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k23, _r23, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k24, _r24, _sum0);
 
                 __m256 _r30 = _mm256_loadu_ps(r3);
                 __m256 _r31 = _mm256_loadu_ps(r3 + 8);
@@ -272,11 +272,11 @@ static void convdw5x5s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k34 = _mm256_loadu_ps(k0 + 32);
                 k0 += 40;
 
-                _sum0 = _mm256_fmadd_ps(_k30, _r30, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k31, _r31, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k32, _r32, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k33, _r33, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k34, _r34, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k30, _r30, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k31, _r31, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k32, _r32, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k33, _r33, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k34, _r34, _sum0);
 
                 __m256 _r40 = _mm256_loadu_ps(r4);
                 __m256 _r41 = _mm256_loadu_ps(r4 + 8);
@@ -291,11 +291,11 @@ static void convdw5x5s2_pack8_avx(const Mat& bottom_blob, Mat& top_blob, const M
                 __m256 _k44 = _mm256_loadu_ps(k0 + 32);
                 k0 -= 160;
 
-                _sum0 = _mm256_fmadd_ps(_k40, _r40, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k41, _r41, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k42, _r42, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k43, _r43, _sum0);
-                _sum0 = _mm256_fmadd_ps(_k44, _r44, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k40, _r40, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k41, _r41, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k42, _r42, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k43, _r43, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_k44, _r44, _sum0);
 
                 _mm256_storeu_ps(outptr0, _sum0);
 
diff --git a/src/layer/x86/convolutiondepthwise_x86.cpp b/src/layer/x86/convolutiondepthwise_x86.cpp
index 4222763e..f01b32a1 100644
--- a/src/layer/x86/convolutiondepthwise_x86.cpp
+++ b/src/layer/x86/convolutiondepthwise_x86.cpp
@@ -30,7 +30,9 @@ namespace ncnn {
 
 #if __SSE2__
 #if __AVX__
+#if __AVX2__
 #include "convolutiondepthwise_3x3_pack8_fp16.h"
+#endif
 #include "convolutiondepthwise_3x3_pack8.h"
 #include "convolutiondepthwise_5x5_pack8.h"
 #endif
@@ -44,7 +46,7 @@ ConvolutionDepthWise_x86::ConvolutionDepthWise_x86()
 {
 #if __SSE2__
     support_packing = true;
-#if __AVX__
+#if __AVX2__
     support_weight_fp16_storage = true;
 #endif
 #endif // __SSE2__
@@ -122,7 +124,8 @@ int ConvolutionDepthWise_x86::create_pipeline(const Option& opt)
         if (opt.use_packing_layout)
         {
 #if __AVX__
-            elempack = channels % 8 == 0 ? 8 : channels % 4 == 0 ? 4 : 1;
+            elempack = channels % 8 == 0 ? 8 : channels % 4 == 0 ? 4
+                                                                 : 1;
 #else
             elempack = channels % 4 == 0 ? 4 : 1;
 #endif
@@ -134,6 +137,7 @@ int ConvolutionDepthWise_x86::create_pipeline(const Option& opt)
         // pack8
         if (elempack == 8)
         {
+#if __AVX2__
             if (opt.use_weight_fp16_storage && kernel_w == 3 && kernel_h == 3 && dilation_w == 1 && dilation_h == 1 && stride_w == 1 && stride_h == 1)
             {
                 Mat weight_data_r2 = weight_data.reshape(maxk, group);
@@ -150,7 +154,7 @@ int ConvolutionDepthWise_x86::create_pipeline(const Option& opt)
                 ncnn::cast_float32_to_float16(weight_data_tmp, weight_data_packed, opt);
                 return 0;
             }
-
+#endif
             Mat weight_data_r2 = weight_data.reshape(maxk, group);
             convert_packing(weight_data_r2, weight_data_packed, 8);
 
@@ -337,7 +341,8 @@ int ConvolutionDepthWise_x86::forward(const Mat& bottom_blob, Mat& top_blob, con
     if (opt.use_packing_layout)
     {
 #if __AVX__
-        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4 : 1;
+        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4
+                                                                     : 1;
 #else
         out_elempack = num_output % 4 == 0 ? 4 : 1;
 #endif
@@ -360,11 +365,14 @@ int ConvolutionDepthWise_x86::forward(const Mat& bottom_blob, Mat& top_blob, con
         {
             if (kernel_w == 3 && kernel_h == 3 && dilation_w == 1 && dilation_h == 1 && stride_w == 1 && stride_h == 1)
             {
+#if __AVX2__
+
                 if (opt.use_weight_fp16_storage)
                 {
                     convdw3x3s1_fp16_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
                 }
-                else
+#endif
+                if (!opt.use_weight_fp16_storage)
                 {
                     convdw3x3s1_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
                 }
@@ -378,11 +386,13 @@ int ConvolutionDepthWise_x86::forward(const Mat& bottom_blob, Mat& top_blob, con
             }
             if (kernel_w == 3 && kernel_h == 3 && dilation_w == 1 && dilation_h == 1 && stride_w == 2 && stride_h == 2)
             {
+#if __AVX2__
                 if (opt.use_weight_fp16_storage)
                 {
                     convdw3x3s2_fp16_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
                 }
-                else
+#endif
+                if (!opt.use_weight_fp16_storage)
                 {
                     convdw3x3s2_pack8_avx(bottom_blob_bordered, top_blob, weight_data_packed, bias_data, opt);
                 }
@@ -439,7 +449,7 @@ int ConvolutionDepthWise_x86::forward(const Mat& bottom_blob, Mat& top_blob, con
                     }
                 }
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int g = 0; g < channels; g++)
                 {
                     float* outptr = top_blob.channel(g);
@@ -463,7 +473,7 @@ int ConvolutionDepthWise_x86::forward(const Mat& bottom_blob, Mat& top_blob, con
                             {
                                 __m256 _val = _mm256_loadu_ps(sptr + space_ofs[k] * 8);
                                 __m256 _w = _mm256_loadu_ps(kptr + k * 8);
-                                _sum = _mm256_fmadd_ps(_val, _w, _sum);
+                                _sum = _mm256_comp_fmadd_ps(_val, _w, _sum);
                             }
 
                             _sum = activation_avx(_sum, activation_type, activation_params);
@@ -504,7 +514,7 @@ int ConvolutionDepthWise_x86::forward(const Mat& bottom_blob, Mat& top_blob, con
                     }
                 }
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int g = 0; g < channels; g++)
                 {
                     float* outptr = top_blob.channel(g);
@@ -582,8 +592,10 @@ int ConvolutionDepthWise_x86::forward(const Mat& bottom_blob, Mat& top_blob, con
     if (opt.use_packing_layout)
     {
 #if __AVX__
-        g_elempack = channels_g % 8 == 0 ? 8 : channels_g % 4 == 0 ? 4 : 1;
-        out_g_elempack = num_output_g % 8 == 0 ? 8 : num_output_g % 4 == 0 ? 4 : 1;
+        g_elempack = channels_g % 8 == 0 ? 8 : channels_g % 4 == 0 ? 4
+                                                                   : 1;
+        out_g_elempack = num_output_g % 8 == 0 ? 8 : num_output_g % 4 == 0 ? 4
+                                                                           : 1;
 #else
         g_elempack = channels_g % 4 == 0 ? 4 : 1;
         out_g_elempack = num_output_g % 4 == 0 ? 4 : 1;
@@ -757,7 +769,7 @@ int ConvolutionDepthWise_x86::forward_int8_x86(const Mat& bottom_blob, Mat& top_
                     }
                 }
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int g = 0; g < channels; g++)
                 {
                     signed char* outptr_s8 = top_blob.channel(g);
@@ -951,7 +963,7 @@ int ConvolutionDepthWise_x86::forward_int8_x86(const Mat& bottom_blob, Mat& top_
                     }
                 }
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int g = 0; g < group; g++)
                 {
                     signed char* outptr_s8 = top_blob.channel(g);
@@ -1055,7 +1067,7 @@ int ConvolutionDepthWise_x86::forward_int8_x86(const Mat& bottom_blob, Mat& top_
             return -100;
     }
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int g = 0; g < group; g++)
     {
         const Mat bottom_blob_bordered_g = bottom_blob_bordered_unpacked.channel_range(channels_g * g / g_elempack, channels_g / g_elempack);
diff --git a/src/layer/x86/dequantize_x86.cpp b/src/layer/x86/dequantize_x86.cpp
index 0d31dddd..76a8ebf0 100644
--- a/src/layer/x86/dequantize_x86.cpp
+++ b/src/layer/x86/dequantize_x86.cpp
@@ -21,6 +21,8 @@
 #endif // __AVX__
 #endif // __SSE2__
 
+#include "x86_usability.h"
+
 namespace ncnn {
 
 Dequantize_x86::Dequantize_x86()
@@ -53,7 +55,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
@@ -68,20 +70,20 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m256 _bias = _mm256_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         float* ptr = (float*)top_blob + i * 8;
 
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale, _bias);
                         _mm256_storeu_ps(ptr, _v);
                     }
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
@@ -89,7 +91,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                         __m256 _bias = _mm256_loadu_ps((const float*)bias_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale, _bias);
                         _mm256_storeu_ps(ptr, _v);
                     }
                 }
@@ -98,7 +100,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
@@ -114,7 +116,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m256 _bias = _mm256_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
@@ -122,13 +124,13 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                         __m256 _scale = _mm256_loadu_ps((const float*)scale_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale, _bias);
                         _mm256_storeu_ps(ptr, _v);
                     }
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
@@ -137,7 +139,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                         __m256 _scale = _mm256_loadu_ps((const float*)scale_data + i * 8);
                         __m256 _bias = _mm256_loadu_ps((const float*)bias_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale, _bias);
                         _mm256_storeu_ps(ptr, _v);
                     }
                 }
@@ -155,7 +157,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const int* intptr = bottom_blob.row<const int>(i);
@@ -176,7 +178,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const int* intptr = bottom_blob.row<const int>(i);
@@ -188,7 +190,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                     for (int j = 0; j < w; j++)
                     {
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale, _bias);
                         _mm256_storeu_ps(ptr, _v);
 
                         intptr += 8;
@@ -211,7 +213,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const int* intptr = bottom_blob.channel(q);
@@ -232,7 +234,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const int* intptr = bottom_blob.channel(q);
@@ -244,7 +246,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                     for (int i = 0; i < size; i++)
                     {
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale, _bias);
                         _mm256_storeu_ps(ptr, _v);
 
                         intptr += 8;
@@ -274,7 +276,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outw; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -289,7 +291,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outw; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -302,7 +304,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outw; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -319,7 +321,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outw; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -335,7 +337,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outw; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -349,7 +351,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outw; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -377,7 +379,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const int* intptr = bottom_blob.row<const int>(i);
@@ -404,7 +406,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const int* intptr = bottom_blob.row<const int>(i);
@@ -447,7 +449,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const int* intptr = bottom_blob.channel(q);
@@ -474,7 +476,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const int* intptr = bottom_blob.channel(q);
@@ -523,7 +525,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -538,7 +540,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -551,7 +553,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -568,7 +570,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -584,7 +586,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -598,7 +600,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -625,7 +627,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const int* intptr = bottom_blob.row<const int>(i);
@@ -646,7 +648,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const int* intptr = bottom_blob.row<const int>(i);
@@ -681,7 +683,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const int* intptr = bottom_blob.channel(q);
@@ -702,7 +704,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const int* intptr = bottom_blob.channel(q);
@@ -745,7 +747,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     ptr[i] = intptr[i] * scale;
@@ -755,7 +757,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 const float bias = bias_data[0];
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     ptr[i] = intptr[i] * scale + bias;
@@ -763,7 +765,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     ptr[i] = intptr[i] * scale + bias_data[i];
@@ -774,7 +776,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
         {
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     ptr[i] = intptr[i] * scale_data[i];
@@ -784,7 +786,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 const float bias = bias_data[0];
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     ptr[i] = intptr[i] * scale_data[i] + bias;
@@ -792,7 +794,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     ptr[i] = intptr[i] * scale_data[i] + bias_data[i];
@@ -812,7 +814,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
         if (bias_data_size == 0)
         {
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < h; i++)
             {
                 const int* intptr = bottom_blob.row<const int>(i);
@@ -841,7 +843,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
         }
         else
         {
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < h; i++)
             {
                 const int* intptr = bottom_blob.row<const int>(i);
@@ -885,7 +887,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
         if (bias_data_size == 0)
         {
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const int* intptr = bottom_blob.channel(q);
@@ -914,7 +916,7 @@ int Dequantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
         }
         else
         {
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const int* intptr = bottom_blob.channel(q);
diff --git a/src/layer/x86/eltwise_x86.cpp b/src/layer/x86/eltwise_x86.cpp
index c8df8d43..4f7cfd09 100644
--- a/src/layer/x86/eltwise_x86.cpp
+++ b/src/layer/x86/eltwise_x86.cpp
@@ -20,6 +20,7 @@
 #include <immintrin.h>
 #endif // __AVX__
 #endif // __SSE2__
+#include "x86_usability.h"
 
 namespace ncnn {
 
@@ -52,7 +53,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
         {
             // first blob
             const Mat& bottom_blob1 = bottom_blobs[1];
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const float* ptr = bottom_blob.channel(q);
@@ -75,7 +76,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
             for (size_t b = 2; b < bottom_blobs.size(); b++)
             {
                 const Mat& bottom_blob2 = bottom_blobs[b];
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob2.channel(q);
@@ -100,7 +101,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
             {
                 // first blob
                 const Mat& bottom_blob1 = bottom_blobs[1];
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob.channel(q);
@@ -123,7 +124,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
                 for (size_t b = 2; b < bottom_blobs.size(); b++)
                 {
                     const Mat& bottom_blob2 = bottom_blobs[b];
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < channels; q++)
                     {
                         const float* ptr = bottom_blob2.channel(q);
@@ -148,7 +149,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
                 const Mat& bottom_blob1 = bottom_blobs[1];
                 __m256 _coeff0 = _mm256_set1_ps(coeffs[0]);
                 __m256 _coeff1 = _mm256_set1_ps(coeffs[1]);
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob.channel(q);
@@ -160,7 +161,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
                         __m256 _p = _mm256_loadu_ps(ptr);
                         __m256 _p1 = _mm256_loadu_ps(ptr1);
                         _p = _mm256_mul_ps(_p, _coeff0);
-                        _p = _mm256_fmadd_ps(_p1, _coeff1, _p);
+                        _p = _mm256_comp_fmadd_ps(_p1, _coeff1, _p);
                         _mm256_storeu_ps(outptr, _p);
 
                         ptr += 8;
@@ -173,7 +174,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
                 {
                     const Mat& bottom_blob2 = bottom_blobs[b];
                     __m256 _coeff = _mm256_set1_ps(coeffs[b]);
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < channels; q++)
                     {
                         const float* ptr = bottom_blob2.channel(q);
@@ -183,7 +184,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
                         {
                             __m256 _p = _mm256_loadu_ps(outptr);
                             __m256 _p1 = _mm256_loadu_ps(ptr);
-                            _p = _mm256_fmadd_ps(_p1, _coeff, _p);
+                            _p = _mm256_comp_fmadd_ps(_p1, _coeff, _p);
                             _mm256_storeu_ps(outptr, _p);
 
                             ptr += 8;
@@ -197,7 +198,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
         {
             // first blob
             const Mat& bottom_blob1 = bottom_blobs[1];
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const float* ptr = bottom_blob.channel(q);
@@ -220,7 +221,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
             for (size_t b = 2; b < bottom_blobs.size(); b++)
             {
                 const Mat& bottom_blob2 = bottom_blobs[b];
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob2.channel(q);
@@ -250,7 +251,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
         {
             // first blob
             const Mat& bottom_blob1 = bottom_blobs[1];
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const float* ptr = bottom_blob.channel(q);
@@ -273,7 +274,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
             for (size_t b = 2; b < bottom_blobs.size(); b++)
             {
                 const Mat& bottom_blob2 = bottom_blobs[b];
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob2.channel(q);
@@ -298,7 +299,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
             {
                 // first blob
                 const Mat& bottom_blob1 = bottom_blobs[1];
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob.channel(q);
@@ -321,7 +322,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
                 for (size_t b = 2; b < bottom_blobs.size(); b++)
                 {
                     const Mat& bottom_blob2 = bottom_blobs[b];
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < channels; q++)
                     {
                         const float* ptr = bottom_blob2.channel(q);
@@ -346,7 +347,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
                 const Mat& bottom_blob1 = bottom_blobs[1];
                 __m128 _coeff0 = _mm_set1_ps(coeffs[0]);
                 __m128 _coeff1 = _mm_set1_ps(coeffs[1]);
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob.channel(q);
@@ -372,7 +373,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
                 {
                     const Mat& bottom_blob2 = bottom_blobs[b];
                     __m128 _coeff = _mm_set1_ps(coeffs[b]);
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < channels; q++)
                     {
                         const float* ptr = bottom_blob2.channel(q);
@@ -397,7 +398,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
         {
             // first blob
             const Mat& bottom_blob1 = bottom_blobs[1];
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const float* ptr = bottom_blob.channel(q);
@@ -420,7 +421,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
             for (size_t b = 2; b < bottom_blobs.size(); b++)
             {
                 const Mat& bottom_blob2 = bottom_blobs[b];
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob2.channel(q);
@@ -448,7 +449,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
     {
         // first blob
         const Mat& bottom_blob1 = bottom_blobs[1];
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < channels; q++)
         {
             const float* ptr = bottom_blob.channel(q);
@@ -468,7 +469,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
         for (size_t b = 2; b < bottom_blobs.size(); b++)
         {
             const Mat& bottom_blob2 = bottom_blobs[b];
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const float* ptr = bottom_blob2.channel(q);
@@ -491,7 +492,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
         {
             // first blob
             const Mat& bottom_blob1 = bottom_blobs[1];
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const float* ptr = bottom_blob.channel(q);
@@ -512,7 +513,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
             for (size_t b = 2; b < bottom_blobs.size(); b++)
             {
                 const Mat& bottom_blob2 = bottom_blobs[b];
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob2.channel(q);
@@ -535,7 +536,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
             const Mat& bottom_blob1 = bottom_blobs[1];
             float coeff0 = coeffs[0];
             float coeff1 = coeffs[1];
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const float* ptr = bottom_blob.channel(q);
@@ -556,7 +557,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
             {
                 const Mat& bottom_blob2 = bottom_blobs[b];
                 float coeff = coeffs[b];
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob2.channel(q);
@@ -578,7 +579,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
     {
         // first blob
         const Mat& bottom_blob1 = bottom_blobs[1];
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < channels; q++)
         {
             const float* ptr = bottom_blob.channel(q);
@@ -599,7 +600,7 @@ int Eltwise_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>&
         for (size_t b = 2; b < bottom_blobs.size(); b++)
         {
             const Mat& bottom_blob2 = bottom_blobs[b];
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const float* ptr = bottom_blob2.channel(q);
diff --git a/src/layer/x86/hardsigmoid_x86.cpp b/src/layer/x86/hardsigmoid_x86.cpp
index d4fc5732..1cdc34ed 100644
--- a/src/layer/x86/hardsigmoid_x86.cpp
+++ b/src/layer/x86/hardsigmoid_x86.cpp
@@ -21,6 +21,8 @@
 #endif // __AVX__
 #endif // __SSE2__
 
+#include "x86_usability.h"
+
 namespace ncnn {
 
 HardSigmoid_x86::HardSigmoid_x86()
@@ -42,7 +44,7 @@ int HardSigmoid_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) co
 #if __AVX__
     if (elempack == 8)
     {
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < channels; q++)
         {
             float* ptr = bottom_top_blob.channel(q);
@@ -53,7 +55,11 @@ int HardSigmoid_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) co
             {
                 __m256 _p = _mm256_loadu_ps(ptr);
                 __m256 _ans = _mm256_set1_ps(beta);
-                _ans = _mm256_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
+#if __AVX2__
+                _ans = _mm256_comp_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
+#else
+                _ans = _mm256_comp_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
+#endif
                 _ans = _mm256_max_ps(_ans, _zero);
                 _ans = _mm256_min_ps(_ans, _one);
                 _mm256_storeu_ps(ptr, _ans);
@@ -68,7 +74,7 @@ int HardSigmoid_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) co
 
     if (elempack == 4)
     {
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < channels; q++)
         {
             float* ptr = bottom_top_blob.channel(q);
@@ -92,7 +98,7 @@ int HardSigmoid_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) co
     }
 #endif // __SSE2__
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int q = 0; q < channels; q++)
     {
         float* ptr = bottom_top_blob.channel(q);
@@ -105,7 +111,11 @@ int HardSigmoid_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) co
         {
             __m256 _p = _mm256_loadu_ps(ptr);
             __m256 _ans = _mm256_set1_ps(beta);
-            _ans = _mm256_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
+#if __AVX2__
+            _ans = _mm256_comp_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
+#else
+            _ans = _mm256_comp_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
+#endif
             _ans = _mm256_max_ps(_ans, _zero);
             _ans = _mm256_min_ps(_ans, _one);
             _mm256_storeu_ps(ptr, _ans);
diff --git a/src/layer/x86/hardswish_x86.cpp b/src/layer/x86/hardswish_x86.cpp
index ff4e0f5a..7d08661a 100644
--- a/src/layer/x86/hardswish_x86.cpp
+++ b/src/layer/x86/hardswish_x86.cpp
@@ -21,6 +21,8 @@
 #endif // __AVX__
 #endif // __SSE2__
 
+#include "x86_usability.h"
+
 namespace ncnn {
 
 HardSwish_x86::HardSwish_x86()
@@ -42,7 +44,7 @@ int HardSwish_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
 #if __AVX__
     if (elempack == 8)
     {
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < channels; q++)
         {
             float* ptr = bottom_top_blob.channel(q);
@@ -53,7 +55,7 @@ int HardSwish_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
             {
                 __m256 _p = _mm256_loadu_ps(ptr);
                 __m256 _ans = _mm256_set1_ps(beta);
-                _ans = _mm256_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
+                _ans = _mm256_comp_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
                 _ans = _mm256_max_ps(_ans, _zero);
                 _ans = _mm256_min_ps(_ans, _one);
                 _ans = _mm256_mul_ps(_ans, _p);
@@ -69,7 +71,7 @@ int HardSwish_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
 
     if (elempack == 4)
     {
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < channels; q++)
         {
             float* ptr = bottom_top_blob.channel(q);
@@ -94,7 +96,7 @@ int HardSwish_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
     }
 #endif // __SSE2__
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int q = 0; q < channels; q++)
     {
         float* ptr = bottom_top_blob.channel(q);
@@ -107,7 +109,7 @@ int HardSwish_x86::forward_inplace(Mat& bottom_top_blob, const Option& opt) cons
         {
             __m256 _p = _mm256_loadu_ps(ptr);
             __m256 _ans = _mm256_set1_ps(beta);
-            _ans = _mm256_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
+            _ans = _mm256_comp_fmadd_ps(_p, _mm256_set1_ps(alpha), _ans);
             _ans = _mm256_max_ps(_ans, _zero);
             _ans = _mm256_min_ps(_ans, _one);
             _ans = _mm256_mul_ps(_ans, _p);
diff --git a/src/layer/x86/innerproduct_x86.cpp b/src/layer/x86/innerproduct_x86.cpp
index afb353c1..69923136 100644
--- a/src/layer/x86/innerproduct_x86.cpp
+++ b/src/layer/x86/innerproduct_x86.cpp
@@ -32,7 +32,7 @@ InnerProduct_x86::InnerProduct_x86()
 {
 #if __SSE2__
     support_packing = true;
-#if __AVX__
+#if __AVX2__
     support_weight_fp16_storage = true;
 #endif
 #endif // __SSE2__
@@ -69,7 +69,8 @@ int InnerProduct_x86::create_pipeline(const Option& opt)
     if (opt.use_packing_layout)
     {
 #if __AVX__
-        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4 : 1;
+        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4
+                                                                     : 1;
 #else
         out_elempack = num_output % 4 == 0 ? 4 : 1;
 #endif
@@ -100,7 +101,7 @@ int InnerProduct_x86::create_pipeline(const Option& opt)
         }
     }
 
-#if __AVX__
+#if __AVX2__
     if (opt.use_weight_fp16_storage && weight_data.elemsize == 4u)
     {
         ncnn::cast_float32_to_float16(weight_data, weight_data_fp16, opt);
@@ -151,14 +152,15 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
         if (opt.use_packing_layout)
         {
 #if __AVX__
-            num_output_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4 : 1;
+            num_output_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4
+                                                                                : 1;
 #else
             num_output_elempack = num_output % 4 == 0 ? 4 : 1;
 #endif
         }
 #endif // __SSE2__
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int j = 0; j < h; j++)
         {
 #if __SSE2__
@@ -204,14 +206,14 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                         __m256 _k5 = _mm256_set1_ps(kptr[5]);
                         __m256 _k6 = _mm256_set1_ps(kptr[6]);
                         __m256 _k7 = _mm256_set1_ps(kptr[7]);
-                        _sum0 = _mm256_fmadd_ps(_val, _k0, _sum0);
-                        _sum1 = _mm256_fmadd_ps(_val, _k1, _sum1);
-                        _sum2 = _mm256_fmadd_ps(_val, _k2, _sum2);
-                        _sum3 = _mm256_fmadd_ps(_val, _k3, _sum3);
-                        _sum4 = _mm256_fmadd_ps(_val, _k4, _sum4);
-                        _sum5 = _mm256_fmadd_ps(_val, _k5, _sum5);
-                        _sum6 = _mm256_fmadd_ps(_val, _k6, _sum6);
-                        _sum7 = _mm256_fmadd_ps(_val, _k7, _sum7);
+                        _sum0 = _mm256_comp_fmadd_ps(_val, _k0, _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_val, _k1, _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_val, _k2, _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_val, _k3, _sum3);
+                        _sum4 = _mm256_comp_fmadd_ps(_val, _k4, _sum4);
+                        _sum5 = _mm256_comp_fmadd_ps(_val, _k5, _sum5);
+                        _sum6 = _mm256_comp_fmadd_ps(_val, _k6, _sum6);
+                        _sum7 = _mm256_comp_fmadd_ps(_val, _k7, _sum7);
 
                         m += 8;
                         kptr += 8;
@@ -267,21 +269,21 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                         __m256 _val7 = _mm256_broadcast_ss(m + 7);
 
                         __m256 _w0 = _mm256_loadu_ps(kptr);
-                        _sum = _mm256_fmadd_ps(_val0, _w0, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val0, _w0, _sum);
                         __m256 _w1 = _mm256_loadu_ps(kptr + 8);
-                        _sum = _mm256_fmadd_ps(_val1, _w1, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val1, _w1, _sum);
                         __m256 _w2 = _mm256_loadu_ps(kptr + 16);
-                        _sum = _mm256_fmadd_ps(_val2, _w2, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val2, _w2, _sum);
                         __m256 _w3 = _mm256_loadu_ps(kptr + 24);
-                        _sum = _mm256_fmadd_ps(_val3, _w3, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val3, _w3, _sum);
                         __m256 _w4 = _mm256_loadu_ps(kptr + 32);
-                        _sum = _mm256_fmadd_ps(_val4, _w4, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val4, _w4, _sum);
                         __m256 _w5 = _mm256_loadu_ps(kptr + 40);
-                        _sum = _mm256_fmadd_ps(_val5, _w5, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val5, _w5, _sum);
                         __m256 _w6 = _mm256_loadu_ps(kptr + 48);
-                        _sum = _mm256_fmadd_ps(_val6, _w6, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val6, _w6, _sum);
                         __m256 _w7 = _mm256_loadu_ps(kptr + 56);
-                        _sum = _mm256_fmadd_ps(_val7, _w7, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val7, _w7, _sum);
 
                         m += 8;
                         kptr += 64;
@@ -294,13 +296,13 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                         __m256 _val3 = _mm256_broadcast_ss(m + 3);
 
                         __m256 _w0 = _mm256_loadu_ps(kptr);
-                        _sum = _mm256_fmadd_ps(_val0, _w0, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val0, _w0, _sum);
                         __m256 _w1 = _mm256_loadu_ps(kptr + 8);
-                        _sum = _mm256_fmadd_ps(_val1, _w1, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val1, _w1, _sum);
                         __m256 _w2 = _mm256_loadu_ps(kptr + 16);
-                        _sum = _mm256_fmadd_ps(_val2, _w2, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val2, _w2, _sum);
                         __m256 _w3 = _mm256_loadu_ps(kptr + 24);
-                        _sum = _mm256_fmadd_ps(_val3, _w3, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val3, _w3, _sum);
 
                         m += 4;
                         kptr += 32;
@@ -309,7 +311,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                     {
                         __m256 _val = _mm256_set1_ps(m[0]);
                         __m256 _w = _mm256_loadu_ps(kptr);
-                        _sum = _mm256_fmadd_ps(_val, _w, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_val, _w, _sum);
 
                         m += 1;
                         kptr += 8;
@@ -356,14 +358,14 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                     for (; i < num_input; i++)
                     {
                         __m128 _val = _mm_loadu_ps(m);
-                        _sum0 = _mm_fmadd_ps(_val, _mm_set1_ps(kptr[0]), _sum0);
-                        _sum1 = _mm_fmadd_ps(_val, _mm_set1_ps(kptr[1]), _sum1);
-                        _sum2 = _mm_fmadd_ps(_val, _mm_set1_ps(kptr[2]), _sum2);
-                        _sum3 = _mm_fmadd_ps(_val, _mm_set1_ps(kptr[3]), _sum3);
-                        _sum4 = _mm_fmadd_ps(_val, _mm_set1_ps(kptr[4]), _sum4);
-                        _sum5 = _mm_fmadd_ps(_val, _mm_set1_ps(kptr[5]), _sum5);
-                        _sum6 = _mm_fmadd_ps(_val, _mm_set1_ps(kptr[6]), _sum6);
-                        _sum7 = _mm_fmadd_ps(_val, _mm_set1_ps(kptr[7]), _sum7);
+                        _sum0 = _mm_comp_fmadd_ps(_val, _mm_set1_ps(kptr[0]), _sum0);
+                        _sum1 = _mm_comp_fmadd_ps(_val, _mm_set1_ps(kptr[1]), _sum1);
+                        _sum2 = _mm_comp_fmadd_ps(_val, _mm_set1_ps(kptr[2]), _sum2);
+                        _sum3 = _mm_comp_fmadd_ps(_val, _mm_set1_ps(kptr[3]), _sum3);
+                        _sum4 = _mm_comp_fmadd_ps(_val, _mm_set1_ps(kptr[4]), _sum4);
+                        _sum5 = _mm_comp_fmadd_ps(_val, _mm_set1_ps(kptr[5]), _sum5);
+                        _sum6 = _mm_comp_fmadd_ps(_val, _mm_set1_ps(kptr[6]), _sum6);
+                        _sum7 = _mm_comp_fmadd_ps(_val, _mm_set1_ps(kptr[7]), _sum7);
 
                         m += 4;
                         kptr += 8;
@@ -420,14 +422,14 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                         __m256 _val5 = _mm256_loadu_ps(m + 40);
                         __m256 _val6 = _mm256_loadu_ps(m + 48);
                         __m256 _val7 = _mm256_loadu_ps(m + 56);
-                        _sum0 = _mm256_fmadd_ps(_val0, _mm256_set1_ps(kptr[0]), _sum0);
-                        _sum1 = _mm256_fmadd_ps(_val1, _mm256_set1_ps(kptr[1]), _sum1);
-                        _sum2 = _mm256_fmadd_ps(_val2, _mm256_set1_ps(kptr[2]), _sum2);
-                        _sum3 = _mm256_fmadd_ps(_val3, _mm256_set1_ps(kptr[3]), _sum3);
-                        _sum0 = _mm256_fmadd_ps(_val4, _mm256_set1_ps(kptr[4]), _sum0);
-                        _sum1 = _mm256_fmadd_ps(_val5, _mm256_set1_ps(kptr[5]), _sum1);
-                        _sum2 = _mm256_fmadd_ps(_val6, _mm256_set1_ps(kptr[6]), _sum2);
-                        _sum3 = _mm256_fmadd_ps(_val7, _mm256_set1_ps(kptr[7]), _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_val0, _mm256_set1_ps(kptr[0]), _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_val1, _mm256_set1_ps(kptr[1]), _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_val2, _mm256_set1_ps(kptr[2]), _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_val3, _mm256_set1_ps(kptr[3]), _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_val4, _mm256_set1_ps(kptr[4]), _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_val5, _mm256_set1_ps(kptr[5]), _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_val6, _mm256_set1_ps(kptr[6]), _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_val7, _mm256_set1_ps(kptr[7]), _sum3);
 
                         m += 64;
                         kptr += 8;
@@ -438,10 +440,10 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                         __m256 _val1 = _mm256_loadu_ps(m + 8);
                         __m256 _val2 = _mm256_loadu_ps(m + 16);
                         __m256 _val3 = _mm256_loadu_ps(m + 24);
-                        _sum0 = _mm256_fmadd_ps(_val0, _mm256_set1_ps(kptr[0]), _sum0);
-                        _sum1 = _mm256_fmadd_ps(_val1, _mm256_set1_ps(kptr[1]), _sum1);
-                        _sum2 = _mm256_fmadd_ps(_val2, _mm256_set1_ps(kptr[2]), _sum2);
-                        _sum3 = _mm256_fmadd_ps(_val3, _mm256_set1_ps(kptr[3]), _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_val0, _mm256_set1_ps(kptr[0]), _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_val1, _mm256_set1_ps(kptr[1]), _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_val2, _mm256_set1_ps(kptr[2]), _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_val3, _mm256_set1_ps(kptr[3]), _sum3);
 
                         m += 32;
                         kptr += 4;
@@ -450,7 +452,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                     {
                         __m256 _val = _mm256_loadu_ps(m);
                         __m256 _k = _mm256_set1_ps(kptr[0]);
-                        _sum0 = _mm256_fmadd_ps(_val, _k, _sum0);
+                        _sum0 = _mm256_comp_fmadd_ps(_val, _k, _sum0);
 
                         m += 8;
                         kptr += 1;
@@ -496,24 +498,24 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                         __m256 _val1 = _mm256_loadu_ps(m + 8);
                         __m256 _val2 = _mm256_loadu_ps(m + 16);
                         __m256 _val3 = _mm256_loadu_ps(m + 24);
-                        _sum0 = _mm256_fmadd_ps(_val0, _mm256_set1_ps(kptr[0]), _sum0);
-                        _sum1 = _mm256_fmadd_ps(_val0, _mm256_set1_ps(kptr[1]), _sum1);
-                        _sum2 = _mm256_fmadd_ps(_val0, _mm256_set1_ps(kptr[2]), _sum2);
-                        _sum3 = _mm256_fmadd_ps(_val0, _mm256_set1_ps(kptr[3]), _sum3);
-                        _sum0 = _mm256_fmadd_ps(_val1, _mm256_set1_ps(kptr[4]), _sum0);
-                        _sum1 = _mm256_fmadd_ps(_val1, _mm256_set1_ps(kptr[5]), _sum1);
-                        _sum2 = _mm256_fmadd_ps(_val1, _mm256_set1_ps(kptr[6]), _sum2);
-                        _sum3 = _mm256_fmadd_ps(_val1, _mm256_set1_ps(kptr[7]), _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_val0, _mm256_set1_ps(kptr[0]), _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_val0, _mm256_set1_ps(kptr[1]), _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_val0, _mm256_set1_ps(kptr[2]), _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_val0, _mm256_set1_ps(kptr[3]), _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_val1, _mm256_set1_ps(kptr[4]), _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_val1, _mm256_set1_ps(kptr[5]), _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_val1, _mm256_set1_ps(kptr[6]), _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_val1, _mm256_set1_ps(kptr[7]), _sum3);
                         kptr += 8;
 
-                        _sum0 = _mm256_fmadd_ps(_val2, _mm256_set1_ps(kptr[0]), _sum0);
-                        _sum1 = _mm256_fmadd_ps(_val2, _mm256_set1_ps(kptr[1]), _sum1);
-                        _sum2 = _mm256_fmadd_ps(_val2, _mm256_set1_ps(kptr[2]), _sum2);
-                        _sum3 = _mm256_fmadd_ps(_val2, _mm256_set1_ps(kptr[3]), _sum3);
-                        _sum0 = _mm256_fmadd_ps(_val3, _mm256_set1_ps(kptr[4]), _sum0);
-                        _sum1 = _mm256_fmadd_ps(_val3, _mm256_set1_ps(kptr[5]), _sum1);
-                        _sum2 = _mm256_fmadd_ps(_val3, _mm256_set1_ps(kptr[6]), _sum2);
-                        _sum3 = _mm256_fmadd_ps(_val3, _mm256_set1_ps(kptr[7]), _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_val2, _mm256_set1_ps(kptr[0]), _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_val2, _mm256_set1_ps(kptr[1]), _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_val2, _mm256_set1_ps(kptr[2]), _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_val2, _mm256_set1_ps(kptr[3]), _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_val3, _mm256_set1_ps(kptr[4]), _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_val3, _mm256_set1_ps(kptr[5]), _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_val3, _mm256_set1_ps(kptr[6]), _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_val3, _mm256_set1_ps(kptr[7]), _sum3);
 
                         m += 32;
                         kptr += 8;
@@ -521,10 +523,10 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                     for (; i < num_input; i++)
                     {
                         __m256 _val = _mm256_loadu_ps(m);
-                        _sum0 = _mm256_fmadd_ps(_val, _mm256_set1_ps(kptr[0]), _sum0);
-                        _sum1 = _mm256_fmadd_ps(_val, _mm256_set1_ps(kptr[1]), _sum1);
-                        _sum2 = _mm256_fmadd_ps(_val, _mm256_set1_ps(kptr[2]), _sum2);
-                        _sum3 = _mm256_fmadd_ps(_val, _mm256_set1_ps(kptr[3]), _sum3);
+                        _sum0 = _mm256_comp_fmadd_ps(_val, _mm256_set1_ps(kptr[0]), _sum0);
+                        _sum1 = _mm256_comp_fmadd_ps(_val, _mm256_set1_ps(kptr[1]), _sum1);
+                        _sum2 = _mm256_comp_fmadd_ps(_val, _mm256_set1_ps(kptr[2]), _sum2);
+                        _sum3 = _mm256_comp_fmadd_ps(_val, _mm256_set1_ps(kptr[3]), _sum3);
 
                         m += 8;
                         kptr += 4;
@@ -648,21 +650,21 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                         __m128 _val7 = _mm_broadcast_ss(m + 7);
 
                         __m128 _w0 = _mm_loadu_ps(kptr);
-                        _sum = _mm_fmadd_ps(_val0, _w0, _sum);
+                        _sum = _mm_comp_fmadd_ps(_val0, _w0, _sum);
                         __m128 _w1 = _mm_loadu_ps(kptr + 4);
-                        _sum = _mm_fmadd_ps(_val1, _w1, _sum);
+                        _sum = _mm_comp_fmadd_ps(_val1, _w1, _sum);
                         __m128 _w2 = _mm_loadu_ps(kptr + 8);
-                        _sum = _mm_fmadd_ps(_val2, _w2, _sum);
+                        _sum = _mm_comp_fmadd_ps(_val2, _w2, _sum);
                         __m128 _w3 = _mm_loadu_ps(kptr + 12);
-                        _sum = _mm_fmadd_ps(_val3, _w3, _sum);
+                        _sum = _mm_comp_fmadd_ps(_val3, _w3, _sum);
                         __m128 _w4 = _mm_loadu_ps(kptr + 16);
-                        _sum = _mm_fmadd_ps(_val4, _w4, _sum);
+                        _sum = _mm_comp_fmadd_ps(_val4, _w4, _sum);
                         __m128 _w5 = _mm_loadu_ps(kptr + 20);
-                        _sum = _mm_fmadd_ps(_val5, _w5, _sum);
+                        _sum = _mm_comp_fmadd_ps(_val5, _w5, _sum);
                         __m128 _w6 = _mm_loadu_ps(kptr + 24);
-                        _sum = _mm_fmadd_ps(_val6, _w6, _sum);
+                        _sum = _mm_comp_fmadd_ps(_val6, _w6, _sum);
                         __m128 _w7 = _mm_loadu_ps(kptr + 28);
-                        _sum = _mm_fmadd_ps(_val7, _w7, _sum);
+                        _sum = _mm_comp_fmadd_ps(_val7, _w7, _sum);
 
                         m += 8;
                         kptr += 32;
@@ -806,7 +808,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                     {
                         __m256 _m = _mm256_loadu_ps(m);
                         __m256 _w = _mm256_loadu_ps(kptr);
-                        _sum = _mm256_fmadd_ps(_m, _w, _sum);
+                        _sum = _mm256_comp_fmadd_ps(_m, _w, _sum);
 
                         m += 8;
                         kptr += 8;
@@ -846,12 +848,12 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
         return 0;
     }
 
-#if __AVX__
+#if __AVX2__
     if (opt.use_weight_fp16_storage)
     {
         return forward_fp16(bottom_blob, top_blob, opt);
     }
-#endif // __AVX__
+#endif // __AVX2__
 
     // flatten
     Mat bottom_blob_flattened = bottom_blob;
@@ -871,7 +873,8 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
     if (opt.use_packing_layout)
     {
 #if __AVX__
-        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4 : 1;
+        out_elempack = num_output % 8 == 0 ? 8 : num_output % 4 == 0 ? 4
+                                                                     : 1;
 #else
         out_elempack = num_output % 4 == 0 ? 4 : 1;
 #endif
@@ -888,7 +891,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
     if (out_elempack == 8)
     {
         // num_output
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < num_output / out_elempack; p++)
         {
             __m256 _sum0 = _mm256_set1_ps(0.f);
@@ -922,21 +925,21 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                 __m256 _val7 = _mm256_broadcast_ss(sptr + 7);
 
                 __m256 _w0 = _mm256_loadu_ps(kptr);
-                _sum0 = _mm256_fmadd_ps(_val0, _w0, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_val0, _w0, _sum0);
                 __m256 _w1 = _mm256_loadu_ps(kptr + 8);
-                _sum1 = _mm256_fmadd_ps(_val1, _w1, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_val1, _w1, _sum1);
                 __m256 _w2 = _mm256_loadu_ps(kptr + 16);
-                _sum2 = _mm256_fmadd_ps(_val2, _w2, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_val2, _w2, _sum2);
                 __m256 _w3 = _mm256_loadu_ps(kptr + 24);
-                _sum3 = _mm256_fmadd_ps(_val3, _w3, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_val3, _w3, _sum3);
                 __m256 _w4 = _mm256_loadu_ps(kptr + 32);
-                _sum4 = _mm256_fmadd_ps(_val4, _w4, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_val4, _w4, _sum4);
                 __m256 _w5 = _mm256_loadu_ps(kptr + 40);
-                _sum5 = _mm256_fmadd_ps(_val5, _w5, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_val5, _w5, _sum5);
                 __m256 _w6 = _mm256_loadu_ps(kptr + 48);
-                _sum6 = _mm256_fmadd_ps(_val6, _w6, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_val6, _w6, _sum6);
                 __m256 _w7 = _mm256_loadu_ps(kptr + 56);
-                _sum7 = _mm256_fmadd_ps(_val7, _w7, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_val7, _w7, _sum7);
 
                 sptr += 8;
                 kptr += 64;
@@ -949,13 +952,13 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                 __m256 _val3 = _mm256_broadcast_ss(sptr + 3);
 
                 __m256 _w0 = _mm256_loadu_ps(kptr);
-                _sum0 = _mm256_fmadd_ps(_val0, _w0, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_val0, _w0, _sum0);
                 __m256 _w1 = _mm256_loadu_ps(kptr + 8);
-                _sum1 = _mm256_fmadd_ps(_val1, _w1, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_val1, _w1, _sum1);
                 __m256 _w2 = _mm256_loadu_ps(kptr + 16);
-                _sum2 = _mm256_fmadd_ps(_val2, _w2, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_val2, _w2, _sum2);
                 __m256 _w3 = _mm256_loadu_ps(kptr + 24);
-                _sum3 = _mm256_fmadd_ps(_val3, _w3, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_val3, _w3, _sum3);
 
                 sptr += 4;
                 kptr += 32;
@@ -964,7 +967,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
             {
                 __m256 _val = _mm256_set1_ps(sptr[0]);
                 __m256 _w = _mm256_loadu_ps(kptr);
-                _sum0 = _mm256_fmadd_ps(_val, _w, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_val, _w, _sum0);
 
                 sptr += 1;
                 kptr += 8;
@@ -989,7 +992,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
     if (out_elempack == 4)
     {
         // num_output
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < num_output / out_elempack; p++)
         {
             __m128 _sum0 = _mm_set1_ps(0.f);
@@ -1026,21 +1029,21 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                 __m128 _val7 = _mm_broadcast_ss(sptr + 7);
 
                 __m128 _w0 = _mm_loadu_ps(kptr);
-                _sum0 = _mm_fmadd_ps(_val0, _w0, _sum0);
+                _sum0 = _mm_comp_fmadd_ps(_val0, _w0, _sum0);
                 __m128 _w1 = _mm_loadu_ps(kptr + 4);
-                _sum1 = _mm_fmadd_ps(_val1, _w1, _sum1);
+                _sum1 = _mm_comp_fmadd_ps(_val1, _w1, _sum1);
                 __m128 _w2 = _mm_loadu_ps(kptr + 8);
-                _sum2 = _mm_fmadd_ps(_val2, _w2, _sum2);
+                _sum2 = _mm_comp_fmadd_ps(_val2, _w2, _sum2);
                 __m128 _w3 = _mm_loadu_ps(kptr + 12);
-                _sum3 = _mm_fmadd_ps(_val3, _w3, _sum3);
+                _sum3 = _mm_comp_fmadd_ps(_val3, _w3, _sum3);
                 __m128 _w4 = _mm_loadu_ps(kptr + 16);
-                _sum4 = _mm_fmadd_ps(_val4, _w4, _sum4);
+                _sum4 = _mm_comp_fmadd_ps(_val4, _w4, _sum4);
                 __m128 _w5 = _mm_loadu_ps(kptr + 20);
-                _sum5 = _mm_fmadd_ps(_val5, _w5, _sum5);
+                _sum5 = _mm_comp_fmadd_ps(_val5, _w5, _sum5);
                 __m128 _w6 = _mm_loadu_ps(kptr + 24);
-                _sum6 = _mm_fmadd_ps(_val6, _w6, _sum6);
+                _sum6 = _mm_comp_fmadd_ps(_val6, _w6, _sum6);
                 __m128 _w7 = _mm_loadu_ps(kptr + 28);
-                _sum7 = _mm_fmadd_ps(_val7, _w7, _sum7);
+                _sum7 = _mm_comp_fmadd_ps(_val7, _w7, _sum7);
 
                 sptr += 8;
                 kptr += 32;
@@ -1102,7 +1105,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
         int remain_num_output_start = 0;
         int nn_num_output = num_output >> 3;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int pp = 0; pp < nn_num_output; pp++)
         {
             int p = pp * 8;
@@ -1146,21 +1149,21 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                 __m256 _m = _mm256_loadu_ps(m);
 
                 __m256 _w0 = _mm256_loadu_ps(w0);
-                _sum0 = _mm256_fmadd_ps(_m, _w0, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_m, _w0, _sum0);
                 __m256 _w1 = _mm256_loadu_ps(w1);
-                _sum1 = _mm256_fmadd_ps(_m, _w1, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_m, _w1, _sum1);
                 __m256 _w2 = _mm256_loadu_ps(w2);
-                _sum2 = _mm256_fmadd_ps(_m, _w2, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_m, _w2, _sum2);
                 __m256 _w3 = _mm256_loadu_ps(w3);
-                _sum3 = _mm256_fmadd_ps(_m, _w3, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_m, _w3, _sum3);
                 __m256 _w4 = _mm256_loadu_ps(w4);
-                _sum4 = _mm256_fmadd_ps(_m, _w4, _sum4);
+                _sum4 = _mm256_comp_fmadd_ps(_m, _w4, _sum4);
                 __m256 _w5 = _mm256_loadu_ps(w5);
-                _sum5 = _mm256_fmadd_ps(_m, _w5, _sum5);
+                _sum5 = _mm256_comp_fmadd_ps(_m, _w5, _sum5);
                 __m256 _w6 = _mm256_loadu_ps(w6);
-                _sum6 = _mm256_fmadd_ps(_m, _w6, _sum6);
+                _sum6 = _mm256_comp_fmadd_ps(_m, _w6, _sum6);
                 __m256 _w7 = _mm256_loadu_ps(w7);
-                _sum7 = _mm256_fmadd_ps(_m, _w7, _sum7);
+                _sum7 = _mm256_comp_fmadd_ps(_m, _w7, _sum7);
 
                 m += 8;
                 w0 += 8;
@@ -1210,7 +1213,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
         int nn_num_output = num_output >> 2;
 #endif // __AVX__
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int pp = 0; pp < nn_num_output; pp++)
         {
             int p = remain_num_output_start + (pp * 4);
@@ -1242,13 +1245,13 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                 __m256 _m = _mm256_loadu_ps(m);
 
                 __m256 _w0 = _mm256_loadu_ps(w0);
-                _sum0 = _mm256_fmadd_ps(_m, _w0, _sum0);
+                _sum0 = _mm256_comp_fmadd_ps(_m, _w0, _sum0);
                 __m256 _w1 = _mm256_loadu_ps(w1);
-                _sum1 = _mm256_fmadd_ps(_m, _w1, _sum1);
+                _sum1 = _mm256_comp_fmadd_ps(_m, _w1, _sum1);
                 __m256 _w2 = _mm256_loadu_ps(w2);
-                _sum2 = _mm256_fmadd_ps(_m, _w2, _sum2);
+                _sum2 = _mm256_comp_fmadd_ps(_m, _w2, _sum2);
                 __m256 _w3 = _mm256_loadu_ps(w3);
-                _sum3 = _mm256_fmadd_ps(_m, _w3, _sum3);
+                _sum3 = _mm256_comp_fmadd_ps(_m, _w3, _sum3);
 
                 m += 8;
                 w0 += 8;
@@ -1315,7 +1318,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
 #endif // __SSE2__
 
         // num_output
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = remain_num_output_start; p < num_output; p++)
         {
             float sum = 0.f;
@@ -1336,7 +1339,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
                 __m256 _m = _mm256_loadu_ps(m);
 
                 __m256 _w = _mm256_loadu_ps(w);
-                _sum = _mm256_fmadd_ps(_m, _w, _sum);
+                _sum = _mm256_comp_fmadd_ps(_m, _w, _sum);
 
                 m += 8;
                 w += 8;
@@ -1377,7 +1380,7 @@ int InnerProduct_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Optio
 
     return 0;
 }
-#if __AVX__
+#if __AVX2__
 
 int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const Option& opt) const
 {
@@ -1412,7 +1415,7 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
     int nn_num_output = num_output >> 3;
     int remain_num_output_start = nn_num_output << 3;
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int pp = 0; pp < nn_num_output; pp++)
     {
         int p = pp * 8;
@@ -1456,28 +1459,28 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
             __m256 _m = _mm256_loadu_ps(m);
 
             __m256 _w0 = loadfp16(w0);
-            _sum0 = _mm256_fmadd_ps(_m, _w0, _sum0);
+            _sum0 = _mm256_comp_fmadd_ps(_m, _w0, _sum0);
 
             __m256 _w1 = loadfp16(w1);
-            _sum1 = _mm256_fmadd_ps(_m, _w1, _sum1);
+            _sum1 = _mm256_comp_fmadd_ps(_m, _w1, _sum1);
 
             __m256 _w2 = loadfp16(w2);
-            _sum2 = _mm256_fmadd_ps(_m, _w2, _sum2);
+            _sum2 = _mm256_comp_fmadd_ps(_m, _w2, _sum2);
 
             __m256 _w3 = loadfp16(w3);
-            _sum3 = _mm256_fmadd_ps(_m, _w3, _sum3);
+            _sum3 = _mm256_comp_fmadd_ps(_m, _w3, _sum3);
 
             __m256 _w4 = loadfp16(w4);
-            _sum4 = _mm256_fmadd_ps(_m, _w4, _sum4);
+            _sum4 = _mm256_comp_fmadd_ps(_m, _w4, _sum4);
 
             __m256 _w5 = loadfp16(w5);
-            _sum5 = _mm256_fmadd_ps(_m, _w5, _sum5);
+            _sum5 = _mm256_comp_fmadd_ps(_m, _w5, _sum5);
 
             __m256 _w6 = loadfp16(w6);
-            _sum6 = _mm256_fmadd_ps(_m, _w6, _sum6);
+            _sum6 = _mm256_comp_fmadd_ps(_m, _w6, _sum6);
 
             __m256 _w7 = loadfp16(w7);
-            _sum7 = _mm256_fmadd_ps(_m, _w7, _sum7);
+            _sum7 = _mm256_comp_fmadd_ps(_m, _w7, _sum7);
 
             m += 8;
             w0 += 8;
@@ -1521,28 +1524,28 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
             __m256 _m = _mm256_loadu_ps(_m_f);
 
             __m256 _w0 = loadfp16(fp16_weights[0]);
-            _sum0 = _mm256_fmadd_ps(_m, _w0, _sum0);
+            _sum0 = _mm256_comp_fmadd_ps(_m, _w0, _sum0);
 
             __m256 _w1 = loadfp16(fp16_weights[1]);
-            _sum1 = _mm256_fmadd_ps(_m, _w1, _sum1);
+            _sum1 = _mm256_comp_fmadd_ps(_m, _w1, _sum1);
 
             __m256 _w2 = loadfp16(fp16_weights[2]);
-            _sum2 = _mm256_fmadd_ps(_m, _w2, _sum2);
+            _sum2 = _mm256_comp_fmadd_ps(_m, _w2, _sum2);
 
             __m256 _w3 = loadfp16(fp16_weights[3]);
-            _sum3 = _mm256_fmadd_ps(_m, _w3, _sum3);
+            _sum3 = _mm256_comp_fmadd_ps(_m, _w3, _sum3);
 
             __m256 _w4 = loadfp16(fp16_weights[4]);
-            _sum4 = _mm256_fmadd_ps(_m, _w4, _sum4);
+            _sum4 = _mm256_comp_fmadd_ps(_m, _w4, _sum4);
 
             __m256 _w5 = loadfp16(fp16_weights[5]);
-            _sum5 = _mm256_fmadd_ps(_m, _w5, _sum5);
+            _sum5 = _mm256_comp_fmadd_ps(_m, _w5, _sum5);
 
             __m256 _w6 = loadfp16(fp16_weights[6]);
-            _sum6 = _mm256_fmadd_ps(_m, _w6, _sum6);
+            _sum6 = _mm256_comp_fmadd_ps(_m, _w6, _sum6);
 
             __m256 _w7 = loadfp16(fp16_weights[7]);
-            _sum7 = _mm256_fmadd_ps(_m, _w7, _sum7);
+            _sum7 = _mm256_comp_fmadd_ps(_m, _w7, _sum7);
         }
 
         __m256 _sums = HorizontalSums(_sum0, _sum1, _sum2, _sum3, _sum4, _sum5, _sum6, _sum7);
@@ -1555,7 +1558,7 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
     int nn_offset = remain_num_output_start;
     remain_num_output_start += (nn_num_output << 2);
 
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int pp = 0; pp < nn_num_output; pp++)
     {
         int p = nn_offset + (pp * 4);
@@ -1587,16 +1590,16 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
             __m256 _m = _mm256_loadu_ps(m);
 
             __m256 _w0 = loadfp16(w0);
-            _sum0 = _mm256_fmadd_ps(_m, _w0, _sum0);
+            _sum0 = _mm256_comp_fmadd_ps(_m, _w0, _sum0);
 
             __m256 _w1 = loadfp16(w1);
-            _sum1 = _mm256_fmadd_ps(_m, _w1, _sum1);
+            _sum1 = _mm256_comp_fmadd_ps(_m, _w1, _sum1);
 
             __m256 _w2 = loadfp16(w2);
-            _sum2 = _mm256_fmadd_ps(_m, _w2, _sum2);
+            _sum2 = _mm256_comp_fmadd_ps(_m, _w2, _sum2);
 
             __m256 _w3 = loadfp16(w3);
-            _sum3 = _mm256_fmadd_ps(_m, _w3, _sum3);
+            _sum3 = _mm256_comp_fmadd_ps(_m, _w3, _sum3);
 
             m += 8;
             w0 += 8;
@@ -1626,16 +1629,16 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
             __m256 _m = _mm256_loadu_ps(_m_f);
 
             __m256 _w0 = loadfp16(fp16_weights[0]);
-            _sum0 = _mm256_fmadd_ps(_m, _w0, _sum0);
+            _sum0 = _mm256_comp_fmadd_ps(_m, _w0, _sum0);
 
             __m256 _w1 = loadfp16(fp16_weights[1]);
-            _sum1 = _mm256_fmadd_ps(_m, _w1, _sum1);
+            _sum1 = _mm256_comp_fmadd_ps(_m, _w1, _sum1);
 
             __m256 _w2 = loadfp16(fp16_weights[2]);
-            _sum2 = _mm256_fmadd_ps(_m, _w2, _sum2);
+            _sum2 = _mm256_comp_fmadd_ps(_m, _w2, _sum2);
 
             __m256 _w3 = loadfp16(fp16_weights[3]);
-            _sum3 = _mm256_fmadd_ps(_m, _w3, _sum3);
+            _sum3 = _mm256_comp_fmadd_ps(_m, _w3, _sum3);
         }
 
         __m128 _sums = HorizontalSums(_sum0, _sum1, _sum2, _sum3);
@@ -1644,7 +1647,7 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
     }
 
 // num_output
-    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
     for (int p = remain_num_output_start; p < num_output; p++)
     {
         float sum = 0.f;
@@ -1665,7 +1668,7 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
             __m256 _m = _mm256_loadu_ps(m);
 
             __m256 _w = loadfp16(w);
-            _sum = _mm256_fmadd_ps(_m, _w, _sum);
+            _sum = _mm256_comp_fmadd_ps(_m, _w, _sum);
 
             m += 8;
             w += 8;
@@ -1686,7 +1689,7 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
             __m256 _m = _mm256_loadu_ps(_m_f);
 
             __m256 _w = loadfp16(fp16_weights);
-            _sum = _mm256_fmadd_ps(_m, _w, _sum);
+            _sum = _mm256_comp_fmadd_ps(_m, _w, _sum);
         }
 
         sum += _mm256_reduce_add_ps(_sum);
@@ -1696,7 +1699,7 @@ int InnerProduct_x86::forward_fp16(const Mat& bottom_blob, Mat& top_blob, const
     }
     return 0;
 }
-#endif // __AVX__
+#endif // __AVX2__
 
 #if NCNN_INT8
 int InnerProduct_x86::create_pipeline_int8_x86(const Option& opt)
@@ -1836,7 +1839,7 @@ int InnerProduct_x86::forward_int8_x86(const Mat& bottom_blob, Mat& top_blob, co
     if (out_elempack == 8)
     {
         // num_output
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < num_output / out_elempack; p++)
         {
             __m128i _sum0 = _mm_setzero_si128();
@@ -1876,7 +1879,7 @@ int InnerProduct_x86::forward_int8_x86(const Mat& bottom_blob, Mat& top_blob, co
     if (out_elempack == 1)
     {
         // num_output
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int p = 0; p < num_output / out_elempack; p++)
         {
             int sum = 0;
diff --git a/src/layer/x86/lstm_x86.cpp b/src/layer/x86/lstm_x86.cpp
index d42195ef..e7e1dfef 100644
--- a/src/layer/x86/lstm_x86.cpp
+++ b/src/layer/x86/lstm_x86.cpp
@@ -46,6 +46,7 @@ int LSTM_x86::create_pipeline(const Option& opt)
     return 0;
 }
 #ifdef __AVX__
+#ifdef __AVX2__
 
 static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const Mat& weight_xc, const Mat& bias_c, const Mat& weight_hc, Mat& hidden_state, Mat& cell_state, const Option& opt)
 {
@@ -118,14 +119,14 @@ static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const M
             for (; nn_num_size > 0; nn_num_size--)
             {
                 __m256 xi = _mm256_loadu_ps(x);
-                _sumI_0 = _mm256_fmadd_ps(loadfp16(weight_xc_I_0), xi, _sumI_0);
-                _sumF_0 = _mm256_fmadd_ps(loadfp16(weight_xc_F_0), xi, _sumF_0);
-                _sumO_0 = _mm256_fmadd_ps(loadfp16(weight_xc_O_0), xi, _sumO_0);
-                _sumG_0 = _mm256_fmadd_ps(loadfp16(weight_xc_G_0), xi, _sumG_0);
-                _sumI_1 = _mm256_fmadd_ps(loadfp16(weight_xc_I_1), xi, _sumI_1);
-                _sumF_1 = _mm256_fmadd_ps(loadfp16(weight_xc_F_1), xi, _sumF_1);
-                _sumO_1 = _mm256_fmadd_ps(loadfp16(weight_xc_O_1), xi, _sumO_1);
-                _sumG_1 = _mm256_fmadd_ps(loadfp16(weight_xc_G_1), xi, _sumG_1);
+                _sumI_0 = _mm256_comp_fmadd_ps(loadfp16(weight_xc_I_0), xi, _sumI_0);
+                _sumF_0 = _mm256_comp_fmadd_ps(loadfp16(weight_xc_F_0), xi, _sumF_0);
+                _sumO_0 = _mm256_comp_fmadd_ps(loadfp16(weight_xc_O_0), xi, _sumO_0);
+                _sumG_0 = _mm256_comp_fmadd_ps(loadfp16(weight_xc_G_0), xi, _sumG_0);
+                _sumI_1 = _mm256_comp_fmadd_ps(loadfp16(weight_xc_I_1), xi, _sumI_1);
+                _sumF_1 = _mm256_comp_fmadd_ps(loadfp16(weight_xc_F_1), xi, _sumF_1);
+                _sumO_1 = _mm256_comp_fmadd_ps(loadfp16(weight_xc_O_1), xi, _sumO_1);
+                _sumG_1 = _mm256_comp_fmadd_ps(loadfp16(weight_xc_G_1), xi, _sumG_1);
                 x += 8;
                 weight_xc_I_0 += 8;
                 weight_xc_F_0 += 8;
@@ -142,14 +143,14 @@ static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const M
             {
                 __m256 h_cont = _mm256_loadu_ps(hidden_ptr_r);
 
-                _sumI_0 = _mm256_fmadd_ps(loadfp16(weight_hc_I_0), h_cont, _sumI_0);
-                _sumF_0 = _mm256_fmadd_ps(loadfp16(weight_hc_F_0), h_cont, _sumF_0);
-                _sumO_0 = _mm256_fmadd_ps(loadfp16(weight_hc_O_0), h_cont, _sumO_0);
-                _sumG_0 = _mm256_fmadd_ps(loadfp16(weight_hc_G_0), h_cont, _sumG_0);
-                _sumI_1 = _mm256_fmadd_ps(loadfp16(weight_hc_I_1), h_cont, _sumI_1);
-                _sumF_1 = _mm256_fmadd_ps(loadfp16(weight_hc_F_1), h_cont, _sumF_1);
-                _sumO_1 = _mm256_fmadd_ps(loadfp16(weight_hc_O_1), h_cont, _sumO_1);
-                _sumG_1 = _mm256_fmadd_ps(loadfp16(weight_hc_G_1), h_cont, _sumG_1);
+                _sumI_0 = _mm256_comp_fmadd_ps(loadfp16(weight_hc_I_0), h_cont, _sumI_0);
+                _sumF_0 = _mm256_comp_fmadd_ps(loadfp16(weight_hc_F_0), h_cont, _sumF_0);
+                _sumO_0 = _mm256_comp_fmadd_ps(loadfp16(weight_hc_O_0), h_cont, _sumO_0);
+                _sumG_0 = _mm256_comp_fmadd_ps(loadfp16(weight_hc_G_0), h_cont, _sumG_0);
+                _sumI_1 = _mm256_comp_fmadd_ps(loadfp16(weight_hc_I_1), h_cont, _sumI_1);
+                _sumF_1 = _mm256_comp_fmadd_ps(loadfp16(weight_hc_F_1), h_cont, _sumF_1);
+                _sumO_1 = _mm256_comp_fmadd_ps(loadfp16(weight_hc_O_1), h_cont, _sumO_1);
+                _sumG_1 = _mm256_comp_fmadd_ps(loadfp16(weight_hc_G_1), h_cont, _sumG_1);
                 hidden_ptr_r += 8;
                 weight_hc_I_0 += 8;
                 weight_hc_F_0 += 8;
@@ -188,14 +189,14 @@ static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const M
                     weight_xc_G_1++;
                 }
                 __m256 xi = _mm256_loadu_ps(_xi_f);
-                _sumI_0 = _mm256_fmadd_ps(loadfp16(fp16_weights[0]), xi, _sumI_0);
-                _sumF_0 = _mm256_fmadd_ps(loadfp16(fp16_weights[1]), xi, _sumF_0);
-                _sumO_0 = _mm256_fmadd_ps(loadfp16(fp16_weights[2]), xi, _sumO_0);
-                _sumG_0 = _mm256_fmadd_ps(loadfp16(fp16_weights[3]), xi, _sumG_0);
-                _sumI_1 = _mm256_fmadd_ps(loadfp16(fp16_weights[4]), xi, _sumI_1);
-                _sumF_1 = _mm256_fmadd_ps(loadfp16(fp16_weights[5]), xi, _sumF_1);
-                _sumO_1 = _mm256_fmadd_ps(loadfp16(fp16_weights[6]), xi, _sumO_1);
-                _sumG_1 = _mm256_fmadd_ps(loadfp16(fp16_weights[7]), xi, _sumG_1);
+                _sumI_0 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[0]), xi, _sumI_0);
+                _sumF_0 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[1]), xi, _sumF_0);
+                _sumO_0 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[2]), xi, _sumO_0);
+                _sumG_0 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[3]), xi, _sumG_0);
+                _sumI_1 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[4]), xi, _sumI_1);
+                _sumF_1 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[5]), xi, _sumF_1);
+                _sumO_1 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[6]), xi, _sumO_1);
+                _sumG_1 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[7]), xi, _sumG_1);
             }
             if (remain_num_output != 0)
             {
@@ -225,14 +226,14 @@ static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const M
                     weight_hc_G_1++;
                 }
                 __m256 h_cont = _mm256_loadu_ps(_hcont_f);
-                _sumI_0 = _mm256_fmadd_ps(loadfp16(fp16_weights[0]), h_cont, _sumI_0);
-                _sumF_0 = _mm256_fmadd_ps(loadfp16(fp16_weights[1]), h_cont, _sumF_0);
-                _sumO_0 = _mm256_fmadd_ps(loadfp16(fp16_weights[2]), h_cont, _sumO_0);
-                _sumG_0 = _mm256_fmadd_ps(loadfp16(fp16_weights[3]), h_cont, _sumG_0);
-                _sumI_1 = _mm256_fmadd_ps(loadfp16(fp16_weights[4]), h_cont, _sumI_1);
-                _sumF_1 = _mm256_fmadd_ps(loadfp16(fp16_weights[5]), h_cont, _sumF_1);
-                _sumO_1 = _mm256_fmadd_ps(loadfp16(fp16_weights[6]), h_cont, _sumO_1);
-                _sumG_1 = _mm256_fmadd_ps(loadfp16(fp16_weights[7]), h_cont, _sumG_1);
+                _sumI_0 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[0]), h_cont, _sumI_0);
+                _sumF_0 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[1]), h_cont, _sumF_0);
+                _sumO_0 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[2]), h_cont, _sumO_0);
+                _sumG_0 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[3]), h_cont, _sumG_0);
+                _sumI_1 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[4]), h_cont, _sumI_1);
+                _sumF_1 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[5]), h_cont, _sumF_1);
+                _sumO_1 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[6]), h_cont, _sumO_1);
+                _sumG_1 = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[7]), h_cont, _sumG_1);
             }
             float sums[8];
             _mm256_storeu_ps(sums, HorizontalSums(_sumI_0, _sumF_0, _sumO_0, _sumG_0, _sumI_1, _sumF_1, _sumO_1, _sumG_1));
@@ -291,10 +292,10 @@ static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const M
             for (; nn_num_size > 0; nn_num_size--)
             {
                 __m256 xi = _mm256_loadu_ps(x);
-                _sumI = _mm256_fmadd_ps(loadfp16(weight_xc_I), xi, _sumI);
-                _sumF = _mm256_fmadd_ps(loadfp16(weight_xc_F), xi, _sumF);
-                _sumO = _mm256_fmadd_ps(loadfp16(weight_xc_O), xi, _sumO);
-                _sumG = _mm256_fmadd_ps(loadfp16(weight_xc_G), xi, _sumG);
+                _sumI = _mm256_comp_fmadd_ps(loadfp16(weight_xc_I), xi, _sumI);
+                _sumF = _mm256_comp_fmadd_ps(loadfp16(weight_xc_F), xi, _sumF);
+                _sumO = _mm256_comp_fmadd_ps(loadfp16(weight_xc_O), xi, _sumO);
+                _sumG = _mm256_comp_fmadd_ps(loadfp16(weight_xc_G), xi, _sumG);
                 x += 8;
                 weight_xc_I += 8;
                 weight_xc_F += 8;
@@ -307,10 +308,10 @@ static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const M
             {
                 __m256 h_cont = _mm256_loadu_ps(hidden_ptr_r);
 
-                _sumI = _mm256_fmadd_ps(loadfp16(weight_hc_I), h_cont, _sumI);
-                _sumF = _mm256_fmadd_ps(loadfp16(weight_hc_F), h_cont, _sumF);
-                _sumO = _mm256_fmadd_ps(loadfp16(weight_hc_O), h_cont, _sumO);
-                _sumG = _mm256_fmadd_ps(loadfp16(weight_hc_G), h_cont, _sumG);
+                _sumI = _mm256_comp_fmadd_ps(loadfp16(weight_hc_I), h_cont, _sumI);
+                _sumF = _mm256_comp_fmadd_ps(loadfp16(weight_hc_F), h_cont, _sumF);
+                _sumO = _mm256_comp_fmadd_ps(loadfp16(weight_hc_O), h_cont, _sumO);
+                _sumG = _mm256_comp_fmadd_ps(loadfp16(weight_hc_G), h_cont, _sumG);
                 hidden_ptr_r += 8;
                 weight_hc_I += 8;
                 weight_hc_F += 8;
@@ -337,10 +338,10 @@ static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const M
                     weight_xc_G++;
                 }
                 __m256 xi = _mm256_loadu_ps(_xi_f);
-                _sumI = _mm256_fmadd_ps(loadfp16(fp16_weights[0]), xi, _sumI);
-                _sumF = _mm256_fmadd_ps(loadfp16(fp16_weights[1]), xi, _sumF);
-                _sumO = _mm256_fmadd_ps(loadfp16(fp16_weights[2]), xi, _sumO);
-                _sumG = _mm256_fmadd_ps(loadfp16(fp16_weights[3]), xi, _sumG);
+                _sumI = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[0]), xi, _sumI);
+                _sumF = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[1]), xi, _sumF);
+                _sumO = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[2]), xi, _sumO);
+                _sumG = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[3]), xi, _sumG);
             }
             if (remain_num_output != 0)
             {
@@ -362,10 +363,10 @@ static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const M
                     weight_hc_G++;
                 }
                 __m256 h_cont = _mm256_loadu_ps(_hcont_f);
-                _sumI = _mm256_fmadd_ps(loadfp16(fp16_weights[0]), h_cont, _sumI);
-                _sumF = _mm256_fmadd_ps(loadfp16(fp16_weights[1]), h_cont, _sumF);
-                _sumO = _mm256_fmadd_ps(loadfp16(fp16_weights[2]), h_cont, _sumO);
-                _sumG = _mm256_fmadd_ps(loadfp16(fp16_weights[3]), h_cont, _sumG);
+                _sumI = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[0]), h_cont, _sumI);
+                _sumF = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[1]), h_cont, _sumF);
+                _sumO = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[2]), h_cont, _sumO);
+                _sumG = _mm256_comp_fmadd_ps(loadfp16(fp16_weights[3]), h_cont, _sumG);
             }
 
             float sums[4];
@@ -445,7 +446,7 @@ static int lstm_fp16(const Mat& bottom_blob, Mat& top_blob, int reverse, const M
 
     return 0;
 }
-
+#endif
 static int lstm(const Mat& bottom_blob, Mat& top_blob, int reverse, const Mat& weight_xc, const Mat& bias_c, const Mat& weight_hc, Mat& hidden_state, Mat& cell_state, const Option& opt)
 {
     int size = bottom_blob.w;
@@ -519,14 +520,14 @@ static int lstm(const Mat& bottom_blob, Mat& top_blob, int reverse, const Mat& w
             for (; nn_num_size > 0; nn_num_size--)
             {
                 __m256 xi = _mm256_loadu_ps(x);
-                _sumI_0 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_I_0), xi, _sumI_0);
-                _sumF_0 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_F_0), xi, _sumF_0);
-                _sumO_0 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_O_0), xi, _sumO_0);
-                _sumG_0 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_G_0), xi, _sumG_0);
-                _sumI_1 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_I_1), xi, _sumI_1);
-                _sumF_1 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_F_1), xi, _sumF_1);
-                _sumO_1 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_O_1), xi, _sumO_1);
-                _sumG_1 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_G_1), xi, _sumG_1);
+                _sumI_0 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_I_0), xi, _sumI_0);
+                _sumF_0 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_F_0), xi, _sumF_0);
+                _sumO_0 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_O_0), xi, _sumO_0);
+                _sumG_0 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_G_0), xi, _sumG_0);
+                _sumI_1 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_I_1), xi, _sumI_1);
+                _sumF_1 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_F_1), xi, _sumF_1);
+                _sumO_1 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_O_1), xi, _sumO_1);
+                _sumG_1 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_G_1), xi, _sumG_1);
                 x += 8;
                 weight_xc_I_0 += 8;
                 weight_xc_F_0 += 8;
@@ -543,14 +544,14 @@ static int lstm(const Mat& bottom_blob, Mat& top_blob, int reverse, const Mat& w
             {
                 __m256 h_cont = _mm256_loadu_ps(hidden_ptr_r);
 
-                _sumI_0 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_I_0), h_cont, _sumI_0);
-                _sumF_0 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_F_0), h_cont, _sumF_0);
-                _sumO_0 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_O_0), h_cont, _sumO_0);
-                _sumG_0 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_G_0), h_cont, _sumG_0);
-                _sumI_1 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_I_1), h_cont, _sumI_1);
-                _sumF_1 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_F_1), h_cont, _sumF_1);
-                _sumO_1 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_O_1), h_cont, _sumO_1);
-                _sumG_1 = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_G_1), h_cont, _sumG_1);
+                _sumI_0 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_I_0), h_cont, _sumI_0);
+                _sumF_0 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_F_0), h_cont, _sumF_0);
+                _sumO_0 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_O_0), h_cont, _sumO_0);
+                _sumG_0 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_G_0), h_cont, _sumG_0);
+                _sumI_1 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_I_1), h_cont, _sumI_1);
+                _sumF_1 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_F_1), h_cont, _sumF_1);
+                _sumO_1 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_O_1), h_cont, _sumO_1);
+                _sumG_1 = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_G_1), h_cont, _sumG_1);
                 hidden_ptr_r += 8;
                 weight_hc_I_0 += 8;
                 weight_hc_F_0 += 8;
@@ -662,10 +663,10 @@ static int lstm(const Mat& bottom_blob, Mat& top_blob, int reverse, const Mat& w
             for (; nn_num_size > 0; nn_num_size--)
             {
                 __m256 xi = _mm256_loadu_ps(x);
-                _sumI = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_I), xi, _sumI);
-                _sumF = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_F), xi, _sumF);
-                _sumO = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_O), xi, _sumO);
-                _sumG = _mm256_fmadd_ps(_mm256_loadu_ps(weight_xc_G), xi, _sumG);
+                _sumI = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_I), xi, _sumI);
+                _sumF = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_F), xi, _sumF);
+                _sumO = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_O), xi, _sumO);
+                _sumG = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_xc_G), xi, _sumG);
                 x += 8;
                 weight_xc_I += 8;
                 weight_xc_F += 8;
@@ -678,10 +679,10 @@ static int lstm(const Mat& bottom_blob, Mat& top_blob, int reverse, const Mat& w
             {
                 __m256 h_cont = _mm256_loadu_ps(hidden_ptr_r);
 
-                _sumI = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_I), h_cont, _sumI);
-                _sumF = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_F), h_cont, _sumF);
-                _sumO = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_O), h_cont, _sumO);
-                _sumG = _mm256_fmadd_ps(_mm256_loadu_ps(weight_hc_G), h_cont, _sumG);
+                _sumI = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_I), h_cont, _sumI);
+                _sumF = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_F), h_cont, _sumF);
+                _sumO = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_O), h_cont, _sumO);
+                _sumG = _mm256_comp_fmadd_ps(_mm256_loadu_ps(weight_hc_G), h_cont, _sumG);
                 hidden_ptr_r += 8;
                 weight_hc_I += 8;
                 weight_hc_F += 8;
@@ -819,6 +820,7 @@ int LSTM_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& opt)
     // Uni directional
     if (direction == 0 || direction == 1)
     {
+#if __AVX2__
         if (opt.use_weight_fp16_storage)
         {
             // Uni directional
@@ -828,11 +830,14 @@ int LSTM_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& opt)
         }
         else
         {
+#endif
             // Uni directional
             int ret = lstm(bottom_blob, top_blob, direction, weight_xc_data.channel(0), bias_c_data.channel(0), weight_hc_data.channel(0), hidden, cell, opt);
             if (ret != 0)
                 return ret;
+#if __AVX2__
         }
+#endif
     }
 
     if (direction == 2)
@@ -844,7 +849,7 @@ int LSTM_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& opt)
         Mat top_blob_reverse(num_output, T, 4u, opt.workspace_allocator);
         if (top_blob_reverse.empty())
             return -100;
-
+#if __AVX2__
         if (opt.use_weight_fp16_storage)
         {
             // Uni directional
@@ -854,14 +859,18 @@ int LSTM_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& opt)
         }
         else
         {
+#endif
+
             // Uni directional
             int ret0 = lstm(bottom_blob, top_blob_forward, 0, weight_xc_data.channel(0), bias_c_data.channel(0), weight_hc_data.channel(0), hidden, cell, opt);
             if (ret0 != 0)
                 return ret0;
+#if __AVX2__
         }
-
+#endif
         hidden.fill(0.0f);
         cell.fill(0.0f);
+#if __AVX2__
         if (opt.use_weight_fp16_storage)
         {
             // Uni directional
@@ -871,11 +880,14 @@ int LSTM_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& opt)
         }
         else
         {
+#endif
             // Uni directional
             int ret1 = lstm(bottom_blob, top_blob_reverse, 1, weight_xc_data.channel(1), bias_c_data.channel(1), weight_hc_data.channel(1), hidden, cell, opt);
             if (ret1 != 0)
                 return ret1;
+#if __AVX2__
         }
+#endif
 
         // concat w
         for (int i = 0; i < T; i++)
@@ -916,7 +928,7 @@ int LSTM_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>& to
     top_blob.create(num_output, T, 4u, opt.blob_allocator);
     if (top_blob.empty())
         return -100;
-
+#if __AVX2__
     if (opt.use_weight_fp16_storage)
     {
         // Uni directional
@@ -926,11 +938,14 @@ int LSTM_x86::forward(const std::vector<Mat>& bottom_blobs, std::vector<Mat>& to
     }
     else
     {
+#endif
         // Uni directional
         int ret = lstm(bottom_blob, top_blob, direction, weight_xc_data.channel(0), bias_c_data.channel(0), weight_hc_data.channel(0), hidden_state, cell_state, opt);
         if (ret != 0)
             return ret;
+#if __AVX2__
     }
+#endif
     return 0;
 #else
     return LSTM::forward(bottom_blobs, top_blobs, opt);
diff --git a/src/layer/x86/quantize_x86.cpp b/src/layer/x86/quantize_x86.cpp
index 174eb127..49bebc85 100644
--- a/src/layer/x86/quantize_x86.cpp
+++ b/src/layer/x86/quantize_x86.cpp
@@ -55,7 +55,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
             {
                 __m256 _scale = _mm256_set1_ps(scale_data[0]);
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     const float* ptr = (const float*)bottom_blob + i * 8;
@@ -63,12 +63,16 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
 
                     __m256 _v = _mm256_loadu_ps(ptr);
                     _v = _mm256_mul_ps(_v, _scale);
+#if __AVX2__
                     *(int64_t*)outptr = float2int8_avx(_v);
+#else
+                    float2int8_loop(_v, outptr);
+#endif
                 }
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     const float* ptr = (const float*)bottom_blob + i * 8;
@@ -77,7 +81,11 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                     __m256 _v = _mm256_loadu_ps(ptr);
                     __m256 _scale = _mm256_loadu_ps((const float*)scale_data + i * 8);
                     _v = _mm256_mul_ps(_v, _scale);
+#if __AVX2__
                     *(int64_t*)outptr = float2int8_avx(_v);
+#else
+                    float2int8_loop(_v, outptr);
+#endif
                 }
             }
         }
@@ -95,13 +103,14 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
             {
                 __m256 _scale = _mm256_set1_ps(scale_data[0]);
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const float* ptr = bottom_blob.row(i);
                     signed char* outptr = top_blob.row<signed char>(i);
 
                     int j = 0;
+#if __AVX2__
                     for (; j + 1 < w; j += 2)
                     {
                         __m256 _v0 = _mm256_loadu_ps(ptr);
@@ -123,11 +132,21 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                         ptr += 8;
                         outptr += 8;
                     }
+#else
+                    for (; j < w; j++)
+                    {
+                        __m256 _v = _mm256_loadu_ps(ptr);
+                        _v = _mm256_mul_ps(_v, _scale);
+                        float2int8_loop(_v, outptr);
+                        ptr += 8;
+                        outptr += 8;
+                    }
+#endif
                 }
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const float* ptr = bottom_blob.row(i);
@@ -136,6 +155,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                     __m256 _scale = _mm256_loadu_ps((const float*)scale_data + i * 8);
 
                     int j = 0;
+#if __AVX2__
                     for (; j + 1 < w; j += 2)
                     {
                         __m256 _v0 = _mm256_loadu_ps(ptr);
@@ -157,6 +177,16 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                         ptr += 8;
                         outptr += 8;
                     }
+#else
+                    for (; j < w; j++)
+                    {
+                        __m256 _v = _mm256_loadu_ps(ptr);
+                        _v = _mm256_mul_ps(_v, _scale);
+                        float2int8_loop(_v, outptr);
+                        ptr += 8;
+                        outptr += 8;
+                    }
+#endif
                 }
             }
         }
@@ -176,13 +206,14 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
             {
                 __m256 _scale = _mm256_set1_ps(scale_data[0]);
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob.channel(q);
                     signed char* outptr = top_blob.channel(q);
 
                     int i = 0;
+#if __AVX2__
                     for (; i + 1 < size; i += 2)
                     {
                         __m256 _v0 = _mm256_loadu_ps(ptr);
@@ -204,11 +235,21 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                         ptr += 8;
                         outptr += 8;
                     }
+#else
+                    for (; i < size; i++)
+                    {
+                        __m256 _v = _mm256_loadu_ps(ptr);
+                        _v = _mm256_mul_ps(_v, _scale);
+                        float2int8_loop(_v, outptr);
+                        ptr += 8;
+                        outptr += 8;
+                    }
+#endif
                 }
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const float* ptr = bottom_blob.channel(q);
@@ -217,6 +258,8 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                     __m256 _scale = _mm256_loadu_ps((const float*)scale_data + q * 8);
 
                     int i = 0;
+#if __AVX2__
+
                     for (; i + 1 < size; i += 2)
                     {
                         __m256 _v0 = _mm256_loadu_ps(ptr);
@@ -238,6 +281,16 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                         ptr += 8;
                         outptr += 8;
                     }
+#else
+                    for (; i < size; i++)
+                    {
+                        __m256 _v = _mm256_loadu_ps(ptr);
+                        _v = _mm256_mul_ps(_v, _scale);
+                        float2int8_loop(_v, outptr);
+                        ptr += 8;
+                        outptr += 8;
+                    }
+#endif
                 }
             }
         }
@@ -262,7 +315,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
             {
                 const float scale = scale_data[0];
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     const float* ptr0 = (const float*)bottom_blob + i * 4;
@@ -276,7 +329,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     const float* ptr0 = (const float*)bottom_blob + i * 4;
@@ -307,7 +360,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                 {
                     __m128 _scale = _mm_set1_ps(scale_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outh; i++)
                     {
                         const float* ptr0 = bottom_blob.row(i * 2);
@@ -348,7 +401,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outh; i++)
                     {
                         const float* ptr0 = bottom_blob.row(i * 2);
@@ -397,7 +450,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                 {
                     const float scale = scale_data[0];
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < h; i++)
                     {
                         const float* ptr0 = bottom_blob.row(i);
@@ -423,7 +476,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < h; i++)
                     {
                         const float* ptr0 = bottom_blob.row(i);
@@ -474,7 +527,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                 {
                     __m128 _scale = _mm_set1_ps(scale_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < outc; q++)
                     {
                         const float* ptr0 = bottom_blob.channel(q * 2);
@@ -515,7 +568,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < outc; q++)
                     {
                         const float* ptr0 = bottom_blob.channel(q * 2);
@@ -564,7 +617,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                 {
                     const float scale = scale_data[0];
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < channels; q++)
                     {
                         const float* ptr0 = bottom_blob.channel(q);
@@ -590,7 +643,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < channels; q++)
                     {
                         const float* ptr0 = bottom_blob.channel(q);
@@ -641,7 +694,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
         {
             const float scale = scale_data[0];
 
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < w; i++)
             {
                 outptr[i] = float2int8(ptr[i] * scale);
@@ -649,7 +702,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
         }
         else
         {
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < w; i++)
             {
                 outptr[i] = float2int8(ptr[i] * scale_data[i]);
@@ -666,7 +719,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
         if (top_blob.empty())
             return -100;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int i = 0; i < h; i++)
         {
             const float* ptr0 = bottom_blob.row(i);
@@ -692,7 +745,7 @@ int Quantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option& o
         if (top_blob.empty())
             return -100;
 
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < channels; q++)
         {
             const float* ptr = bottom_blob.channel(q);
diff --git a/src/layer/x86/requantize_x86.cpp b/src/layer/x86/requantize_x86.cpp
index 8c223f63..2b07bf8a 100644
--- a/src/layer/x86/requantize_x86.cpp
+++ b/src/layer/x86/requantize_x86.cpp
@@ -16,9 +16,9 @@
 
 #if __SSE2__
 #include <emmintrin.h>
-#if __AVX__
+#if __AVX2__
 #include <immintrin.h>
-#endif // __AVX__
+#endif // __AVX2__
 #endif // __SSE2__
 
 #include "x86_activation.h"
@@ -51,7 +51,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (scale_in_data_size == 1 && scale_out_data_size == 1)
             {
-#if __AVX__
+#if __AVX2__
                 __m256 _scale_in = _mm256_set1_ps(scale_in_data[0]);
                 __m256 _scale_out = _mm256_set1_ps(scale_out_data[0]);
 #else
@@ -61,13 +61,13 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
                         _v = _mm256_mul_ps(_v, _scale_in);
                         _v = activation_avx(_v, activation_type, activation_params);
@@ -88,21 +88,21 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else if (bias_data_size == 1)
                 {
-#if __AVX__
+#if __AVX2__
                     __m256 _bias = _mm256_set1_ps(bias_data[0]);
 #else
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 #endif
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -121,16 +121,16 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _bias = _mm256_loadu_ps((const float*)bias_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -152,7 +152,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else if (scale_in_data_size == 1 && scale_out_data_size > 1)
             {
-#if __AVX__
+#if __AVX2__
                 __m256 _scale_in = _mm256_set1_ps(scale_in_data[0]);
 #else
                 __m128 _scale_in = _mm_set1_ps(scale_in_data[0]);
@@ -160,13 +160,13 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _scale_out = _mm256_loadu_ps((const float*)scale_out_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
                         _v = _mm256_mul_ps(_v, _scale_in);
@@ -190,22 +190,22 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else if (bias_data_size == 1)
                 {
-#if __AVX__
+#if __AVX2__
                     __m256 _bias = _mm256_set1_ps(bias_data[0]);
 #else
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 #endif
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _scale_out = _mm256_loadu_ps((const float*)scale_out_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -226,17 +226,17 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _scale_out = _mm256_loadu_ps((const float*)scale_out_data + i * 8);
                         __m256 _bias = _mm256_loadu_ps((const float*)bias_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -260,7 +260,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else if (scale_in_data_size > 1 && scale_out_data_size == 1)
             {
-#if __AVX__
+#if __AVX2__
                 __m256 _scale_out = _mm256_set1_ps(scale_out_data[0]);
 #else
                 __m128 _scale_out = _mm_set1_ps(scale_out_data[0]);
@@ -268,13 +268,13 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _scale_in = _mm256_loadu_ps((const float*)scale_in_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
                         _v = _mm256_mul_ps(_v, _scale_in);
@@ -298,22 +298,22 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else if (bias_data_size == 1)
                 {
-#if __AVX__
+#if __AVX2__
                     __m256 _bias = _mm256_set1_ps(bias_data[0]);
 #else
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 #endif
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _scale_in = _mm256_loadu_ps((const float*)scale_in_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -334,17 +334,17 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _scale_in = _mm256_loadu_ps((const float*)scale_in_data + i * 8);
                         __m256 _bias = _mm256_loadu_ps((const float*)bias_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -370,13 +370,13 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _scale_in = _mm256_loadu_ps((const float*)scale_in_data + i * 8);
                         __m256 _scale_out = _mm256_loadu_ps((const float*)scale_out_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
@@ -403,23 +403,23 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else if (bias_data_size == 1)
                 {
-#if __AVX__
+#if __AVX2__
                     __m256 _bias = _mm256_set1_ps(bias_data[0]);
 #else
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 #endif
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _scale_in = _mm256_loadu_ps((const float*)scale_in_data + i * 8);
                         __m256 _scale_out = _mm256_loadu_ps((const float*)scale_out_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -442,18 +442,18 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 8;
                         signed char* ptr = (signed char*)top_blob + i * 8;
 
-#if __AVX__
+#if __AVX2__
                         __m256 _scale_in = _mm256_loadu_ps((const float*)scale_in_data + i * 8);
                         __m256 _scale_out = _mm256_loadu_ps((const float*)scale_out_data + i * 8);
                         __m256 _bias = _mm256_loadu_ps((const float*)bias_data + i * 8);
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -490,13 +490,13 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const int* intptr = bottom_blob.row<const int>(i);
                     signed char* ptr = top_blob.row<signed char>(i);
 
-#if __AVX__
+#if __AVX2__
                     __m256 _scale_in = scale_in_data_size == 1 ? _mm256_set1_ps(scale_in_data[0]) : _mm256_loadu_ps((const float*)scale_in_data + i * 8);
                     __m256 _scale_out = scale_out_data_size == 1 ? _mm256_set1_ps(scale_out_data[0]) : _mm256_loadu_ps((const float*)scale_out_data + i * 8);
 #else
@@ -508,7 +508,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                     for (int j = 0; j < w; j++)
                     {
-#if __AVX__
+#if __AVX2__
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
                         _v = _mm256_mul_ps(_v, _scale_in);
                         _v = activation_avx(_v, activation_type, activation_params);
@@ -533,13 +533,13 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     const int* intptr = bottom_blob.row<const int>(i);
                     signed char* ptr = top_blob.row<signed char>(i);
 
-#if __AVX__
+#if __AVX2__
                     __m256 _scale_in = scale_in_data_size == 1 ? _mm256_set1_ps(scale_in_data[0]) : _mm256_loadu_ps((const float*)scale_in_data + i * 8);
                     __m256 _scale_out = scale_out_data_size == 1 ? _mm256_set1_ps(scale_out_data[0]) : _mm256_loadu_ps((const float*)scale_out_data + i * 8);
                     __m256 _bias = bias_data_size == 1 ? _mm256_set1_ps(bias_data[0]) : _mm256_loadu_ps((const float*)bias_data + i * 8);
@@ -554,9 +554,9 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                     for (int j = 0; j < w; j++)
                     {
-#if __AVX__
+#if __AVX2__
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -592,13 +592,13 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const int* intptr = bottom_blob.channel(q);
                     signed char* ptr = top_blob.channel(q);
 
-#if __AVX__
+#if __AVX2__
                     __m256 _scale_in = scale_in_data_size == 1 ? _mm256_set1_ps(scale_in_data[0]) : _mm256_loadu_ps((const float*)scale_in_data + q * 8);
                     __m256 _scale_out = scale_out_data_size == 1 ? _mm256_set1_ps(scale_out_data[0]) : _mm256_loadu_ps((const float*)scale_out_data + q * 8);
 #else
@@ -610,7 +610,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                     for (int i = 0; i < size; i++)
                     {
-#if __AVX__
+#if __AVX2__
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
                         _v = _mm256_mul_ps(_v, _scale_in);
                         _v = activation_avx(_v, activation_type, activation_params);
@@ -635,13 +635,13 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     const int* intptr = bottom_blob.channel(q);
                     signed char* ptr = top_blob.channel(q);
 
-#if __AVX__
+#if __AVX2__
                     __m256 _scale_in = scale_in_data_size == 1 ? _mm256_set1_ps(scale_in_data[0]) : _mm256_loadu_ps((const float*)scale_in_data + q * 8);
                     __m256 _scale_out = scale_out_data_size == 1 ? _mm256_set1_ps(scale_out_data[0]) : _mm256_loadu_ps((const float*)scale_out_data + q * 8);
                     __m256 _bias = bias_data_size == 1 ? _mm256_set1_ps(bias_data[0]) : _mm256_loadu_ps((const float*)bias_data + q * 8);
@@ -656,9 +656,9 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                     for (int i = 0; i < size; i++)
                     {
-#if __AVX__
+#if __AVX2__
                         __m256 _v = _mm256_cvtepi32_ps(_mm256_loadu_si256((const __m256i*)intptr));
-                        _v = _mm256_fmadd_ps(_v, _scale_in, _bias);
+                        _v = _mm256_comp_fmadd_ps(_v, _scale_in, _bias);
                         _v = activation_avx(_v, activation_type, activation_params);
                         _v = _mm256_mul_ps(_v, _scale_out);
                         *(int64_t*)ptr = float2int8_avx(_v);
@@ -703,7 +703,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -724,7 +724,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -743,7 +743,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -768,7 +768,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -790,7 +790,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -810,7 +810,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -836,7 +836,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -858,7 +858,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -878,7 +878,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -902,7 +902,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -925,7 +925,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 {
                     __m128 _bias = _mm_set1_ps(bias_data[0]);
 
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -946,7 +946,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < w; i++)
                     {
                         const int* intptr = (const int*)bottom_blob + i * 4;
@@ -984,7 +984,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outh; i++)
                     {
                         const int* intptr0 = bottom_blob.row<const int>(i * 2);
@@ -1016,7 +1016,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < outh; i++)
                     {
                         const int* intptr0 = bottom_blob.row<const int>(i * 2);
@@ -1053,7 +1053,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < h; i++)
                     {
                         const int* intptr = bottom_blob.row<const int>(i);
@@ -1087,7 +1087,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int i = 0; i < h; i++)
                     {
                         const int* intptr = bottom_blob.row<const int>(i);
@@ -1140,7 +1140,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < outc; q++)
                     {
                         const int* intptr0 = bottom_blob.channel(q * 2);
@@ -1172,7 +1172,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < outc; q++)
                     {
                         const int* intptr0 = bottom_blob.channel(q * 2);
@@ -1209,7 +1209,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 if (bias_data_size == 0)
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < channels; q++)
                     {
                         const int* intptr = bottom_blob.channel(q);
@@ -1243,7 +1243,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
                 }
                 else
                 {
-                    #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                     for (int q = 0; q < channels; q++)
                     {
                         const int* intptr = bottom_blob.channel(q);
@@ -1301,7 +1301,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in;
@@ -1312,7 +1312,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 const float bias = bias_data[0];
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in + bias;
@@ -1321,7 +1321,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in + bias_data[i];
@@ -1335,7 +1335,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in;
@@ -1346,7 +1346,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 const float bias = bias_data[0];
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in + bias;
@@ -1355,7 +1355,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in + bias_data[i];
@@ -1369,7 +1369,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in_data[i];
@@ -1380,7 +1380,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 const float bias = bias_data[0];
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in_data[i] + bias;
@@ -1389,7 +1389,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in_data[i] + bias_data[i];
@@ -1401,7 +1401,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
         {
             if (bias_data_size == 0)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in_data[i];
@@ -1412,7 +1412,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             {
                 const float bias = bias_data[0];
 
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in_data[i] + bias;
@@ -1421,7 +1421,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float v = intptr[i] * scale_in_data[i] + bias_data[i];
@@ -1442,7 +1442,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
         if (bias_data_size == 0)
         {
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < h; i++)
             {
                 const int* intptr = bottom_blob.row<const int>(i);
@@ -1460,7 +1460,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
         }
         else
         {
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int i = 0; i < h; i++)
             {
                 const int* intptr = bottom_blob.row<const int>(i);
@@ -1492,7 +1492,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
 
         if (bias_data_size == 0)
         {
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const int* intptr = bottom_blob.channel(q);
@@ -1510,7 +1510,7 @@ int Requantize_x86::forward(const Mat& bottom_blob, Mat& top_blob, const Option&
         }
         else
         {
-            #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
             for (int q = 0; q < channels; q++)
             {
                 const int* intptr = bottom_blob.channel(q);
diff --git a/src/layer/x86/scale_x86.cpp b/src/layer/x86/scale_x86.cpp
index c3cd7032..50a8c9fb 100644
--- a/src/layer/x86/scale_x86.cpp
+++ b/src/layer/x86/scale_x86.cpp
@@ -20,7 +20,7 @@
 #include <immintrin.h>
 #endif // __AVX__
 #endif // __SSE2__
-
+#include "x86_usability.h"
 namespace ncnn {
 
 Scale_x86::Scale_x86()
@@ -50,7 +50,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
             if (bias_term)
             {
                 const float* bias = bias_data;
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float* ptr = (float*)bottom_top_blob + i * 8;
@@ -58,13 +58,13 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
                     __m256 _p = _mm256_loadu_ps(ptr);
                     __m256 _s = _mm256_loadu_ps(scale + i * 8);
                     __m256 _bias = _mm256_loadu_ps(bias + i * 8);
-                    _p = _mm256_fmadd_ps(_p, _s, _bias);
+                    _p = _mm256_comp_fmadd_ps(_p, _s, _bias);
                     _mm256_storeu_ps(ptr, _p);
                 }
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float* ptr = (float*)bottom_top_blob + i * 8;
@@ -84,7 +84,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
 
             if (bias_term)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     float* ptr = bottom_top_blob.row(i);
@@ -94,7 +94,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
                     for (int j = 0; j < w; j++)
                     {
                         __m256 _p = _mm256_loadu_ps(ptr);
-                        _p = _mm256_fmadd_ps(_p, _s, _bias);
+                        _p = _mm256_comp_fmadd_ps(_p, _s, _bias);
                         _mm256_storeu_ps(ptr, _p);
 
                         ptr += 8;
@@ -103,7 +103,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     float* ptr = bottom_top_blob.row(i);
@@ -130,7 +130,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
 
             if (bias_term)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     float* ptr = bottom_top_blob.channel(q);
@@ -140,7 +140,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
                     for (int i = 0; i < size; i++)
                     {
                         __m256 _p = _mm256_loadu_ps(ptr);
-                        _p = _mm256_fmadd_ps(_p, _s, _bias);
+                        _p = _mm256_comp_fmadd_ps(_p, _s, _bias);
                         _mm256_storeu_ps(ptr, _p);
 
                         ptr += 8;
@@ -149,7 +149,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     float* ptr = bottom_top_blob.channel(q);
@@ -181,7 +181,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
             if (bias_term)
             {
                 const float* bias = bias_data;
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float* ptr = (float*)bottom_top_blob + i * 4;
@@ -195,7 +195,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < w; i++)
                 {
                     float* ptr = (float*)bottom_top_blob + i * 4;
@@ -215,7 +215,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
 
             if (bias_term)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     float* ptr = bottom_top_blob.row(i);
@@ -234,7 +234,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int i = 0; i < h; i++)
                 {
                     float* ptr = bottom_top_blob.row(i);
@@ -261,7 +261,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
 
             if (bias_term)
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     float* ptr = bottom_top_blob.channel(q);
@@ -280,7 +280,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
             }
             else
             {
-                #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
                 for (int q = 0; q < channels; q++)
                 {
                     float* ptr = bottom_top_blob.channel(q);
@@ -314,7 +314,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
     {
         const float* scale_ptr = scale_blob;
         const float* bias_ptr = bias_data;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < channels; q++)
         {
             float* ptr = bottom_top_blob.channel(q);
@@ -335,7 +335,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
             for (; nn > 0; nn--)
             {
                 __m256 _p = _mm256_loadu_ps(ptr);
-                _p = _mm256_fmadd_ps(_p, _s, _bias);
+                _p = _mm256_comp_fmadd_ps(_p, _s, _bias);
                 _mm256_storeu_ps(ptr, _p);
 
                 ptr += 8;
@@ -353,7 +353,7 @@ int Scale_x86::forward_inplace(std::vector<Mat>& bottom_top_blobs, const Option&
     else
     {
         const float* scale_ptr = scale_blob;
-        #pragma omp parallel for num_threads(opt.num_threads)
+#pragma omp parallel for num_threads(opt.num_threads)
         for (int q = 0; q < channels; q++)
         {
             float* ptr = bottom_top_blob.channel(q);
diff --git a/src/layer/x86/x86_activation.h b/src/layer/x86/x86_activation.h
index af35b679..d0944c6c 100644
--- a/src/layer/x86/x86_activation.h
+++ b/src/layer/x86/x86_activation.h
@@ -138,7 +138,11 @@ static inline __m256 tanh_avx(__m256 inputs)
 {
     const __m256 one = _mm256_set1_ps(1.0f);
     const __m256 two = _mm256_set1_ps(2.0f);
+#if __AVX2__
     return _mm256_fmsub_ps(sigmoid_avx(_mm256_mul_ps(inputs, two)), two, one);
+#else
+    return _mm256_sub_ps(_mm256_mul_ps(sigmoid_avx(_mm256_mul_ps(inputs, two)), two), one);
+#endif
 }
 
 static inline __m256 mish_avx(__m256 inputs)
diff --git a/src/layer/x86/x86_usability.h b/src/layer/x86/x86_usability.h
index 5a6682eb..987a1e6d 100644
--- a/src/layer/x86/x86_usability.h
+++ b/src/layer/x86/x86_usability.h
@@ -119,14 +119,35 @@ static inline __m128i float2int8_sse(const __m128& _v0, const __m128& _v1, const
 
 #if __AVX__
 #include <immintrin.h>
+#ifndef __AVX2__
+static inline __m256 _mm256_comp_fmadd_ps(__m256 _a, const __m256 _b, const __m256 _c)
+{
+    return _mm256_add_ps(_mm256_mul_ps(_a, _b), _c);
+}
+static inline __m128 _mm_comp_fmadd_ps(__m128 _a, const __m128 _b, const __m128 _c)
+{
+    return _mm_add_ps(_mm_mul_ps(_a, _b), _c);
+}
+#else
+static inline __m128 _mm_comp_fmadd_ps(__m128 _a, const __m128 _b, const __m128 _c)
+{
+    return _mm_fmadd_ps(_a, _b, _c);
+}
+static inline __m256 _mm256_comp_fmadd_ps(__m256 _a, const __m256 _b, const __m256 _c)
+{
+    return _mm256_fmadd_ps(_a, _b, _c);
+}
+#endif
+#if __AVX2__
 
 static inline __m256 loadfp16(const unsigned short* ptr)
 {
     return _mm256_cvtph_ps(_mm_lddqu_si128((__m128i*)(ptr)));
 }
+#endif
 static inline __m256 _mm256_fmadd_1_ps(__m256 a, __m256 b, float c)
 {
-    return _mm256_fmadd_ps(b, _mm256_set1_ps(c), a);
+    return _mm256_comp_fmadd_ps(b, _mm256_set1_ps(c), a);
 }
 
 static inline __m256 _mm256_fmrsub_1_ps(__m256 a, __m256 b, float c)
@@ -212,7 +233,7 @@ static inline float _mm256_reduce_add_ps(__m256 x)
     /* Conversion to float is a no-op on x86-64 */
     return _mm_cvtss_f32(x32);
 }
-
+#if __AVX2__
 static inline int64_t float2int8_avx(const __m256& _v0)
 {
     __m256i _v0_i = _mm256_cvtps_epi32(_mm256_round_ps(_v0, _MM_FROUND_TO_NEAREST_INT | _MM_FROUND_NO_EXC));
@@ -249,10 +270,21 @@ static inline __m128i float2int8_avx(const __m256& _v0, const __m256& _v1)
 
     return _mm256_extractf128_si256(_v8, 0);
 }
+#else
+static inline void float2int8_loop(const __m256& _v0, signed char* output)
+{
+    float data_0[8];
+    _mm256_storeu_ps(data_0, _v0);
+    for (int i = 0; i < 8; i++)
+    {
+        output[i] = float2int8(data_0[i]);
+    }
+}
+#endif
 
-static inline void _mm256_fmadd_ps4(__m256& _sum,
-                                    const __m256& _w0, const __m256& _w1, const __m256& _w2, const __m256& _w3,
-                                    const __m256& _v0, const __m256& _v1, const __m256& _v2, const __m256& _v3)
+static inline void _mm256_comp_fmadd_ps4(__m256& _sum,
+                                         const __m256& _w0, const __m256& _w1, const __m256& _w2, const __m256& _w3,
+                                         const __m256& _v0, const __m256& _v1, const __m256& _v2, const __m256& _v3)
 {
     __m256 _mul0 = _mm256_mul_ps(_w0, _v0);
     __m256 _mul1 = _mm256_mul_ps(_w1, _v1);
@@ -264,14 +296,15 @@ static inline void _mm256_fmadd_ps4(__m256& _sum,
     _sum = _mm256_add_ps(_sum, _sum0123);
 }
 
-static inline void _mm256_fmadd_ps8(__m256& _sum,
-                                    const __m256& _w0, const __m256& _w1, const __m256& _w2, const __m256& _w3, const __m256& _w4, const __m256& _w5, const __m256& _w6, const __m256& _w7,
-                                    const __m256& _v0, const __m256& _v1, const __m256& _v2, const __m256& _v3, const __m256& _v4, const __m256& _v5, const __m256& _v6, const __m256& _v7)
+static inline void _mm256_comp_fmadd_ps8(__m256& _sum,
+                                         const __m256& _w0, const __m256& _w1, const __m256& _w2, const __m256& _w3, const __m256& _w4, const __m256& _w5, const __m256& _w6, const __m256& _w7,
+                                         const __m256& _v0, const __m256& _v1, const __m256& _v2, const __m256& _v3, const __m256& _v4, const __m256& _v5, const __m256& _v6, const __m256& _v7)
 {
-    _mm256_fmadd_ps4(_sum, _w0, _w1, _w2, _w3, _v0, _v1, _v2, _v3);
+    _mm256_comp_fmadd_ps4(_sum, _w0, _w1, _w2, _w3, _v0, _v1, _v2, _v3);
 
-    _mm256_fmadd_ps4(_sum, _w4, _w5, _w6, _w7, _v4, _v5, _v6, _v7);
+    _mm256_comp_fmadd_ps4(_sum, _w4, _w5, _w6, _w7, _v4, _v5, _v6, _v7);
 }
+
 #endif // __AVX__
 #endif // __SSE2__
 
diff --git a/src/layer/x86/yolov3detectionoutput_x86.cpp b/src/layer/x86/yolov3detectionoutput_x86.cpp
index 2d9ca221..6e61e3c4 100644
--- a/src/layer/x86/yolov3detectionoutput_x86.cpp
+++ b/src/layer/x86/yolov3detectionoutput_x86.cpp
@@ -54,9 +54,9 @@ int Yolov3DetectionOutput_x86::forward(const std::vector<Mat>& bottom_blobs, std
         size_t mask_offset = b * num_box;
         int net_w = (int)(anchors_scale[b] * w);
         int net_h = (int)(anchors_scale[b] * h);
-        //printf("%d %d\n", net_w, net_h);
+//printf("%d %d\n", net_w, net_h);
 
-        //printf("%d %d %d\n", w, h, channels);
+//printf("%d %d %d\n", w, h, channels);
         #pragma omp parallel for num_threads(opt.num_threads)
         for (int pp = 0; pp < num_box; pp++)
         {
@@ -107,7 +107,7 @@ int Yolov3DetectionOutput_x86::forward(const std::vector<Mat>& bottom_blobs, std
                     float* ptr = ((float*)scores.data) + i * w + j;
                     float* end = ptr + num_class * cs;
                     int q = 0;
-#if __AVX__
+#if __AVX2__
                     float* end8 = ptr + (num_class & -8) * cs;
                     unsigned long index;
 
diff --git a/src/layer_registry.h.in b/src/layer_registry.h.in
index f544dd43..0d422779 100644
--- a/src/layer_registry.h.in
+++ b/src/layer_registry.h.in
@@ -11,6 +11,11 @@ static const layer_registry_entry layer_registry_avx2[] = {
 @layer_registry_avx2@
 };
 #endif // NCNN_RUNTIME_CPU && NCNN_AVX2
+#if NCNN_RUNTIME_CPU && (NCNN_AVX || NCNN_AVX2)
+static const layer_registry_entry layer_registry_avx[] = {
+@layer_registry_avx@
+};
+#endif // NCNN_RUNTIME_CPU && (NCNN_AVX || NCNN_AVX2)
 
 #if NCNN_RUNTIME_CPU && NCNN_ARM82 && !__APPLE__
 static const layer_registry_entry layer_registry_arm82[] = {
diff --git a/src/net.cpp b/src/net.cpp
index 3ff12424..927411f0 100644
--- a/src/net.cpp
+++ b/src/net.cpp
@@ -800,8 +800,8 @@ int NetPrivate::convert_layout(Mat& bottom_blob, const Layer* layer, const Optio
         {
             if (elembits == 32)
             {
-#if NCNN_AVX2
-                if (elemcount % 8 == 0 && ncnn::cpu_support_x86_avx2())
+#if (NCNN_AVX2 || NCNN_AVX)
+                if (elemcount % 8 == 0 && (ncnn::cpu_support_x86_avx2() || ncnn::cpu_support_x86_avx()))
                     dst_elempack = 8;
                 else if (elemcount % 4 == 0)
                     dst_elempack = 4;
diff --git a/src/platform.h.in b/src/platform.h.in
index 2c5eeddd..b7ccd9c0 100644
--- a/src/platform.h.in
+++ b/src/platform.h.in
@@ -30,6 +30,7 @@
 #cmakedefine01 NCNN_VULKAN
 #cmakedefine01 NCNN_RUNTIME_CPU
 #cmakedefine01 NCNN_AVX2
+#cmakedefine01 NCNN_AVX
 #cmakedefine01 NCNN_ARM82
 #cmakedefine01 NCNN_ARM82DOT
 #cmakedefine01 NCNN_MSA
@@ -64,10 +65,22 @@ namespace ncnn {
 class NCNN_EXPORT Mutex
 {
 public:
-    Mutex() { InitializeSRWLock(&srwlock); }
-    ~Mutex() {}
-    void lock() { AcquireSRWLockExclusive(&srwlock); }
-    void unlock() { ReleaseSRWLockExclusive(&srwlock); }
+    Mutex()
+    {
+        InitializeSRWLock(&srwlock);
+    }
+    ~Mutex()
+    {
+    }
+    void lock()
+    {
+        AcquireSRWLockExclusive(&srwlock);
+    }
+    void unlock()
+    {
+        ReleaseSRWLockExclusive(&srwlock);
+    }
+
 private:
     friend class ConditionVariable;
     // NOTE SRWLock is available from windows vista
@@ -77,11 +90,26 @@ private:
 class NCNN_EXPORT ConditionVariable
 {
 public:
-    ConditionVariable() { InitializeConditionVariable(&condvar); }
-    ~ConditionVariable() {}
-    void wait(Mutex& mutex) { SleepConditionVariableSRW(&condvar, &mutex.srwlock, INFINITE, 0); }
-    void broadcast() { WakeAllConditionVariable(&condvar); }
-    void signal() { WakeConditionVariable(&condvar); }
+    ConditionVariable()
+    {
+        InitializeConditionVariable(&condvar);
+    }
+    ~ConditionVariable()
+    {
+    }
+    void wait(Mutex& mutex)
+    {
+        SleepConditionVariableSRW(&condvar, &mutex.srwlock, INFINITE, 0);
+    }
+    void broadcast()
+    {
+        WakeAllConditionVariable(&condvar);
+    }
+    void signal()
+    {
+        WakeConditionVariable(&condvar);
+    }
+
 private:
     CONDITION_VARIABLE condvar;
 };
@@ -90,9 +118,21 @@ static unsigned __stdcall start_wrapper(void* args);
 class NCNN_EXPORT Thread
 {
 public:
-    Thread(void* (*start)(void*), void* args = 0) { _start = start; _args = args; handle = (HANDLE)_beginthreadex(0, 0, start_wrapper, this, 0, 0); }
-    ~Thread() {}
-    void join() { WaitForSingleObject(handle, INFINITE); CloseHandle(handle); }
+    Thread(void* (*start)(void*), void* args = 0)
+    {
+        _start = start;
+        _args = args;
+        handle = (HANDLE)_beginthreadex(0, 0, start_wrapper, this, 0, 0);
+    }
+    ~Thread()
+    {
+    }
+    void join()
+    {
+        WaitForSingleObject(handle, INFINITE);
+        CloseHandle(handle);
+    }
+
 private:
     friend unsigned __stdcall start_wrapper(void* args)
     {
@@ -108,21 +148,47 @@ private:
 class NCNN_EXPORT ThreadLocalStorage
 {
 public:
-    ThreadLocalStorage() { key = TlsAlloc(); }
-    ~ThreadLocalStorage() { TlsFree(key); }
-    void set(void* value) { TlsSetValue(key, (LPVOID)value); }
-    void* get() { return (void*)TlsGetValue(key); }
+    ThreadLocalStorage()
+    {
+        key = TlsAlloc();
+    }
+    ~ThreadLocalStorage()
+    {
+        TlsFree(key);
+    }
+    void set(void* value)
+    {
+        TlsSetValue(key, (LPVOID)value);
+    }
+    void* get()
+    {
+        return (void*)TlsGetValue(key);
+    }
+
 private:
     DWORD key;
 };
-#else // (defined _WIN32 && !(defined __MINGW32__))
+#else  // (defined _WIN32 && !(defined __MINGW32__))
 class NCNN_EXPORT Mutex
 {
 public:
-    Mutex() { pthread_mutex_init(&mutex, 0); }
-    ~Mutex() { pthread_mutex_destroy(&mutex); }
-    void lock() { pthread_mutex_lock(&mutex); }
-    void unlock() { pthread_mutex_unlock(&mutex); }
+    Mutex()
+    {
+        pthread_mutex_init(&mutex, 0);
+    }
+    ~Mutex()
+    {
+        pthread_mutex_destroy(&mutex);
+    }
+    void lock()
+    {
+        pthread_mutex_lock(&mutex);
+    }
+    void unlock()
+    {
+        pthread_mutex_unlock(&mutex);
+    }
+
 private:
     friend class ConditionVariable;
     pthread_mutex_t mutex;
@@ -131,11 +197,27 @@ private:
 class NCNN_EXPORT ConditionVariable
 {
 public:
-    ConditionVariable() { pthread_cond_init(&cond, 0); }
-    ~ConditionVariable() { pthread_cond_destroy(&cond); }
-    void wait(Mutex& mutex) { pthread_cond_wait(&cond, &mutex.mutex); }
-    void broadcast() { pthread_cond_broadcast(&cond); }
-    void signal() { pthread_cond_signal(&cond); }
+    ConditionVariable()
+    {
+        pthread_cond_init(&cond, 0);
+    }
+    ~ConditionVariable()
+    {
+        pthread_cond_destroy(&cond);
+    }
+    void wait(Mutex& mutex)
+    {
+        pthread_cond_wait(&cond, &mutex.mutex);
+    }
+    void broadcast()
+    {
+        pthread_cond_broadcast(&cond);
+    }
+    void signal()
+    {
+        pthread_cond_signal(&cond);
+    }
+
 private:
     pthread_cond_t cond;
 };
@@ -143,9 +225,18 @@ private:
 class NCNN_EXPORT Thread
 {
 public:
-    Thread(void* (*start)(void*), void* args = 0) { pthread_create(&t, 0, start, args); }
-    ~Thread() {}
-    void join() { pthread_join(t, 0); }
+    Thread(void* (*start)(void*), void* args = 0)
+    {
+        pthread_create(&t, 0, start, args);
+    }
+    ~Thread()
+    {
+    }
+    void join()
+    {
+        pthread_join(t, 0);
+    }
+
 private:
     pthread_t t;
 };
@@ -153,49 +244,98 @@ private:
 class NCNN_EXPORT ThreadLocalStorage
 {
 public:
-    ThreadLocalStorage() { pthread_key_create(&key, 0); }
-    ~ThreadLocalStorage() { pthread_key_delete(key); }
-    void set(void* value) { pthread_setspecific(key, value); }
-    void* get() { return pthread_getspecific(key); }
+    ThreadLocalStorage()
+    {
+        pthread_key_create(&key, 0);
+    }
+    ~ThreadLocalStorage()
+    {
+        pthread_key_delete(key);
+    }
+    void set(void* value)
+    {
+        pthread_setspecific(key, value);
+    }
+    void* get()
+    {
+        return pthread_getspecific(key);
+    }
+
 private:
     pthread_key_t key;
 };
 #endif // (defined _WIN32 && !(defined __MINGW32__))
-#else // NCNN_THREADS
+#else  // NCNN_THREADS
 class NCNN_EXPORT Mutex
 {
 public:
-    Mutex() {}
-    ~Mutex() {}
-    void lock() {}
-    void unlock() {}
+    Mutex()
+    {
+    }
+    ~Mutex()
+    {
+    }
+    void lock()
+    {
+    }
+    void unlock()
+    {
+    }
 };
 
 class NCNN_EXPORT ConditionVariable
 {
 public:
-    ConditionVariable() {}
-    ~ConditionVariable() {}
-    void wait(Mutex& /*mutex*/) {}
-    void broadcast() {}
-    void signal() {}
+    ConditionVariable()
+    {
+    }
+    ~ConditionVariable()
+    {
+    }
+    void wait(Mutex& /*mutex*/)
+    {
+    }
+    void broadcast()
+    {
+    }
+    void signal()
+    {
+    }
 };
 
 class NCNN_EXPORT Thread
 {
 public:
-    Thread(void* (*/*start*/)(void*), void* /*args*/ = 0) {}
-    ~Thread() {}
-    void join() {}
+    Thread(void* (*/*start*/)(void*), void* /*args*/ = 0)
+    {
+    }
+    ~Thread()
+    {
+    }
+    void join()
+    {
+    }
 };
 
 class NCNN_EXPORT ThreadLocalStorage
 {
 public:
-    ThreadLocalStorage() { data = 0; }
-    ~ThreadLocalStorage() {}
-    void set(void* value) { data = value; }
-    void* get() { return data; }
+    ThreadLocalStorage()
+    {
+        data = 0;
+    }
+    ~ThreadLocalStorage()
+    {
+    }
+    void set(void* value)
+    {
+        data = value;
+    }
+    void* get()
+    {
+        return data;
+    }
+
 private:
     void* data;
 };
@@ -204,8 +344,16 @@ private:
 class NCNN_EXPORT MutexLockGuard
 {
 public:
-    MutexLockGuard(Mutex& _mutex) : mutex(_mutex) { mutex.lock(); }
-    ~MutexLockGuard() { mutex.unlock(); }
+    MutexLockGuard(Mutex& _mutex)
+        : mutex(_mutex)
+    {
+        mutex.lock();
+    }
+    ~MutexLockGuard()
+    {
+        mutex.unlock();
+    }
+
 private:
     Mutex& mutex;
 };
@@ -226,13 +374,19 @@ private:
 #if NCNN_STDIO
 #if NCNN_PLATFORM_API && __ANDROID_API__ >= 8
 #include <android/log.h>
-#define NCNN_LOGE(...) do { \
-    fprintf(stderr, ##__VA_ARGS__); fprintf(stderr, "\n"); \
-    __android_log_print(ANDROID_LOG_WARN, "ncnn", ##__VA_ARGS__); } while(0)
+#define NCNN_LOGE(...)                                                \
+    do {                                                              \
+        fprintf(stderr, ##__VA_ARGS__);                               \
+        fprintf(stderr, "\n");                                        \
+        __android_log_print(ANDROID_LOG_WARN, "ncnn", ##__VA_ARGS__); \
+    } while (0)
 #else // NCNN_PLATFORM_API && __ANDROID_API__ >= 8
 #include <stdio.h>
-#define NCNN_LOGE(...) do { \
-    fprintf(stderr, ##__VA_ARGS__); fprintf(stderr, "\n"); } while(0)
+#define NCNN_LOGE(...)                  \
+    do {                                \
+        fprintf(stderr, ##__VA_ARGS__); \
+        fprintf(stderr, "\n");          \
+    } while (0)
 #endif // NCNN_PLATFORM_API && __ANDROID_API__ >= 8
 #else
 #define NCNN_LOGE(...)
diff --git a/tests/test_packing.cpp b/tests/test_packing.cpp
index 8bcc8a88..1f65943a 100644
--- a/tests/test_packing.cpp
+++ b/tests/test_packing.cpp
@@ -601,7 +601,7 @@ static int test_packing_0()
            || test_packing_gpu_image2buffer(a, 4, 8)
            || test_packing_gpu_image2buffer(a, 8, 4)
 #endif // NCNN_VULKAN
-        ;
+           ;
 }
 
 static int test_packing_1()
@@ -656,7 +656,7 @@ static int test_packing_1()
            || test_packing_gpu_image2buffer(a, 4, 8)
            || test_packing_gpu_image2buffer(a, 8, 4)
 #endif // NCNN_VULKAN
-        ;
+           ;
 }
 
 static int test_packing_2()
@@ -711,7 +711,7 @@ static int test_packing_2()
            || test_packing_gpu_image2buffer(a, 4, 8)
            || test_packing_gpu_image2buffer(a, 8, 4)
 #endif // NCNN_VULKAN
-        ;
+           ;
 }
 
 int main()
diff --git a/tests/testutil.h b/tests/testutil.h
index 58cca68c..d0efdaec 100644
--- a/tests/testutil.h
+++ b/tests/testutil.h
@@ -421,8 +421,8 @@ int test_layer_cpu(int typeindex, const ncnn::ParamDict& pd, const std::vector<n
 
             if (elembits == 32)
             {
-#if NCNN_AVX2
-                if (elemcount % 8 == 0 && ncnn::cpu_support_x86_avx2())
+#if (NCNN_AVX2 || NCNN_AVX)
+                if (elemcount % 8 == 0 && (ncnn::cpu_support_x86_avx2() || ncnn::cpu_support_x86_avx()))
                     dst_elempack = 8;
                 else if (elemcount % 4 == 0)
                     dst_elempack = 4;
@@ -851,8 +851,8 @@ int test_layer_cpu(int typeindex, const ncnn::ParamDict& pd, const std::vector<n
 
         if (elembits == 32)
         {
-#if NCNN_AVX2
-            if (elemcount % 8 == 0 && ncnn::cpu_support_x86_avx2())
+#if (NCNN_AVX2 || NCNN_AVX)
+            if (elemcount % 8 == 0 && (ncnn::cpu_support_x86_avx2() || ncnn::cpu_support_x86_avx()))
                 dst_elempack = 8;
             else if (elemcount % 4 == 0)
                 dst_elempack = 4;
